name: Multinational Retail Data Centralisation
description: |
  You work for a multinational company that sells various goods across the globe. 

  Currently, their sales data is spread across many different data sources making it not easily accessible or analysable by current members of the team.
  
  In an effort to become more data-driven, your organisation would like to make its sales data accessible from one centralised location.

  Your first goal will be to produce a system that stores the current company data in a database so that it's accessed from one centralised location and acts as a single source of truth for sales data. 

  Secondly, you will then query the database to get up-to-date metrics for the business.

  Finally, to make the data more readily available, you will create an API linked to the database that will allow the data analytics and data science team to easily access the data they need.

#               _   _                _                              _ 
#   _ __ ___   (_) | |   ___   ___  | |_    ___    _ __     ___    / |
#  | '_ ` _ \  | | | |  / _ \ / __| | __|  / _ \  | '_ \   / _ \   | |
#  | | | | | | | | | | |  __/ \__ \ | |_  | (_) | | | | | |  __/   | |
#  |_| |_| |_| |_| |_|  \___| |___/  \__|  \___/  |_| |_|  \___|   |_|

id: 8955a07c-2223-4757-9f74-2aa287aa1aca
requires_aws: False
rotate_to: 6461e8ce-1944-46dc-8b25-436ff0045280
requires_github: True
cover_img: https://aicore-portal-public-prod-307050600709.s3.eu-west-1.amazonaws.com/images/scenarios/retail-data-centralisation.png
milestones:
  - name: Set up the environment
    tasks:
      - name: Join the project calendar
        id: 13e995f1-1f33-4f51-8621-bc4750f33aff
        description: |
          To keep up with all the events associated with this project, please join its respective Google Calendar using this [link](https://calendar.google.com/calendar/u/1?cid=Y19lNzUwZjU2NGJhZTY1MGU5OGIzYWMyNDBhNzUzNzcxNjAxNTg3Yzg2NGEzYWUxMzgzZGZiZjAzODkzZTI1YmE1QGdyb3VwLmNhbGVuZGFyLmdvb2dsZS5jb20).
        duration: 0
      - name: Set up Github
        id: 5ddde2df-752f-498e-903e-68a222675117
        description: |
          In this scenario, you'll use GitHub to track changes to your code and save them online in a GitHub repo.
          Hit the button on the right to automatically create a new GitHub repo. 
          We'll tell you when you need to use it as you go through the project.
        duration: 1
        prerequisites:
          - 86076c6d-6df5-4c14-8f5e-7d3dfe661de6
          - b457ee87-74b7-414d-bbc8-43fb8acc8cc4
    description: Set up your dev environment to get started
    id: 2f6635c5-ac86-4db1-bb72-b00223fba501

    #              _   _                _                              ____  
    #  _ __ ___   (_) | |   ___   ___  | |_    ___    _ __     ___    |___ \ 
    # | '_ ` _ \  | | | |  / _ \ / __| | __|  / _ \  | '_ \   / _ \     __) |
    # | | | | | | | | | | |  __/ \__ \ | |_  | (_) | | | | | |  __/    / __/ 
    # |_| |_| |_| |_| |_|  \___| |___/  \__|  \___/  |_| |_|  \___|   |_____|
                                                                        

  - name: Extract and clean the data from the data sources.
    tasks:
      - name: Set up a new database to store the data.
        id: 3c53eb27-01c9-4127-b9c3-b2c963d65701
        description: |
          Initialise a new database locally to store the extracted data.<br> 
          Set up a new database within pgadmin4 and name it `Sales_Data`.<br> 
          This database will store all the company information once you extract it for the various data sources.<br>
              
        duration: 1
        prerequisites:
          - 13308eb8-cd86-4e7e-b91a-271e01020dc9 # 1 SQL Setup
      - name: Initialise the three project Classes.
        id: 867dacdc-a793-4dd4-858b-585dfa1995ec
        description: |
          In this task you will be defining the scripts and Classes you will use to extract and clean the data from multiple data sources.<br>
          The Class methods won't be defined in this step yet they will be defined when required in the subsequent tasks.<br><br>

          `Step 1`:<br>
          Create a new Python script named `data_extraction.py` and within it, create a class named `DataExtractor`.<br> 
          It will have no initialisation method.<br> 
          This class will work as a utility class, in it you will be creating methods that help extract data from different data sources.<br> 
          The methods contained will be fit to extract data from a particular data source, these sources will include CSV files, an API and an S3 bucket.<br><br> 

          `Step 2`:<br> 
          Create another script named `database_utils.py` and within it, create a class `DatabaseConnector` which you will use to connect with and upload data to the database.<br><br> 

          `Step 3`:<br> 
          Finally, create a script named `data_cleaning.py` this script will contain a class `DataClean` with methods to clean data from each of the data sources.  
        duration: 2
        prerequisites:
          - e8254ad1-ac7f-49e5-b123-5d20fd4dcc4d # Principles of OOP design
  
      - name: Extract and clean the user data.
        id: 2f5884f9-390a-4c71-9291-762d7b3896a6
        description: |
          The historical data of users is currently stored in an AWS database in the cloud.<br>
          You will now create methods in your `DataExtractor` and `DatabaseConnector` class which help extract the information from an AWS RDS database.<br><br>

          `Step 1`:<br>  
          Create a `db_creds.yaml` file containing the database credentials, they are as follows:<br><br>

          RDS_HOST: data-handling-project-readonly.cq2e8zno855e.eu-west-1.rds.amazonaws.com<br>
          RDS_PASSWORD: AiCore2022<br>
          RDS_USER: aicore_admin<br>
          RDS_DATABASE: postgres<br>
          RDS_PORT: 5432<br><br>

          Now you will need to develop methods in your `DatabaseConnector` class to extract the data from the database.<br><br>

          `Step 2`:<br>  
          Create a method `read_db_creds` this will read the credentials `yaml` file and return a dictionary of the credentials.<br> 
          You will need to `pip install PyYAML` and `import yaml` to do this.<br><br>
         
          `Step 3`:<br>  
          Now create a method `init_db_engine` which will read the credentials from the return of `read_db_creds` and initialise and return an sqlalchemy database engine.<br><br>

          `Step 4`:<br>  
          Using the engine from `init_db_engine` create a method `list_db_tables` to list all the tables in the database so you know which tables you can extract data from.<br>
          Develop a method inside your `DataExtractor` class to read the data from the RDS database.<br><br>
          
          `Step 5`:<br>  
          Develop a method called `extract_rds_table` in your `DataExtractor` class which will extract the database table to a pandas Dataframe.<br>  
          It will take in an instance of your `DatabaseConnector` class and the table name as an argument and return a pandas Dataframe.<br>
          Use your `list_db_tables` method to get the name of the table containing user data.<br> 
          Use the `read_rds_table` method to extract the table containing user data and return a pandas Dataframe.<br><br>

          `Step 6`:<br>  
          Create a method called `clean_user_data` in the `DataClean` class which will perform the cleaning of the user data.<br>
          
          You will need clean the user data, look out for `NULL` values, errors with dates, incorrectly typed values and rows filled with the wrong information.<br><br>

          `Step 7`:<br>  
          Now create a method in your `DatabaseConnector` class called `upload_to_db`. This method will take in a Pandas dataframe and table name to upload to as an arguement.<br><br>

          `Step 8`:<br>  
          Once extracted and cleaned use the `upload_to_db` method to store the data in your `Sales_Data` database in a table named `dim_users`.<br><br>
        prerequisites:
          - 3fad431d-917f-4989-abc8-6d22d2961e37 # 1 Data Cleaning in Pandas
          - 45d60050-aef5-400b-a6cb-e35f35f9ace9 # 4 Data and File Formats
          - 3949170b-c8b8-4353-9983-cdfb18b6efbe # 7 Pandas - Advanced  Operations
          - 84a9c4c6-3bb2-4095-a96a-6d4ec15498c3 # postgresql and SQLalchemy
        duration: 5
    
      - name: Extracting users and cleaning card details.
        id: 0db69d1c-f599-4550-a9d6-678f30e12d84
        description: |
          The user card details are stored in a PDF document in an AWS S3 bucket. 

          `Step 1`:<br>
          Install the Python package `tabular-py` this will help you to extract data from a `pdf` document. The documentation can be found [here](https://pypi.org/project/tabula-py/).<br><br>

          `Step 2`:<br>
          Create a method in your `DataExtractor` class called `retrieve_pdf_data`, which take in a link as an argument and return a pandas Dataframe.<br>
          Use the `tabular-py` Python package, imported with `tabula` to extract all pages from the pdf document at following [link](https://data-handling-public.s3.eu-west-1.amazonaws.com/card_details.pdf).<br>
          Then return a dataframe of the extracted data.<br><br>

          `Step 3`:<br> 
          Create a method in called `clean_card_data` your `DataClean` class to clean the data to remove any erroneous values, NULL values or errors with formatting.<br><br> 

          `Step 4`:<br>
          Once cleaned, upload the table with your `upload_to_db` method to the database in a table called `dim_card_details`.
        prerequisites:
          - 7e5a5266-ea05-4aa6-9bbe-b8affd65e280
        duration: 4
     
      - name: Extract and clean the details of each store.
        id: 6aeed593-79cb-43dc-a826-08f82994448e
        description: |
          The store data can be retrieved through the use of an API.<br>

          The API has two GET methods. One will return the number of stores in the business and the other to retrieve a store given a store number.<br> 

          To connect to the API you will need to include the API key to connect to the API in the method header.<br>

          Create a dictionary to store the header it will have a key `x-api-key` with the value `yFBQbwXe9J3sd6zWVAMrK6lcxxr0q1lr2PT6DDMX`.<br>

          The two endpoints for the API are as follows:

          <ul>
            <li>Retrieve a store: `https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/{store_number}`</li>
            <li>Return the number of stores: `https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores`</li>
          </ul>
        
          `Step 1`:<br>
          Create a method in your `DataExtractor` class called `list_number_of_stores` which returns the number of stores to extract. It should take in the number of stores endpoint as a argument.<br><br> 

          `Step 2`:<br>
          Now that you know how many stores need to be extracted from the API.<br><br> 

          `Step 3`:<br>
          Create another method `retrieve_stores_data` which will take the retrieve a store endpoint as an argument and extracts all the stores from the API saving them in a pandas dataframe.<br><br>

          `Step 4`:<br>
          Create a method in the `DataClean` class `called_clean_store_data` which cleans the data retrieve from the API and returns a pandas Dataframe.<br><br>
          
          `Step 5`:<br>
          Upload your Dataframe to the database using the `upload_to_db` method storing it in the table `dim_store_details`.

        duration: 3
        prerequisites:
          - bf5e1b7b-0556-4417-809a-85e544ba2e0e # 1 APIs and Requests - Overview
      - name: Extract and clean the product details.
        id: 3721b8ce-8556-4cb2-8ecc-4ade78cf4e79
        description: |
          The information for each product the company currently sells is stored in CSV format in an S3 bucket on AWS.<br>

          `Step 1`:<br> 
          Create a method in `DataExtractor` called `extract_from_s3` which uses the `boto3` package to download and extract the information returning a pandas Dataframe.<br> 

          The S3 address for the products data is the following `s3://data-handling-public/products.csv` the method will take this address in as an argument and return the pandas Dataframe.<br><br>

          `Step 2`:<br> 
          Create a method in the `DataClean` class called `convert_product_weights` this will take the products dataframe as an argument and return the products Dataframe.<br>
          If you check the `weight` column in the Dataframe the weights all have difference units.<br>
          Convert them all to a decimal value representing their weight in `kg`. Use a 1:1 ratio of `ml` to `g` as a rough estimate for the rows containing `ml`.<br>
          Develop the method to cleanup the weight column and remove all excess characters then represent the weights as a float.<br><br> 
          
          `Step 3`:<br>
          Now create another method called `clean_products_data` this method will clean the dataframe of any addtional erroneous values.<br><br> 

          `Step 4`:<br>
          Once complete insert the data into the `Sales_Data` database using your `upload_to_db` method storing it in a table named `dim_products`.
        duration: 3
        prerequisites:
          - d867845b-17a7-401b-b9aa-c0de78e835f0 # EC2 
          
      - name: Retrieve and clean the orders table.
        id: 9f805534-4527-4f87-a984-9982c6d09210
        description: |
          This table which acts as the single source of truth for all orders the company has made in the past is stored in a database on AWS RDS.<br><br>

          `Step 1`:<br> 
          Using the database table listing methods you created earlier `list_db_tables`, list all the tables in the database to get the name of the table containing all information about the product orders.<br><br> 

          `Step 2`:<br> 
          Extract the orders data using the `extract_rds_table` method you create earlier returning a pandas Dataframe.<br><br>

          `Step 3`:<br> 
          Create a method in `DataClean` called `clean_orders_data` which will clean the orders table data.<br>

          You should remove the columns "first_name", "last_name" and "1" to have the table in the correct form before uploading to the database.<br>

          You will see that the orders data contains column headers which are the same in other tables.<br>

          This table will act as the source of truth for your sales data and will be at the center of your star based database schema.<br><br> 

          `Step 4`:<br> 
          Once cleaned upload using the `upload_to_db` method and store in a table called `orders_table`,
        duration: 2

      - name: Retrieve and clean the date events data.
        id: 47890f99-f062-4914-9bdc-5b59054fd139
        description: |
          The final source of data is a JSON file containing the details of when each sale happened, as well as related attributes.<br>
          
          The file is currently stored on S3 and can be found at the following link `https://data-handling-public.s3.eu-west-1.amazonaws.com/date_details.json`.<br>

          Extract the file and perform any necessary cleaning, then upload the data to the database naming the table `dim_date_times`.
        duration: 5
    description: |
        Your first mission will be to extract all the data from the multitude of data sources, clean it, and then store it in a database that you will create. 
    id: 57e7edae-54df-497a-aafc-44c15f6bc7f8

#               _   _                _                              _____ 
#   _ __ ___   (_) | |   ___   ___  | |_    ___    _ __     ___    |___ / 
#  | '_ ` _ \  | | | |  / _ \ / __| | __|  / _ \  | '_ \   / _ \     |_ \ 
#  | | | | | | | | | | |  __/ \__ \ | |_  | (_) | | | | | |  __/    ___) |
#  |_| |_| |_| |_| |_|  \___| |___/  \__|  \___/  |_| |_|  \___|   |____/


  - name: Create the database schema
    tasks:
      - name: Cast the columns of the `orders_table` to the correct data types.
        id: 9e31c062-a1bc-4d85-b7e9-e58c2150015e
        description: |
          Change the data types to correspond to those seen in the table below.
          <pre>
          +------------------+--------------------+--------------------+
          |   orders_table   | current data type  | required data type |
          +------------------+--------------------+--------------------+
          | date_uuid        | TEXT               | UUID               |
          | user_uuid        | TEXT               | UUID               |
          | card_number      | TEXT               | VARCHAR(?)         |
          | store_code       | TEXT               | VARCHAR(?)         |
          | product_code     | TEXT               | VARCHAR(?)         |
          | product_quantity | BIGINT             | SMALLINT           |
          +------------------+--------------------+--------------------+
          </pre>

          The `?` in `VARCHAR` should be replaced with an integer representing the maximum length of the values in that column.
            
        duration: 2
        prerequisites:
          - 13308eb8-cd86-4e7e-b91a-271e01020dc9 # 1 SQL Setup
          - 9edc0c54-ff0e-4d4c-bdda-49ed3527a94e # 2 SQL Basics
          - c7ace46d-395a-47fe-b311-fd3b604270c0 # 3 CRUD
          - cf776c94-6dbe-4c80-8a00-af842f5c66ea # 4 Joins
          - 30d1f6c4-3518-4b0e-a6e7-d3198fb12c62 # 5 Aggregations
          - c5573ed3-239a-41d4-a435-74db5e153547 # 6 Subqueries

      - name: Cast the columns of the `dim_users_table` to the correct data types.
        id: b9cc0c33-134b-418e-9b26-27cb4c6df109
        description: |
          The column required to be changed in the users table are as follows:
          <pre>
          +----------------+--------------------+--------------------+
          | dim_user_table | current data type  | required data type |
          +----------------+--------------------+--------------------+
          | first_name     | TEXT               | VARCHAR(255)       |
          | last_name      | TEXT               | VARCHAR(255)       |
          | date_of_birth  | TEXT               | DATE               |
          | country_code   | TEXT               | VARCHAR(?)         |
          | user_uuid      | TEXT               | UUID               |
          | join_date      | TEXT               | DATE               |
          +----------------+--------------------+--------------------+
          </pre>   
      
        duration: 2
      - name: Update the `dim_store_details` table.
        id: 50ba6e0a-0b45-4bd4-8d8d-00be75923bc0
        description: |
          There are two latitude columns in the store details table. 
          Using SQL, merge one of the columns into the other so you have one `latitude` column.

          Then set the data types for each column as shown below:
          <pre>
          +---------------------+-------------------+------------------------+
          | store_details_table | current data type |   required data type   |
          +---------------------+-------------------+------------------------+
          | longitude           | TEXT              | FLOAT                  |
          | locality            | TEXT              | VARCHAR(255)           |
          | store_code          | TEXT              | VARCHAR(?)             |
          | staff_numbers       | TEXT              | SMALLINT               |
          | opening_date        | TEXT              | DATE                   |
          | store_type          | TEXT              | VARCHAR(255) NULLABLE  |
          | latitude            | TEXT              | FLOAT                  |
          | country_code        | TEXT              | VARCHAR(?)             |
          | continent           | TEXT              | VARCHAR(255)           |
          +---------------------+-------------------+------------------------+
          </pre> 
          
          There is a row that represents the business's website change the location column values where they're `null` to `N/A`.

        duration: 2

      - name: Make changes to the `dim_products` table for the delivery team.
        id: 1c75c2bf-6ca3-4020-9c56-7e7b64587587
        description: |
          You will need to do some work on the `products` table before casting the data types correctly. 

          The `product_price` column has a `Â£` character which you need to remove using SQL.

          The team that handles the deliveries would like a new human-readable column added for the weight so they can quickly make decisions on delivery weights.

          Add a new column `weight_class` which will contain human-readable values based on the weight range of the product. 
          <pre>
          +--------------------------+-------------------+
          | weight_class VARCHAR(?)  | weight range(kg)  |
          +--------------------------+-------------------+
          | Light                    | < 2               |
          | Mid_Sized                | 3 - 40            |
          | Heavy                    | 41 - 140          |
          | Truck_Required           | > 141             |
          +----------------------------+-----------------+
          </pre> 
        duration: 2

      - name: Update the `dim_products` table with the required data types.
        id: 2b40909b-92cd-487f-878a-16ddd135d8aa
        description: |
          After all the columns are created and cleaned, change the data types of the `products` table. 
          
          You will want to rename the `removed` column to `still_available` before changing its data type. 

          Make the changes to the columns to cast them to the following data types:
          <pre>
          +-----------------+--------------------+--------------------+
          |  dim_products   | current data type  | required data type |
          +-----------------+--------------------+--------------------+
          | product_price   | TEXT               | FLOAT              |
          | weight          | TEXT               | FLOAT              |
          | EAN             | TEXT               | VARCHAR(?)         |
          | product_code    | TEXT               | VARCHAR(?)         |
          | date_added      | TEXT               | DATE               |
          | uuid            | TEXT               | UUID               |
          | still_available | TEXT               | BOOL               |
          | weight_class    | TEXT               | VARCHAR(?)         |
          +-----------------+--------------------+--------------------+
          </pre> 
        duration: 2
      - name: Update the `dim_date_times` table.
        id: aae83411-96bb-43c2-ba87-619b6de582f8
        description: |
          Now update the date table with the correct types:
          <pre>
          +-----------------+-------------------+--------------------+
          | dim_date_times  | current data type | required data type |
          +-----------------+-------------------+--------------------+
          | month           | TEXT              | CHAR(?)            |
          | year            | TEXT              | CHAR(?)            |
          | day             | TEXT              | CHAR(?)            |
          | time_period     | TEXT              | VARCHAR(?)         |
          | date_uuid       | TEXT              | UUID               |
          +-----------------+-------------------+--------------------+
          </pre> 
        duration: 2
      - name: Updating the `dim_card_details` table.
        id: be541086-1901-4a66-96b1-7e99282afd37
        description: |
          Now we need to update the last table for the card details.

          Make the associated changes after finding out what the lengths of each variable should be:
          <pre>
          +------------------------+-------------------+--------------------+
          |    dim_card_details    | current data type | required data type |
          +------------------------+-------------------+--------------------+
          | card_number            | TEXT              | VARCHAR(?)         |
          | expiry_date            | TEXT              | VARCHAR(?)         |
          | date_payment_confirmed | TEXT              | DATE               |
          +------------------------+-------------------+--------------------+
          </pre> 

        duration: 2

      - name: Create the primary keys in the dimension tables.
        id: c4bf70eb-d361-40e1-8df1-edf86ad73cf6
        description: |
          Now that the tables have the appropriate data types we can begin adding the primary keys to each of the tables prefixed with `dim`. 

          Each table will serve the `orders_table` which will be the single source of truth for our orders. 

          Check the column header of the `orders_table` you will see all but one of the columns exist in one of our tables prefixed with `dim`.

          We need to update the columns in the `dim` tables with a primary key that matches the same column in the `orders_table`. 

          Using SQL, update the respective columns as primary key columns. 
        duration: 2

      - name: Finalising the star-based schema & adding the foreign keys to the orders table.
        id: 1909286b-1882-4709-b403-fb13e9628a2b
        description: |
          With the primary keys created in the tables prefixed with `dim` we can now create the foreign keys in the `orders_table` to reference the primary keys in the other tables.

          Use SQL to create those foreign key constraints that reference the primary keys of the other table. 

          This makes the star-based database schema complete. 
        duration: 2

    description: | 
      Develop the star-based schema of the database, ensuring that the columns are of the correct data types.
        
    id: 733a1a6e-a404-45ab-a7d7-5946530c2e13

#               _   _                _                              _  _   
#   _ __ ___   (_) | |   ___   ___  | |_    ___    _ __     ___    | || |  
#  | '_ ` _ \  | | | |  / _ \ / __| | __|  / _ \  | '_ \   / _ \   | || |_ 
#  | | | | | | | | | | |  __/ \__ \ | |_  | (_) | | | | | |  __/   |__   _|
#  |_| |_| |_| |_| |_|  \___| |___/  \__|  \___/  |_| |_|  \___|      |_|

  - name: Querying the data.
    tasks:
      - name: How many stores does the business have and in which countries?
        id: 1056d4e3-cc55-42db-a20a-ffe119265298
        description: |
          The Operations team would like to know which countries we currently operate in and which country now has the most stores.
          Perform a query on the database to get the information, it should return the following information:
          <pre>
          +----------+-----------------+
          | country  | total_no_stores |
          +----------+-----------------+
          | GB       |             142 |
          | DE       |              67 |
          | US       |              18 |
          | NULL     |               0 |
          +----------+-----------------+
          </pre>
        duration: 1
      - name: Which locations currently have the most stores?
        id: f3982823-8a76-4e6a-a4f3-646d08949091
        description: |
          The business stakeholders would like to know which locations currently have the most stores. 

          They would like to close some stores before opening more in other locations.

          Find out which locations have the most stores currently. The query should return the following:
          <pre>
          +-------------------+-----------------+
          |     locality      | total_no_stores |
          +-------------------+-----------------+
          | Westbury          |               7 |
          | Clacton-on-Sea    |               7 |
          | Cowes             |               6 |
          | Penzance          |               6 |
          | Winsford          |               6 |
          | Rutherglen        |               6 |
          | Rhosllanerchrugog |               6 |
          +-------------------+-----------------+
          </pre>
        duration: 1
      - name: Which months produce the most sales typically?
        id: bbb610d4-207f-43de-95af-ad107617357b
        description: |
          Query the database to find out which months typically have the most sales your query should return the following information:
          <pre>
          +-------------+-------+
          | total_sales | month |
          +-------------+-------+
          |   681159.77 |     1 |
          |   666127.53 |     7 |
          |   665033.65 |    12 |
          |   658066.53 |     8 |
          |   655492.08 |     5 |
          |   650981.32 |    10 |
          +-------------+-------+
          </pre> 
        duration: 3
      - name: How many sales are coming from online?
        id: 3f07a15a-5118-4d91-8e5e-b9147dd5bb89
        description: |
          The company is looking to increase its online sales. 
          
          They want to know how many sales are happening online vs offline.

          Calculate how many products were sold and the amount of sales made for online and offline purchases. 

          You should get the following information:
          <pre>
          +------------------+-------------------------+----------+
          | numbers_of_sales | product_quantity_count  | location |
          +------------------+-------------------------+----------+
          |            26957 |                  107739 | Web      |
          |            93166 |                  374047 | Offline  |
          +------------------+-------------------------+----------+
          </pre>
        duration: 3

      - name: What percentage of sales come through each type of store?
        id: 9ae96097-99af-4743-a866-f8a68b876457
        description: |
          The sales team wants to know which of the different store types is generated the most revenue so they know where to focus.

          Find out the total and percentage of sales coming from each of the different store types.

          The query should return:
          <pre>
          +-------------+-------------+---------------------+
          | store_type  | total_sales | percentage_total(%) |
          +-------------+-------------+---------------------+
          | Local       |  3478242.16 |               44.87 |
          | Web portal  |  1727455.49 |               22.44 |
          | Super Store |  1216121.41 |               15.63 |
          | Mall Kiosk  |   702607.85 |                8.96 |
          | Outlet      |   636743.14 |                8.10 |
          +-------------+-------------+---------------------+
          </pre>
        duration: 4
      - name: Which month in each year produced the most sales?
        id: c2682e3a-262c-4db0-9746-75d4200b05c4
        description: |
          The company stakeholders want assurances that the company has been doing well recently. 

          Find which months and years have had the most sales historically.

          The query should return the following information:
          <pre>
          +-------------+------+-------+
          | total_sales | year | month |
          +-------------+------+-------+
          |    30154.00 | 2006 |     5 |
          |    28047.11 | 2014 |     6 |
          |    27994.59 | 2021 |    11 |
          |    27047.16 | 1998 |     8 |
          |    26953.73 | 2019 |     1 |
          +-------------+------+-------+
          </pre>

        duration: 4
      - name: What is our staff headcount?
        id: aab13b6f-9411-4675-9fa2-04ce7f31d01a
        description: |
          The operations team would like to know the overall staff numbers in each location around the world. 
          Perform a query to determine the staff numbers in each of the countries the company sells in. 

          The query should return the values:
          <pre>
          +---------------------+--------------+
          | total_staff_numbers | country_code |
          +---------------------+--------------+
          |               12982 | GB           |
          |                6123 | DE           |
          |                1384 | US           |
          |                 325 | Web          |
          +---------------------+--------------+
          </pre>

        duration: 4
      - name: Which German store type is selling the most?
        id: 06fedb59-245c-4ee2-8e18-07dbf911def7
        description: |
          The sales team is looking to expand their territory in Germany. Determine which type of store is generating the most sales in Germany.

          The query will return:
          <pre>
          +--------------+-------------+--------------+
          | total_sales  | store_type  | country_code |
          +--------------+-------------+--------------+
          |      3216.88 | Outlet      | DE           |
          |      4299.15 | Mall Kiosk  | DE           |
          |      3259.23 | Super Store | DE           |
          |     16583.22 | Local       | DE           |
          +--------------+-------------+--------------+
          </pre>

        duration: 4
      - name: How quickly is the company making sales?
        id: 66270c55-b8e3-486e-8c83-76699a84ebc3
        description: |
         Sales would like the get an accurate metric for how quickly the company is making sales.

         Determine the average time taken between each sale grouped by year, the query should return the following information:
          <pre>
          +------+-----------------------------------------------------------------------+
          | year |                           actual_time_taken                           |
          +------+-----------------------------------------------------------------------+
          | 2022 | { "hours": 2, "minutes": 13, "seconds": 6, "milliseconds": 313.993 }  |
          | 2021 | { "hours": 2, "minutes": 11, "seconds": 56, "milliseconds": 199.548 } |
          | 2020 | { "hours": 2, "minutes": 12, "seconds": 3, "milliseconds": 535.204 }  | 
          | 2019 | { "hours": 2, "minutes": 10, "seconds": 47, "milliseconds": 79.871 }  |
          | 2018 | { "hours": 2, "minutes": 10, "seconds": 35, "milliseconds": 807.157 } |
          +------+-----------------------------------------------------------------------+
          </pre>

          Hint: You will need the SQL command `LEAD`.
        duration: 5
    description: |
      Your boss is excited that you now have the schema for the database and all the sales data is in one location. 

      Since you've done such a great job they would like you to get some up-to-date metrics from the data. 

      The business can then start making more data-driven decisions and get a better understanding of its sales. 

      In this milestone, you will be tasked with answering business questions and extracting the data from the database using SQL.
    id: c19438e6-576f-4198-a314-4844242746c5

#   __  __   _   _                _                              ____  
#  |  \/  | (_) | |   ___   ___  | |_    ___    _ __     ___    | ___| 
#  | |\/| | | | | |  / _ \ / __| | __|  / _ \  | '_ \   / _ \   |___ \ 
#  | |  | | | | | | |  __/ \__ \ | |_  | (_) | | | | | |  __/    ___) |
#  |_|  |_| |_| |_|  \___| |___/  \__|  \___/  |_| |_|  \___|   |____/ 
                                                                    
