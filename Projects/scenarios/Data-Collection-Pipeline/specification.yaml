name: Data Collection Pipeline
description: |
  An implementation of an industry grade data collection pipeline that runs scalably in the cloud. 
  It uses Python code to automatically control your browser, extract information from a website, and store it on the cloud in a data warehouses and data lake.
  The system conforms to industry best practices such as being containerised in Docker and running automated tests.
id: bd3421ec-39f8-41bc-8a9c-076b086021df
UMLDiagramUrl: https://lucid.app/documents/embeddedchart/9c6fe65c-a781-4830-b47e-5f7bd2f92b78
requires_aws: False
requires_github: True
rotate_to: 6461e8ce-1944-46dc-8b25-436ff0045280
cover_img: https://aicore-portal-public-prod-307050600709.s3.eu-west-1.amazonaws.com/images/scenarios/data-collection.png
milestones:
  - name: Set up the environment
    tasks:
      - name: Set up Github
        id: 032dcdb6-69e1-450c-96aa-819a45d6aca1
        description: |
          In this project, you'll use GitHub to track changes to your code and save them online in a GitHub repo. Hit the button on the right to automatically create a new GitHub repo. We'll tell you when you need to use it as you go through the project.
        duration: 1
        prerequisites:
          - 86076c6d-6df5-4c14-8f5e-7d3dfe661de6
          - b457ee87-74b7-414d-bbc8-43fb8acc8cc4
    description: Let's set up your dev environment to get started!
    id: 793ccbbd-0d50-40df-9fc8-42c49438da31

  - name: Decide which website you are going to collect data from
    id: e7062089-88c0-4241-ad86-045c0bd0f1a0
    description: Choose a website that contains data you are interested in, to collect and build a dataset from
    tasks:
      - name: Select a website
        id: 5030ecc3-f7fa-4286-b709-34303ef3e3f5
        description: |
          Shortlist a few of your favourite products. 
          Do they display useful info that you could collect? 

          Can you think of any data that could be used to provide business value to them if it could be understood and modelled?


          Before you pick one, you should read a few pieces of advice:

          - Don't choose something because you think it sounds impressive. Instead, choose something you're passionate about.

          - Don't worry about picking something that seems like an uncommon data application. Some of our favourite projects have been from online book stores or retail brands.

          The data collection should be the simple part of this project. At the end of the day it doesn't matter how complicated your data is - what matters is building the system around it.

          Typical mistakes here include:

          - Spending more than 2 days thinking about this step. Decide quickly and get to work. It is likely that your code can be adapted to another data source later if you change your mind.

          - Letting things get too complex, too quickly. Many people initially decide that they want to do something like scrape the whole of Amazon. Even a few hundred datapoints can be enough. Get something simple working from end to end.

          - Trying to build something that is far too general. Focus. Zone in on a particular topic you are interested in. Don't try to scrape every category from every site. Laptops will have very different attributes to clothing items on Amazon.


          Some sites are easier to work with than others. So we've shortlisted a good range of options for you to work with which we know work well.
          Pick one to get started with.

          The following list was shortlisted by the team:
          - Holidays:
            - Agoda
            - Wego
          - Entertainment:
            - Rotten Tomatoes
            - IMDB
            - SoundCloud
            - Metacritic
            - A-Z Animals
          - Ecommerce:
            - Lego
            - Square Enix
            - Ikea
            - Trustpilot
            - Ocado
            - Waterstones
            - John Lewis
          - Health and Nutrition:
            - MyProtein
            - Lamberts
            - Gorilla Mind
          - Finance:
            - Coin Market
        duration: 4

  - name: Prototype finding the individual page for each entry
    description: Find links to the many pages that contain data which you want to collect.
    id: 17023301-9c14-4fd6-825b-4e7aa86afffe
    tasks:
      - name: Create a Scraper class
        id: c2c28740-76c2-4235-9517-e4297436a105
        description: |
          This class will contain all methods used to scrape data from your chosen website. 

          Once you have created your methods to navigate the website and get the required data, you should be able to initialise an instance of this class and use it to scrape the website selected. 

          Don't worry about adding all the methods just now, you will eventually populate the class with the necessary methods.
        prerequisites:
          - ee468b4e-b54c-464f-becb-caa81721ad06 # 3 Web Scraping- HTML and BeautifulSoup
          - da4bdfbb-94cd-4438-8e42-c38d555c1147 # 4 Web Scraping - Selenium Overview
          - a43cf0f7-a434-404d-ae95-bed356f0c24f # 5 Web Scraping - Using Selenium
        duration: 6
      - name: Using Selenium, create different methods to navigate the webpage
        id: 19f8f957-e734-4f13-866f-c76898353415
        description: |

          This could include code to scroll the website, click a next button on product details page or simply navigate to the next required webpage. 

          Just think about what tasks you usually perform while browsing different websites.
        duration: 3
        prerequisites:
          - c2db5943-3f32-4cde-b8f5-396009dbd0a5 # 8 Object Oriented Programming
          - 16ab03ff-af98-41b0-8658-d3ed4c89b897 # 6 Web Scraping - Advanced Selenium
      - name: Implement a method to bypass cookies or login if required
        id: 251b0fa6-c002-4575-a866-4494e37d3883
        description: |
          Some websites will only allow you to collect the data after logging in or accepting cookie banners. 

          This can easily be automated. Once you've overcome that, move on. 

          Be careful, if there is no accept cookies button, the method will fail. 

          Think about how you can prevent the method from stopping the code running when it throws an exception.
        duration: 2
        prerequisites:
          - c2db5943-3f32-4cde-b8f5-396009dbd0a5 # 8 Object Oriented Programming

      - name: Create a method to get links to each page where the details can be found and store these in a list
        id: 8c204a6e-e894-4890-b656-c918ac06fd83
        description: |
          Find the most efficient way to do this. 
          Don't have your code type into a textbox and then press enter if you could instead just figure out the pattern of the URLs generated for each search, and create them directly. 
          This list of urls will then be iterated through to scrape all data from each page.
        duration: 3
        prerequisites:
          - c2db5943-3f32-4cde-b8f5-396009dbd0a5 # 8 Object Oriented Programming
      - name: Run main body of code only within `if __name__ == "__main__"` block
        id: 8568705a-b7a3-4dfe-982a-1d8f75e422cf
        description: |
          Within the initialiser, have your class call all of the methods implemented so far.   


          Initialise the class within the `if __name__ == "__main__"` block, so that it only runs if this file is run directly rather than on any import, and make sure it all works.
        prerequisites:
          - c2db5943-3f32-4cde-b8f5-396009dbd0a5 # 8 Object Oriented Programming
          - 9d80fd8f-d7fb-450c-913c-8d3b01027468
        duration: 3

      - name: Begin documenting your experience
        id: a6816c59-f59c-4350-a634-976fa4a819a4
        description: |
          Now that you have chosen your website and built the initial scraper class, add documentation to your README file following this [guide](https://github.com/AI-Core/ExampleDocumentation).

          Make sure to include your reasoning for choosing your website, the technologies you've used etc.

        duration: 2

  - name: Retrieve data from details page
    description: Get all of the data for each record from each corresponding page
    id: b4815cdd-40ab-4ad7-a279-eb79263d58e9
    tasks:
      - name: Create a function to retrieve text and image data from a single details page
        id: e43fe482-d349-45a9-8bbf-5c193a7e89d4
        description: |
          What is each piece of information that you might want from this page? 

          Create one method to extract the image data and another to extract all text data from the page.

          For some websites there may not be useful image data to scrape.

          If this is the case, practice retrieving image data by collecting src links for a few images on the page, this could even just be scraping the websites logo. 

          Use XPath expressions to select the element and extract the text or src from each element.

          Print out the extracted data to ensure it's in the format you want and the methods are working as expected.
        duration: 3

      - name: Extract data and store in dictionary which maps feature name to feature value
        id: 42ae22fd-90ce-48fc-a583-ae01177c49ac
        description: |
          Your dictionary should include all details for each record, its unique ID, timestamp of when it was scraped and links to any images associated with each record.
        duration: 2

      - name: Save the raw data dictionaries locally
        id: e12a7169-74db-4414-abbe-a671bdb226bd
        description: |
          At some point in the future you may realise that you wanted to do something with this data which you don't realise now. 

          Because of that, it's useful to save all of this data so that if you do need it, you've got it.

          Firstly, write code to create a folder called `raw_data` in the root of your project if it doesn't yet exist.

          Within there, create a folder with the id of the product as it's name.

          In that folder, save the dictionary in a file called `data.json`.

          Creating the product folder and data file should be done programmatically as your code runs.

          Don't spend time now scraping as much data as you can, as you'll need to do this again later. Simply make sure you can store it locally.
        duration: 3

      - name: Create a method to find image links and download images if applicable
        id: 545500c9-c524-436e-82c1-b2dd7f1b5dc8
        description: |
          Now you should be able to find the link to each image in your webpage and download them. 

          Don't worry about downloading every image from the website as it can take a lot of space on your hard drive. Just knowing how to retrieve the images will be enough. 

          You might need to look at a library the `urllib` or `requests` to download them.

          Just search stack overflow for this, and copy the function which you find.

          It's critical to save the images, not just save their link. 

          Otherwise, if the website changes the URL at which these images are stored, your entire dataset will be useless.

          Inside the folder you previously created for each datapoint, create a folder called `images`.

          Name each image as `<date>_<time>_<order of image>.<image file extension>` so `03102022_142011_1.jpg` for example, this will ensure if the image gets misplaced you know which tabular data it is associated with.

          If your data naturally falls into categories, have Python create a subfolder which has the subcategory as the name when it finds a new category.
        duration: 2
        prerequisites:
          - 6155d1da-281a-4b3c-a2c4-8dead86af8e2 # Downloading Images from URLs

      - name: Document the experience gained from this milestone
        id: 00b23b1c-2de2-4ef4-a572-df1c596443bf
        description: |
          Continuing to follow the guidelines for documentation, add your experience and insight to your README file.

          Talk about the methods you have added and the reasoning behind your approach.
        duration: 1

  - name: Documentation and testing
    description: Set up testing for all of the code that you've written so far to catch any bugs and ensure that every part works as expected.
    id: 5f71c04b-2494-4f89-ae26-682cf7aee8bb
    tasks:
      - name: Refactor and optimise current code
        id: 060d561f-b16f-41cd-a369-7d91874faa26
        description: |

          Refactoring will be a continuous and constant process, but this is the time to really scrutinise your code. 

          You can use the following list to make improvements:

          - Are your methods and variables named appropriately i.e `create_list_of_website_links()` instead of `links()`. `for element in web_element_list` instead of `for i in list`.
          - Look for areas of code being repeated. If you see repeated code this is an indication that a method/function could perform this task in these cases.
          - If you have longer methods, is it because they have multiple concerns contained within these methods? Does your method create a dictionary and upload to the cloud? Then you should have two separate methods, one for uploading and one to create the dictionary.
          - Are your methods only used within your class? Then they should be made private or protected.
          - Have you added a `if __name__ == “__main__”:` statement to your code?
          - Are your imports and `from` statements in a consistent order? For example, you can order them alphabetically, with `from` statements before `import`s. Whatever you choose, be consistent.
          - Are there nested loops within your code? This is likely not optimal. You could likely break this up into two separate loops.
          - Don't overly use `self` within your code; it can make it harder to test and debug.
          - If you have used recursion, did you implement memoisation or tabulation to make it more performant?
          - Have you used `import *` statement? Change it to the exact method or class to import.
          - Make sure all docstrings are consistent for all methods.
          - Have you added typing to your methods?
        duration: 3

      - name: Add docstrings to all functions
        id: 87e0b186-0bae-46a4-b7e2-fc24aec92dc3
        description: |
          Select a consistent format to create your docstrings. 

          Do you prefer the way Google, numpy, or Epytext format? 

          Docstring all your methods so that they are easy to understand by other users of your scraper.
        prerequisites:
          - 4997b2d1-d068-4a69-a88c-5d1b153ddfc6 # 3 Class Decorators
          - 9de4031e-e4c8-4adc-845c-1222986607d5 # 4 Docstring and Typing
        duration: 3

      - name: Create a unit test for each of your public methods
        id: 39964c88-7722-4e5b-aa60-09d015593a90
        description: |
          These tests could be as simple as checking your method returns the correct data type or as complicated as checking your are getting all the required data from each url. 

          Think about what tests you feel would make your scraper as robust as possible and implement them.
        prerequisites:
          - cb7026ee-cbb2-4943-bd9b-06fb9e43cd40 # 5 Testing
          - 54bbdd22-459e-4f2e-945b-45f523172754 # 6 Project Structure
        duration: 4

      - name: Create a file which runs all of your tests when you run it
        id: 247cb600-d67f-4625-973f-4682a922423d
        description: |

          Create a file that performs integration testing of your scraper. 

          Be careful with all the imports, as navigating through different folders is a bit of a challenge.
        duration: 2

      - name: Test your unit tests are passing for all of your public methods.
        id: 3453612f-a2a5-48d6-b3e9-6e62de096437
        description: |
          If your test is failing, think about why it might be failing. 

          Is it because the method isn't functioning as it should? 

          Or does the url you're testing against no longer exist? 

          Make changes to each piece of code until they are all passing their tests.
        duration: 1

      - name: Update your documentation
        id: d750a7fc-d0a5-4279-923c-1a1fa113b0bc
        description: Update your README file. Add documentation on unit testing and how testing works for your scraper.
        duration: 1

  - name: Containerising the scraper
    description: To take steps towards running the system on the cloud, we need to package it together in a self-contained unit - a container.

    id: 9dd2d00f-f610-4833-a543-cd0038869c04
    tasks:
      - name: Final refactoring of code
        id: 33fdf986-7ac3-4ed2-856c-e9f6c5fe8228
        duration: 2
        description: |
          Take the time now to evaluate whether you code could have been laid out better. If so, make the corresponding changes.

          You can follow this list as a guide:

          - Are your methods and variables named appropriately i.e `create_list_of_website_links()` instead of `links()`. `for element in web_element_list` instead of `for i in list`.
          - Look for areas of code being repeated. If you see repeated code this is an indication that a method/function could perform this task in these cases.
          - If you have longer methods, is it because they have multiple concerns contained within these methods? Does your method create a dictionary and upload to the cloud? Then you should have two separate methods, one for uploading and one to create the dictionary.
          - Are your methods only used within your class? Then they should be made private or protected.
          - Have you added a `if __name__ == “__main__”:` statement to your code?
          - Are your imports and `from` statements in a consistent order? For example, you can order them alphabetically, with `from` statements before `import`s. Whatever you choose, be consistent.
          - Are there nested loops within your code? This is likely not optimal. You could likely break this up into two separate loops.
          - Don't overly use `self` within your code; it can make it harder to test and debug.
          - If you have used recursion, did you implement memoisation or tabulation to make it more performant?
          - Have you used `import *` statement? Change it to the exact method or class to import.
          - Make sure all docstrings are consistent for all methods.
          - Have you added typing to your methods?

      - name: Check all tests are passing
        id: f7319100-02de-4dd9-9a8b-1355907354e3
        duration: 1
        description: |
          Run all tests and ensure they are passing.

          If not, make the necessary changes to fix them.

      - name: Run the scraper in headless mode
        id: 4d6d8d6b-92ba-4ed6-9c9b-f9bdf9e2fd27
        description: |
          Add headless as an additional flag to your webdrivers options and test the scraper works in headless mode. 

          Running your scraper in headless mode without the Graphical User Interface(GUI) will be required for the scraper to run within the a Docker container.

          You will need to add different options to your driver. 

          Take a look at this [link](https://stackoverflow.com/questions/50642308/webdriverexception-unknown-error-devtoolsactiveport-file-doesnt-exist-while-t) for more information.
        duration: 2

      - name: Create a Docker image which runs the scraper
        id: c97d05b3-599a-405c-92fd-da73b4b28b47
        description: |
          Create a Dockerfile to build your scraper image locally.

          It will need to include instructions to:

          1. Choose a base image

          2. Put everything required by the scraper within the container

          3. Install any dependencies

          4. Run the main Python file

          As well as anything else required by your implementation.

          Once you've done that, build the image.

          Also, if your scraper relies on selenium, you might want to take a look at [this file](https://aicore-files.s3.amazonaws.com/Foundations/DevOps/docker_selenium.md) to create the Dockerfile for Google Chrome.

          If you're using Firefox you may want to look at [this file](https://aicore-files.s3.amazonaws.com/Cloud-DevOps/docker_firefox.md).

          Run a container from your newly created image to ensure it is working locally before deploying to the cloud.

        prerequisites:
          - 06d96433-7b50-41bc-bbbc-227f1f70152b #  What is Docker?
          - b8873f7d-087f-479b-a2f5-b26dbf77dc7a #  Docker Essentials
          - fc11754a-1137-46ba-818f-0e03b76c1c26
          - ed1216b9-8db3-45d0-9940-e33cbc0beba1
        duration: 6

      - name: Push the container to Docker Hub
        id: 6b679d87-69b0-4165-85fe-86724a08196b
        description: |
          If you haven't already got one, create a [Dockerhub](https://hub.docker.com) account.

          Once you're happy your container is running correctly push the Docker image to Docker Hub.
        duration: 3

  - name: Set up a CI/CD pipeline for your Docker image
    description: Create a CI/CD pipeline to build and deploy your Docker image to DockerHub.
    id: 981a964b-464f-4c05-a34c-abdaf0feab7a
    tasks:
      - name: Set up the GitHub secrets
        id: 7580dd28-088d-4003-8d2d-50bac6156ce5
        description: |
          Set up the relevant GitHub secrets that contains the credentials required to push to your Dockerhub account.

          Check out which secret variables are required [here](https://docs.docker.com/ci-cd/github-actions/).
        prerequisites:
          - a58ce9ff-7145-47a0-b2b8-34f47a481f88 # 1 GitHub Actions
        duration: 5

      - name: Create the GitHub action
        id: 23c00e74-4c60-4830-9766-9075cc64d9ee
        description: |
          Create a GitHub action that is triggered on a `push` to the `main` branch of your repository. 

          The action needs to build the Docker image and push it to your Dockerhub account.
        duration: 1

      - name: Update your documentation
        id: 69ae3e2b-d017-4ca7-bec1-536dc048bc2a
        description: |
          Update your README file with what you have done for this milestone.

          Talk about CI/CD pipelines and the process you have developed.

          Go over your entire docs and make sure they read well and are clear and concise.
        duration: 1
