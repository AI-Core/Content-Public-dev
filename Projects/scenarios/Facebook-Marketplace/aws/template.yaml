AWSTemplateFormatVersion: "2010-09-09"
Description: Sample template to be used as the basis of all project templates.

Parameters:
  ProjectUserPassword:
    Type: String
    Description: The password for the project users IAM user
    NoEcho: true
  AdminUserPassword:
    Type: String
    Description: The password for the marking users IAM user
    NoEcho: true
  ProjectID:
    Type: String
    Description: The project_id of the project which should be created.
  UserID:
    Type: String
    Description: The user_id of the user for whom the project is being created.
  SourceBucketName:
    Type: String
    Description: The name of the s3 bucket containing the projects data.
  KeyPairName:
    Type: String
    Description: The keyname to use for the created ssh keypair.
    Default: project_key

Resources:
  # AdminAccess role for the account setup lambda
  AdminRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AdministratorAccess

  # User for the individual undertaking the project
  ProjectUser:
    Type: AWS::IAM::User
    Properties:
      UserName: user
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/IAMUserChangePassword
      LoginProfile:
        Password: !Ref ProjectUserPassword
        PasswordResetRequired: False

  # User for the engineer marking and verifying the project. Allows readonly access to all resources.
  AdminUser:
    Type: AWS::IAM::User
    Properties:
      UserName: admin
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AdministratorAccess
      LoginProfile:
        Password: !Ref AdminUserPassword
        PasswordResetRequired: False

  CreateEC2KeypairFn:
    Type: AWS::Lambda::Function
    Properties:
      Role: !GetAtt AdminRole.Arn
      FunctionName: CreateEC2Keypair
      Environment:
        Variables:
          SOURCE_BUCKET: !Ref SourceBucketName
          USER_ID: !Ref UserID
          PROJECT_ID: !Ref ProjectID
          KEY_NAME: !Ref KeyPairName
      Code:
        ZipFile: |
          import boto3
          import os
          import cfnresponse
          source_bucket = os.environ.get("SOURCE_BUCKET")
          project_id = os.environ.get("PROJECT_ID")
          user_id = os.environ.get("USER_ID")
          ssh_keyname = os.environ.get("KEY_NAME")
          def create_keypair(s3, ec2):
              """Creates and saves a new ec2 keypair."""
              # Generate keypair
              key_pair = ec2.create_key_pair(KeyName=ssh_keyname)
              # save keypair to s3
              s3.put_object(
                  Body=key_pair["KeyMaterial"],
                  Bucket=source_bucket,
                  Key=f"keys/{user_id}_{project_id}.pem",
              )
          def delete_keypair(s3, ec2):
              ec2.delete_key_pair(KeyName=ssh_keyname)
          def lambda_handler(event, context):
              try:
                  print("Event", event)
                  request_type = event["RequestType"]
                  s3 = boto3.client("s3")
                  ec2 = boto3.client("ec2")
                  if request_type == "Create":
                      create_keypair(s3, ec2)
                  elif request_type == "Delete":
                      delete_keypair(s3, ec2)
                  else:
                      print(f"RequestType {request_type} is not handled.")
                  responseValue = 120
                  responseData = {}
                  responseData["Data"] = responseValue
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData)
              except Exception as e:
                  print("Error", e)
                  responseData = {}
                  responseData["Data"] = str(e)
                  cfnresponse.send(event, context, cfnresponse.FAILED, responseData)
      Runtime: python3.9
      Handler: index.lambda_handler
      Timeout: 60

  CreateEC2Keypair:
    Type: AWS::CloudFormation::CustomResource
    Properties:
      ServiceToken: !GetAtt CreateEC2KeypairFn.Arn

  # This policy applies to the user who will undertake the project.
  ProjectUserPolicy:
    Type: AWS::IAM::Policy
    Properties:
      Users:
        - !Ref ProjectUser
      PolicyName: ProjectUserPolicy
      PolicyDocument:
        Version: "2012-10-17"
        # Fill in this statement with all the permissions required for the project. Any resources tagged with access:deny will be inaccessbile to the user.
        Statement:
          - Effect: Allow
            Action:
              - "lambda:*"
            Resource: "*"
          - Effect: Allow
            Action:
              - "s3:GetObject"
              - "s3:PutObject"
              - "s3:ListBucket"
            Resource:
              - !Sub arn:aws:s3:::${DataLake}
              - !Sub arn:aws:s3:::${DataLake}/*
          - Effect: Allow
            Action:
              - ec2:DescribeInstances
              - ec2:ModifyVolume
              - ec2:DescribeVolumes
            Resource: "*"
          - Effect: Allow
            Action:
              - s3:ListAllMyBuckets
            Resource: "*"
          - Effect: Allow
            Resource: "*"
            Action:
              - "rds:CreateDBInstance"
              - "rds:DescribeDBInstances"
          - Effect: Allow
            Resource:
              - !Sub arn:aws:s3:::${SourceBucketName}
              - !Sub arn:aws:s3:::${SourceBucketName}/${ProjectID}/aws/assets/*
            Action:
              - s3:ListBucket
              - s3:GetObject
          - Effect: Allow
            Resource:
              - !Sub arn:aws:s3:::${SourceBucketName}
              - !Sub arn:aws:s3:::${SourceBucketName}/*
            Action:
              - s3:ListBucket
              - s3:GetObject
  DataLake:
    Type: AWS::S3::Bucket
    DeletionPolicy: Delete
    Properties:
      AccessControl: Private

  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          Effect: Allow
          Principal:
            Service: lambda.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
        - "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
      Policies:
        - PolicyName: S3Policy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - "s3:*"
                  - "logs:*"
                Resource:
                  - !GetAtt DataLake.Arn
                  - !Sub arn:aws:s3:::${DataLake}/*
              - Effect: Allow
                Action:
                  - "s3:ListBucket"
                  - "s3:GetObject"
                Resource:
                  - !Sub arn:aws:s3:::${SourceBucketName}
                  - !Sub arn:aws:s3:::${SourceBucketName}/*

  s3BucketTransferLambda:
    Type: AWS::Lambda::Function
    Properties:
      Role: !GetAtt LambdaExecutionRole.Arn
      FunctionName: s3BucketTransfer
      Environment:
        Variables:
          OriginBucket: !Ref SourceBucketName
          DestinationBucket: !Ref DataLake
          ProjectID: !Ref ProjectID
      Code:
        ZipFile: |
          import os
          import boto3
          import json
          import cfnresponse
          dest_bucket = os.environ.get('DestinationBucket')
          origin_bucket = os.environ.get('OriginBucket')
          project_id = os.environ.get('ProjectID')
          s3 = boto3.resource('s3')

          def empty_delete_buckets(bucket_name):
              """
              Empties and deletes the bucket
              :param bucket_name:
              :param region:
              :return:
              """
              print("trying to empty the bucket")
              s3_client = boto3.client('s3')
              s3_resource = boto3.resource('s3')
              try:
                  bucket = s3_resource.Bucket(bucket_name).load()
              except ClientError:
                  print(f"bucket {bucket_name} does not exist")
                  return
              # Check if versioning is enabled
              response = s3_client.get_bucket_versioning(Bucket=bucket_name)
              status = response.get('Status','')
              if status == 'Enabled':
                  response = s3_client.put_bucket_versioning(Bucket=bucket_name,
                                                             VersioningConfiguration={'Status': 'Suspended'})
              paginator = s3_client.get_paginator('list_object_versions')
              page_iterator = paginator.paginate(
                  Bucket=bucket_name
              )
              for page in page_iterator:
                  print(page)
                  if 'DeleteMarkers' in page:
                      delete_markers = page['DeleteMarkers']
                      if delete_markers is not None:
                          for delete_marker in delete_markers:
                              key = delete_marker['Key']
                              versionId = delete_marker['VersionId']
                              s3_client.delete_object(Bucket=bucket_name, Key=key, VersionId=versionId)
                  if 'Versions' in page and page['Versions'] is not None:
                      versions = page['Versions']
                      for version in versions:
                          print(version)
                          key = version['Key']
                          versionId = version['VersionId']
                          s3_client.delete_object(Bucket=bucket_name, Key=key, VersionId=versionId)
              object_paginator = s3_client.get_paginator('list_objects_v2')
              page_iterator = object_paginator.paginate(
                  Bucket=bucket_name
              )
              for page in page_iterator:
                  if 'Contents' in page:
                      for content in page['Contents']:
                          key = content['Key']
                          s3_client.delete_object(Bucket=bucket_name, Key=content['Key'])
              print("Successfully emptied the bucket")

          def lambda_handler(event, context):
            print("request received:" + json.dumps(event))
            bucket = s3.Bucket(origin_bucket)
            new_bucket = s3.Bucket(dest_bucket)
            try:
              if event["RequestType"] == "Delete":
                empty_delete_buckets(dest_bucket)
                send_cfn_response(event, context, cfnresponse.SUCCESS)
              else:
                for obj in bucket.objects.filter(Prefix=f'{project_id}/aws/assets/'):
                  file_name = os.path.basename(obj.key)
                  s3.Object(dest_bucket, file_name).copy_from(CopySource={"Bucket": obj.bucket_name, "Key": obj.key})
                send_cfn_response(event, context, cfnresponse.SUCCESS)
            except Exception as e:
              print(e)
              send_cfn_response(event, context, cfnresponse.FAILED)

          def send_cfn_response(event, context, response_status):
            response_body = {'Status': response_status,
                              'Reason': 'Log stream name: ' + context.log_stream_name,
                              'PhysicalResourceId': context.log_stream_name,
                              'StackId': event['StackId'],
                              'RequestId': event['RequestId'],
                              'LogicalResourceId': event['LogicalResourceId'],
                              'Data': json.loads("{}")}
            cfnresponse.send(event, context, response_status, responseData=response_body)

      Runtime: python3.8
      Handler: index.lambda_handler
      Timeout: 60

  InvokeS3Transfer:
    Type: AWS::CloudFormation::CustomResource
    DependsOn:
      - DataLakeBucketPolicy
    Version: 1.0
    Properties:
      ServiceToken: !GetAtt s3BucketTransferLambda.Arn

  s3KeyTransferLambda:
    Type: AWS::Lambda::Function
    Properties:
      Role: !GetAtt LambdaExecutionRole.Arn
      FunctionName: s3KeyTransfer
      Environment:
        Variables:
          OriginBucket: !Ref SourceBucketName
          DestinationBucket: !Ref DataLake
          ProjectID: !Ref ProjectID
          UserID: !Ref UserID
      Code:
        ZipFile: |
          import os
          import boto3
          import json
          import cfnresponse
          dest_bucket = os.environ.get('DestinationBucket')
          origin_bucket = os.environ.get('OriginBucket')
          user_id = os.environ.get('UserID')
          project_id = os.environ.get('ProjectID')
          s3 = boto3.resource('s3')

          def empty_delete_buckets(bucket_name):
              """
              Empties and deletes the bucket
              :param bucket_name:
              :param region:
              :return:
              """
              print(f"trying to delete the bucket {bucket_name}")
              s3_client = boto3.client('s3')
              s3_resource = boto3.resource('s3')
              try:
                  bucket = s3_resource.Bucket(bucket_name).load()
              except ClientError:
                  print(f"bucket {bucket_name} does not exist")
                  return
              # Check if versioning is enabled
              response = s3_client.get_bucket_versioning(Bucket=bucket_name)
              status = response.get('Status','')
              if status == 'Enabled':
                  response = s3_client.put_bucket_versioning(Bucket=bucket_name,
                                                             VersioningConfiguration={'Status': 'Suspended'})
              paginator = s3_client.get_paginator('list_object_versions')
              page_iterator = paginator.paginate(
                  Bucket=bucket_name
              )
              for page in page_iterator:
                  print(page)
                  if 'DeleteMarkers' in page:
                      delete_markers = page['DeleteMarkers']
                      if delete_markers is not None:
                          for delete_marker in delete_markers:
                              key = delete_marker['Key']
                              versionId = delete_marker['VersionId']
                              s3_client.delete_object(Bucket=bucket_name, Key=key, VersionId=versionId)
                  if 'Versions' in page and page['Versions'] is not None:
                      versions = page['Versions']
                      for version in versions:
                          print(version)
                          key = version['Key']
                          versionId = version['VersionId']
                          s3_client.delete_object(Bucket=bucket_name, Key=key, VersionId=versionId)
              object_paginator = s3_client.get_paginator('list_objects_v2')
              page_iterator = object_paginator.paginate(
                  Bucket=bucket_name
              )
              for page in page_iterator:
                  if 'Contents' in page:
                      for content in page['Contents']:
                          key = content['Key']
                          s3_client.delete_object(Bucket=bucket_name, Key=content['Key'])
              print("Successfully emptied the bucket")

          def lambda_handler(event, context):
            print("request received:" + json.dumps(event))
            print(event["RequestType"])
            bucket = s3.Bucket(origin_bucket)
            new_bucket = s3.Bucket(dest_bucket)
            try:
              if event["RequestType"] == "Delete":
                empty_delete_buckets(dest_bucket)
                send_cfn_response(event, context, cfnresponse.SUCCESS)
              else:
                for obj in bucket.objects.filter(Prefix=f"keys/{user_id}_{project_id}.pem"):
                  dest_key = obj.key
                  s3.Object(dest_bucket, dest_key).copy_from(CopySource={"Bucket": obj.bucket_name, "Key": obj.key})
                send_cfn_response(event, context, cfnresponse.SUCCESS)
            except Exception as e:
              print(e)
              send_cfn_response(event, context, cfnresponse.FAILED)

          def send_cfn_response(event, context, response_status):
            response_body = {'Status': response_status,
                               'Reason': 'Log stream name: ' + context.log_stream_name,
                               'PhysicalResourceId': context.log_stream_name,
                               'StackId': event['StackId'],
                               'RequestId': event['RequestId'],
                               'LogicalResourceId': event['LogicalResourceId'],
                               'Data': json.loads("{}")}
            cfnresponse.send(event, context, response_status, responseData=response_body)
      Runtime: python3.8
      Handler: index.lambda_handler
      Timeout: 60

  InvokeKeyTransfer:
    Type: AWS::CloudFormation::CustomResource
    DependsOn:
      - DataLakeBucketPolicy
    Version: 1.0
    Properties:
      ServiceToken: !GetAtt s3KeyTransferLambda.Arn

  # Security group for all EC2 instances defined in the project. Required as an export for the parent template.
  EC2SecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: TestSecurityGroup
      GroupDescription: "Allow HTTP/HTTPS and SSH inbound and outbound traffic"
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: 0.0.0.0/0
        - IpProtocol: tcp
          FromPort: 8080
          ToPort: 8080
          CidrIp: 0.0.0.0/0

  DataLakeBucketPolicy:
    Type: AWS::S3::BucketPolicy
    DependsOn: s3BucketTransferLambda
    Properties:
      Bucket: !Ref DataLake
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Action:
              - "s3:GetObject"
              - "s3:PutObject"
            Effect: Allow
            Resource:
              - !Sub arn:aws:s3:::${DataLake}
              - !Sub arn:aws:s3:::${DataLake}/*
            Principal:
              AWS: "*"
            Condition:
              StringEquals:
                aws:PrincipalOrgID: "o-avgoew93lc"

  EC2InstanceProfile:
    Type: AWS::IAM::InstanceProfile
    DependsOn: EC2IAMPolicy
    Properties:
      Roles:
        - !Ref EC2IAMRole

  APIEC2Instance:
    DeletionPolicy: Delete
    DependsOn: EC2IAMPolicy
    Type: AWS::EC2::Instance
    Properties:
      ImageId: ami-0bf84c42e04519c85
      InstanceType: t3.micro
      KeyName: !Ref KeyPairName
      IamInstanceProfile: !Ref EC2InstanceProfile
      SecurityGroupIds:
        - !Ref EC2SecurityGroup
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash

          sudo useradd ec2-user

          sudo yum update -y

          export HOME=/home/ec2-user

          chown ec2-user /home/ec2-user

          sudo yum install -y docker &&
          sudo systemctl enable docker.service &&
          sudo systemctl start docker.service

          wget https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) &&
          sudo mv docker-compose-$(uname -s)-$(uname -m) /usr/local/bin/docker-compose &&
          sudo chmod -v +x /usr/local/bin/docker-compose

          sudo chmod 666 /var/run/docker.sock

          sudo yum remove -y awscli

          docker pull verc112/pinterest_api:latest

          docker pull verc112/pinterest_user_emulation:latest

          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip" &&
          unzip awscliv2.zip &&
          sudo ./aws/install

          aws s3 cp s3://${SourceBucketName}/${ProjectID}/datasets/images_fb.zip /home/ec2-user/images_fb.zip
          aws s3 cp s3://${SourceBucketName}/${ProjectID}/datasets/Images.csv /home/ec2-user/Images.csv
          aws s3 cp s3://${SourceBucketName}/${ProjectID}/datasets/Products.csv /home/ec2-user/Products.csv

  EC2IAMRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          Effect: Allow
          Principal:
            Service: ec2.amazonaws.com
          Action: sts:AssumeRole
      RoleName: ec2s3role

  EC2IAMPolicy:
    Type: AWS::IAM::Policy
    Properties:
      PolicyName: s3ec2policy
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Action:
              - s3:GetObject
              - s3:ListBucket
            Effect: Allow
            Resource:
              - !Sub "arn:aws:s3:::${SourceBucketName}"
              - !Sub "arn:aws:s3:::${SourceBucketName}/*"
          - Action:
              - ec2:DescribeInstances
            Effect: Allow
            Resource: "*"
          - Action:
              - s3:GetObject
              - s3:ListBucket
              - s3:PutObject
            Effect: Allow
            Resource:
              - !Sub arn:aws:s3:::${DataLake}
              - !Sub arn:aws:s3:::${DataLake}/*
      Roles:
        - !Ref EC2IAMRole
