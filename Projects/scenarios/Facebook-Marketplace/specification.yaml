name: Facebook Marketplace's Recommendation Ranking System
description: |
  Facebook Marketplace is a platform for buying and selling products on Facebook.

  This is an implementation of the system behind the marketplace, which uses AI to recommend the most relevant listings based on a personalised search query.
id: 9f1749cc-4ba8-4037-ade0-414fcadf848b
cover_img: https://aicore-portal-public-prod-307050600709.s3.eu-west-1.amazonaws.com/images/scenarios/facebook-marketplace.png
requires_aws: True
rotate_to: ec1090b6-791b-4a2a-8aa6-6575c48f29bb
requires_github: True
milestones:
  - name: Set up the environment
    tasks:
      - name: Join the project calendar
        id: f5c421aa-08f4-43e1-93fd-6319bb29e5cb
        description: |
          To keep up with all the events associated with this project, please join its respective Google Calendar using this [link](https://calendar.google.com/calendar/u/1?cid=Y190MGtyOGw2MGw4MnR0NjlkanFobGI3Y2FxOEBncm91cC5jYWxlbmRhci5nb29nbGUuY29t).
        duration: 0
      - name: Set up Github
        id: 032dcdb6-69e1-450c-96aa-819a45d6afa1
        description: |
          In this project, you'll use GitHub to track changes to your code and save them online in a GitHub repo. Hit the button on the right to automatically create a new GitHub repo. We'll tell you when you need to use it as you go through the project.
        duration: 1
        prerequisites:
          - 86076c6d-6df5-4c14-8f5e-7d3dfe661de6
          - b457ee87-74b7-414d-bbc8-43fb8acc8cc4
      - name: Set up AWS
        id: 432dcdb6-69e1-450c-96aa-819a45d6aea2
        description: |
          In this project, we'll use some services running in the cloud. Hit the button on the right to automatically create a new AWS cloud account. We'll tell you when we need to use it as we go through the project.
        duration: 1
    description: Let's set up your dev environment to get started!
    id: 493ccbbd-0d50-40df-9fc8-42c49438da51

  - name: An overview of the system
    tasks:
      - name: Watch this video
        id: 2b1259bf-f776-4d4d-8905-bbb21e2e9be7
        description: |
          Check out this video which gives an overview of the system
        duration: 1
        type: video
        video_url: https://www.youtube.com/embed/1Z5V2VrHTTA
    description: |
      Check out this video which gives an overview of the system
    id: 67e9c6a5-5792-45cc-942b-255adee7a2f9

  - name: Explore the dataset
    tasks:
      - name: Clean the tabular dataset
        id: f29a128c-3d0d-4c92-a891-2e3d989dd2fe
        description: |
          The dataset is in the EC2 instance provided when you created the AWS account from the portal. To access it, you have to access the AWS account, and ssh from your local machine.

          To ssh into it, you have to download the private key from the S3 bucket. So, go to your AWS account, look for S3, and inside you will see a bucket. Inside it, you will see different files, and one of the folders is called `keys`. Inside it, you will see a `.pem` file. Download it, and then you can ssh into the EC2 instance.

          Inside the EC2, you will find two csv files, and one folder. The two csv files are the dataset, and the folder contains the images corresponding to the products that you see in the dataset. Note: If you don't see the files, wait a few minutes, and try to list the files again. They might still be transferring.

          Important: The `.csv` file contains line terminators that are not compatible with the default pandas reader. In order to load it, you'll need to set a different line terminator when you load it in.

          The tabular dataset that you have contains information about the listing, including its price, location, and description.

          Create a file named `clean_tabular_data.py` within your repository. In this file, you will write code to clean the tabular dataset.

          Here are the data cleaning steps you need to perform:

          1.  You need to first remove all the null values in any column.

          2.  Convert the prices into a numerical format. You can do this by removing the pound sign and, for some entries, the comma.

          Now, you will also need to extract the main category of product which will be considered as a label for each product image. This will be achieved in next step.

        prerequisites:
          - 5c42936e-e9d3-4821-b6fd-1bb18adfa226 # 1 Data visualisation
          - 3fad431d-917f-4989-abc8-6d22d2961e37 # 1 Data Cleaning in Pandas
          - 2f2a6aee-4b86-4aea-bdee-962b61c3ddb4 # 2 EDA and Basic Visualisation
          - b17e0a6b-68db-4a1f-9433-04ab57d6da3a # 3 Missing Data
        duration: 15
      
      - name: Extracting Labels for Classification
        id: 03ae989e-042c-445a-bd1d-9392f939129e
        description: |
          The `Images.csv` files has a _id_ and _product id_ . The _id_ corresponds to the fliename of the image for the respective product ( _product id_  ). You have to assign a label to each image (_id_) .

          Here are a few things that you should remember when extracting and assigning labels to images : 

          1.  You have to assign a label to each category, for example "Home & Garden" is category 0, "Appliances" is category 1, etc.  in the `Products.csv` . Extract the root category from the _category_ column. Ex - root category of 'Home & Garden / Dining, Living Room Furniture / Mirrors, Clocks & Ornaments' is `Home & Garden` .In this manner, extract a  root category and assign label to everyone. This will be your `encoder`, which can be a dictionary. Thus, the output you obtain from the model will contain a list of numbers, which correspond to the categories, but you need to translate them. That's way, in addition to creating the `encoder`, you should also create a `decoder`.

          2.  The task is to extract label for every image i.e. _id_ and put in a separate column `labels` and save the dataframe as `training_data.csv`.  Hint : You need to merge the two dataframes here to find the appropriate category of every _id_ .

          You can create a notebook `sandbox.ipynb` to do this task. We will be using this notebook for all the future steps as well.

        prerequisites:
          - 5c42936e-e9d3-4821-b6fd-1bb18adfa226 # 1 Data visualisation
          - 3fad431d-917f-4989-abc8-6d22d2961e37 # 1 Data Cleaning in Pandas
          - 2f2a6aee-4b86-4aea-bdee-962b61c3ddb4 # 2 EDA and Basic Visualisation
          - b17e0a6b-68db-4a1f-9433-04ab57d6da3a # 3 Missing Data
        duration: 15

      - name: Clean the image dataset
        id: f28ce658-91ae-440f-af82-431b2e87b537
        description: |
          The image dataset that you have has contains multiple images of the products.

          Take a look at the images, do they have the same size? The same number of channels? If not, then you need to change that so that they are all consistent.

          Create a file named `clean_images.py` in your repository. 

          In this file, you will write code to clean the image dataset.

          Create a pipeline that will apply the necessary cleaning to the image dataset by defining a function called `clean_image_data`. 

          It should take in a filepath to the folder which contains the images, then clean them and save them into a new folder called "cleaned_images".

          You can use this [file](https://aicore-files.s3.amazonaws.com/MLOps/clean_images.py) to get started. make sure you make it work for your dataset and you give the desired image size.
        duration: 6
      - name: Begin documenting your experience
        id: 56901a2d-ad92-4b10-aa34-6910e6ec4cd2
        description: |
          Now that you have cleaned both the image and tabular datasets, add documentation to your README file following this [guide](https://github.com/AI-Core/ExampleDocumentation).

          Include brief descriptions on the process for cleaning the data (especially images).
        prerequisites:
          - 86076c6d-6df5-4c14-8f5e-7d3dfe661de6 # 1 Git & GitHub
        duration: 3
    description: |
      Check out the dataset and take a look at how it is structured. You will need to perform a bit of data cleaning on it.
    id: 00a1a0ee-0f85-4d75-9bea-32d9113646cb

  # - name: Create simple Machine Learning models
  #   tasks:
  #     - name: Create a simple regression model
  #       id: b250cf1f-2010-47d1-a92c-8d2134ec28a5
  #       description: |
  #         As a warm up to get started with models, you will create a simple regression model.

  #         Regression will not be used in the final system which focuses on learning product embeddings by pre-training on classification tasks.

  #         In this case, you will use the tabular dataset that you have cleaned to make a regression model that predicts the price of the product.

  #         Use the features mentioned in the first task in the first milestone: product name, product description, and location to predict the price of the product.

  #         Take a look at this [website](https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html) to apply TF-IDF (Term Frequency Inverse Document Frequency) to the text data, which will assign a weight to each word in the text.

  #         Don't worry if the metrics are not as good as you expected. On next milestones you will use neural networks to make better models.
  #       prerequisites:
  #         - 89b7fb50-9de8-46a4-bf86-d54cf316b899 # 1 Data for ML
  #         - 54d541f6-987b-461e-a64b-1c3e76e810c5 # 2 Intro to Models - Linear regression
  #         - ab8dba5e-5bd2-4e09-8d00-32ba8484596c # 3 Validation and Testing
  #         - 00640ac4-3411-4d51-8618-9d1081b4abaa # 4 Gradient based optimisation
  #         - a027ad34-762b-46d7-84b4-fe753229c620 # 5 Bias & variance
  #         - 5906e2d6-b031-4f0f-9535-f2558a7a521b # 6 Hyperparameters, Grid Search & K-Fold Cross Validation
  #         - 6d31dd18-6250-4e3b-8bd3-695a87e1bc6f # Maximum Likelihood Estimation
  #         - 8191c8fe-19a0-442b-94ec-f1f68972a869 # Evaluation Metrics - Regression
  #       duration: 8

  #     - name: Create a simple classification model
  #       id: 9ef2b18b-561c-4d32-bec9-597ed3b4b58b
  #       description: |

  #         With the image dataset you cleaned previously, you will create a simple classification model that predicts the category of each product.

  #         The column corresponding to the category has many subcategories. You don't need to go to the lowest level of the hierarchy, you can just use the most general category. For example, if the category is: "Home & Garden / Garden & Patio / Garden Building & Decoration / Plants & Flowers", just use "Home & Garden".

  #         Then, you have to perfom a assign a number to each category, so eventually you will end up with a column of numbers (Remember that ML models don't like text!).

  #         Once all the images have the same size and channels, transform the images into a numpy array.

  #         Important! You can add numpy arrays to a pandas dataframe, and their type will still be `np.array`. However, once you save it as a csv file, the type will be `str`. So, instead, save the dataframe as a pickle file.

  #         Finally, the columns will be merged to their corresponding product, so you can now pass the entire dataset to the model you will train.

  #         This model won't have great performance, but that's fine because we just want to use it as a benchmark.
  #         The next steps will be improving the model to beat the baseline set by this model.
  #       prerequisites:
  #         - b097a50c-2ad4-4cdd-8596-38d8d6efca6b # 7 Regularisation
  #         - e6991438-bdcd-4170-9e28-a9e7d8ab3569 # 1 Classification
  #         - 776fec70-0835-4e53-a655-7f83e1aec42a # 2 Multiclass classification
  #         - 525b4998-158b-4a69-940e-a1eb75b8e542 # 3 Evaluation Metrics - Classification
  #       duration: 6

  #     - name: Update your documentation
  #       id: 90cd333b-3ba1-4513-be21-644e7ce51b57
  #       description: |
  #         Update your README file, talk about why you used each model and how you used it.

  #         Report the metrics of the models you created, and what improvements can be made.
  #       duration: 1
  #   description: |
  #     Create a simple regression and classification model.

  #     The tabular dataset that you have cleaned is used to create a regression model that predicts the price of the product, and the image dataset that you have cleaned is used to create a classification model that predicts the type of the product.
  #   id: 91d89bb2-4c36-433d-ad19-d374800430c9

  - name: Create the vision model
    tasks:
      - name: Create a PyTorch Dataset
        id: fd765267-28c9-44cd-b4fc-69cd519f1796
        description: |
          In the next task you will create an image classification model, but first, you need to create a dataset that feeds entries to the model.

          First, you need to create an image dataset where each image is correctly labeled. To do so, you can use (for example) `torch.utils.data.Dataset`, simply create a class that inherits from `torch.utils.data.Dataset` and implement the `__len__` and `__getitem__` methods.

          Remember, how do you load batches of data samples in PyTorch? You can use the `torch.utils.data.DataLoader` class, which is a wrapper around a dataset. It takes care of shuffling the data and loading batches of data.

          There are a few things to remember when you create the Dataset:

          - The dataset should contain the images and the labels. You have two columns in `training_data.csv`, one for the images and one for the labels, so you should know what category correspond to what image.

          - You have to use the label assigned to each category, for example "Home & Garden" is category 0, "Appliances" is category 1, etc. This will be your `encoder`, which can be a dictionary. Thus, the output you obtain from the model will contain a list of numbers, which correspond to the categories, but you need to translate them. That's way, in addition to creating the encoder, you should also create a decoder.

          - While not necessary, you can add image transformations to your dataset. For example, rotate the images or flip them horizontally. This is called data augmentation. Optional

          A useful step here is to check the PyTorch documentation on [cutom dataset ](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files).
        prerequisites:
          - c796c6ec-59a3-4be8-acc1-478b7e57ab0b # 1 What is deep learning?
          - 1419b0b3-f91d-4a0a-b4f3-4d20342fc636 # What is PyTorch?
          - 4cdfb904-d2b7-4c5e-a2d4-7f18f08622ea # PyTorch Tensors
          - 049a81a7-59a4-47af-9ed7-d046810f4b26 # PyTorch Datasets
          - 2097084f-788c-4326-8f4f-b421c4eef1a0 # Custom PyTorch Datasets
          - 6e24dcd1-2e31-46c2-a83e-0b03eff05006 # PyTorch Transforms
          - 8014afb2-eff4-4aa6-bb88-b8082146d53c # PyTorch DataLoaders
          - cb6c9af9-af27-4a7a-b9a3-c199be79e573 # Creating a PyTorch Dataset from the classic Diabetes dataset
          - 23dbf9ff-70c9-4513-924e-ae3393c9e9f8 # Navigating the PyTorch docs
        duration: 10

      - name: Use  a Pretrained Model
        id: ee820e4d-0bba-4784-b02b-2a1510498f23
        description: |
          You can create your own CNN architecture, but you can also leverage the work that's been done by others and use a pre-trained model off the shelf.

          Instead, use transfer learning to fine tune ResNet-50 to model a CNN that can classify the images you will pass using the dataset you created in the previous task.

          To do so, replace the final linear layer of the model with another linear layer whose output size is the same as the number of categories you have.

          If you get stuck, take a look at [this tutorial ](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) where they use ResNet-18 and fine tune it

          Important! When training the model, you used the encoder to transform the categories into numbers that the model could understand. Make sure you save the decoder, so next time you perform a prediction, you can use the decoder to get the original category. Save it as a pickle file (or any format that can store a python dictionary) under the name: image_decoder.pkl.
        prerequisites:
          - 87504401-fe8f-4280-b7c3-4621e68e62aa # Building ML models in PyTorch - Linear regression example
          - 85c81f9a-d998-47f5-afc6-db35f6c49954 # Training loops
          - 2faa29c6-4070-454f-8b87-9d51b7500fae # Optimisation in PyTorch
          - e38c7147-ba61-4ff4-867e-bd2284bdd3ca # Neural network notation
          - 2a5f36fb-5bba-42c6-9da0-57ed19d2a834 # Why you need activation functions
          - 00bd7132-890d-475e-9f76-0f0ec923e9c3 # Common Activation Functions
          - 4b983bdf-a28f-4b87-bdbe-464384efac2a # Logistic regression in PyTorch
          - d661c61d-a77c-470f-9512-db06c4cdb212 # Multiclass Classification in PyTorch
          - f8589a8f-2d51-4dd0-8ee3-ffc3b421ce03 # Building our first neural network in PyTorch
          - 5f9f45bd-b0f5-4717-ab89-71f548631ab3 # What are CNNs?
          - bcf57998-417d-4349-909f-c2085b5bdea3 # The Convolution Operation
          - 789ec86b-352d-4eee-b5ae-2ee9cfc959c0 # Building CNNs in PyTorch
          - 5d6bc862-c132-4abe-8e78-d4ad9df7bbae # Autograd
          - 43b8324e-12df-4b34-ab93-f4778ffd8036 # 4 Backpropagation
        duration: 60
        
      # - name: Build a convolutional neural network
      #   id: 56b5e3cd-6e58-45b1-a93f-e8e822b82402
      #   description: |
      #     Create a convolutional network that predicts the category of each image.

      #     This neural network should be able to beat the model you created previously, but it's probably still not good enough.

      #     Don't worry about getting particularly good performance metrics before moving on. At this step, you might expect obtaining accuracy metrics of around 0.15 or 0.2, depending on the amount of layers you use and the complexity of the model. 

      #     In the next tasks, you'll refine the model significantly.
        # prerequisites:
        #   - e38c7147-ba61-4ff4-867e-bd2284bdd3ca # Neural network notation
        #   - 2a5f36fb-5bba-42c6-9da0-57ed19d2a834 # Why you need activation functions
        #   - 00bd7132-890d-475e-9f76-0f0ec923e9c3 # Common Activation Functions
        #   - 4b983bdf-a28f-4b87-bdbe-464384efac2a # Logistic regression in PyTorch
        #   - d661c61d-a77c-470f-9512-db06c4cdb212 # Multiclass Classification in PyTorch
        #   - f8589a8f-2d51-4dd0-8ee3-ffc3b421ce03 # Building our first neural network in PyTorch
        #   - 5f9f45bd-b0f5-4717-ab89-71f548631ab3 # What are CNNs?
        #   - bcf57998-417d-4349-909f-c2085b5bdea3 # The Convolution Operation
        #   - 789ec86b-352d-4eee-b5ae-2ee9cfc959c0 # Building CNNs in PyTorch
        # duration: 4

      - name: Build the training loop
        id: 9a836522-d751-47c5-8ad6-ac6a564a084f
        description: |
          Define a function called `train` which takes in a model as its first positional argument.
          It should also take in a keyword argument `epochs` for the number of epochs that it will be trained for.

          Inside that function, you will need to loop through batches of the dataset and update the model's parameters.
          You should loop through the entire dataset as many times as specified by the `epochs` parameter.

          Print the loss after every prediction to get a first idea of whether your training is working.

          This task is not about getting the model to perform well, but rather just to prove that your training loop works.
        prerequisites:
          - 87504401-fe8f-4280-b7c3-4621e68e62aa # Building ML models in PyTorch - Linear regression example
          - 85c81f9a-d998-47f5-afc6-db35f6c49954 # Training loops
          - 2faa29c6-4070-454f-8b87-9d51b7500fae # Optimisation in PyTorch

          - 5d6bc862-c132-4abe-8e78-d4ad9df7bbae # Autograd
          - 43b8324e-12df-4b34-ab93-f4778ffd8036 # 4 Backpropagation
        duration: 6

      # - name: Visualize and evaluate the performance of your model
      #   id: 54073c39-0249-4d8c-b534-170474e619fe
      #   description: |
      #     Use Tensorboard to measure the performance over time.
      #     You are performing a multi-class classification task, so you will have to use a classification metric, for example, `Cross-Entropy`.

      #     Remember to run different experiments to enhance the model, and to prevent overfitting or underfitting.
      #   duration: 4
      #   prerequisites:
      #     - fa913a8a-f4c7-4ad8-8410-13cff6a4ad85 # 1 Tensorboard

      # - name: Fine-tune a pre-trained model
      #   id: 29b6616a-bdba-4efc-8cf9-ab4d0996421d
      #   description: |
      #     You can create your own CNN architecture, but you can also leverage the work that's been done by others and use a pre-trained model off the shelf.

      #     Instead, use transfer learning to fine tune ResNet-50 to model a CNN that can classify the images you will pass using the dataset you created in the previous task.

      #     To do so, replace the final linear layer of the model with another linear layer whose output size is the same as the number of categories you have.

      #     If you get stuck, take a look at [this tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) where they use ResNet-18 and fine tune it

      #     Important! When training the model, you used the encoder to transform the categories into numbers that the model could understand. Make sure you save the decoder, so next time you perform a prediction, you can use the decoder to get the original category. Save it as a pickle file (or any format that can store a python dictionary) under the name: `image_decoder.pkl`.

      #   prerequisites:
      #     - 30d74ec3-d063-464b-8d7a-4ddbdafb62f7 # Exploring Pre-trained Models on PyTorch Hub
      #     - 6aa028ac-607f-498a-aa50-94780c8f7700 # What is Transfer Learning?
      #     - 759acea3-c699-49cb-9569-6868da32b97d # 2 Transfer Learning
      #   duration: 3

      - name: Save the weights whilst training
        id: ff789907-01c0-4363-a067-255ef2365ca0
        description: |
          Set up your training loop to save the weights of the model at the end of every epoch.

          Create a folder called `model_evaluation` and within there programmatically create a folder for each model you train. 
          It will help to include the timestamp of when this model was used in the name this folder.

          Within that model folder create a folder called `weights` and save the weights of the model in there using a filename that indicates which epoch those parameters were from.

          You might end up with billions of parameters stored in here, so it might make sense to add these files to your .gitignore file so you don't accidentally commit them.

          It might also be useful to save the metrics of the model at the end of each epoch.
        duration: 3
        prerequisites:
          - 0a6fc357-4b14-482a-a221-7f373169f163 # Saving and loading PyTorch models

      - name: Finetune the pretrained model
        id: 2c95cb38-6a27-44ad-b0a5-88128cf11b84
        description: |
          Once you are able to attain beyond 30% accuracy in the testing set, it's time to fine tune the pretrained model. 
          
          Remember that we were using a pretrained model with already defined weights. In this step, we will unfreeze the last 2 layers of the pretrained model and retrain our entire model. 
          
          This will just make our model more apt for our dataset. This way we are just changing the way how we pick the high level features.

          `NOTE`: Make sure you train your model long enough.
        duration: 1
        prerequisites:
          - 0a6fc357-4b14-482a-a221-7f373169f163 # Saving and loading PyTorch models

      - name: Create feature extraction model 
        id: 1567e8df-6605-426c-a26c-4ef7da711ce1
        description: |
          Later in the project, we are building an image search using FAISS. We will need features from different images and then do a search on these features and return their respective images in case of a similarity match. To achieve this, we need a feature extraction model.

          We trained a classification model in the last task, now we will convert it into a feature extraction model. All we need to do is remove the last few layers ( FC Layers ) from the model . Design the last layer of this feature extraction model so that it should have 1000 Neurons. 

          Create a new folder called final_models next to the other model_evaluation folder.

          Save the final model weights with the name image_model.pt within this folder.
        duration: 6

      - name: Create an image processor script
        id: 89fdb520-4a13-4239-bc01-dc9b719e0899
        description: |
          Create a script called `image_processor.py` that will take in an image and will apply the transformations needed to be fed to the model.

          Remember that when you trained the model, you passed a batch of images, so the input had the following shape: `(batch_size, n_channels, height, width)`. However, now, you will be passing in a single image, so the input shape will be: `(n_channels, height, width)`.

          Thus, you will need to add a dimension to the beginning of the image to make it a batch of size 1, so that the model can process it.

          Make sure that, if you applied transformations in the Dataset you created to train the model, you have to apply the same transformations to the image you are passing in.

        duration: 6

      - name: Update your documentation
        id: 0a706cb1-294e-4b05-b692-7b1b9b84a615
        description: |
          Add a section to your README file that describes how you used the model, and its performance.

          Include screenshots of the Tensorboard visualization, that always draws attention!
        duration: 1
        prerequisites:
          - 86076c6d-6df5-4c14-8f5e-7d3dfe661de6 # Git
          - b457ee87-74b7-414d-bbc8-43fb8acc8cc4 # GitHub
    description: |
      Train a CNN to classify the category of each product from their images and then use it as a feature extraction model.

    id: afa5b769-40f6-4343-bc9f-0c3432e72c54

  - name: Create FAISS search index
    description: |
      Build a FAISS search index using image embeddings of the dataset to perform vector similarity search.  

    id: a6e77a8e-9e78-4012-8e5a-a87ae25c3174
    tasks:
      - name: Use feature extraction model to get image embeddings
        id: 4c917f42-84c9-43dd-a105-0e1508c86cfe
        description: |
          Use the Feature extraction model to extract feature or `image embeddings` for every image in the training dataset.

          These `image embeddings` will be later used to find to find similar images of a target image.

          Create a dictionary where every key is `image_id` and value is the `image embedding` . Save this dictionary as a json `image_embeddings.json` .
        duration: 6

      - name: Build the FAISS search index
        id: c20154c3-8112-48ea-a147-ba8f0b5c9d72
        description: |
          In this steps, we will load the saved dictionary in the previous step and fit into the Faiss search. Faiss needs two things for vector search:

          1.  Respective Index for the Vector data

          2.  Vector data (image embeddings)

          We will fit the Faiss model with `image_id` which will be the index and respective `image embedding`. Once done we will use this model to perform vector search.

          The idea is to extract features of any given image and search for similar images using FAISS similarity search.
        duration: 10
      
      - name: Update your documentation
        id: 4db8d670-9d00-40a3-853a-271a83c9b48e
        description: |
          Add a section to your README file that describes how you used the model, and its performance.
          
          Include screenshots of the Tensorboard visualization, that always draws attention!

        duration: 5


#   - name: Create the text understanding model
#     tasks:
#       - name: Create text embeddings
#         id: 3de17a3d-9def-4ba9-a0a9-1169a68c7ef8
#         description: |
#           Turn your text data into vector embeddings by creating a model that creates the embeddings.

  #           To do this, train your own word2vec model. Take a good read of [this article](https://neptune.ai/blog/word-embeddings-guide) and [this one](https://jalammar.github.io/illustrated-word2vec/) to get a better understanding of how word embeddings work.

  #           Alternatively, you can use BERT to create the embeddings, check this [demo](https://youtu.be/WOr2qhd1t-A) to learn how to create embeddings using BERT.

  #           You can decide the length of the embeddings, but remember that the longer the embeddings, the more computationally expensive it will be to train the model.

  #           Don't train the model yet! Simply get familiar on how to create the embeddings and how to use them.
  #         duration: 6

  #       - name: Create a PyTorch Dataset for the text data
  #         id: 4a0920fc-0b52-4e20-86ae-099bef7f3159
  #         description: |
  #           Create a PyTorch Dataset that takes the sentences that you embedded, and creates a dataset that can be used to train a model.

  #           Then, you can use Torch DataLoader to create batches of data for training. Again, you can check the [demo](https://youtu.be/WOr2qhd1t-A) to know how to create the Dataset and the Dataloader.

  #           Similar to the image Dataset, at this step you need an encoder and decoder to know how to turn the text data into numerical data. Make sure you create them as attributes

  #           The Facebook team padded each sequence of text with zeros so that they were are all the same length. This is because they will each produce a vector of the same size when convolved over by a CNN, which is the next step.
  #         duration: 2

  #       - name: Perform classification based on the text data
  #         id: 355bb4a8-782d-43d0-98b3-c5f3f7221746
  #         description: |
  #           Train a CNN to classify the product category by convolving over the text embeddings you created.

  #           The Facebook team used a simple CNN with a window size of 2, so that it convolved over the embeddings of only 2 words at a time.

  #           This means that the size of your convolution kernel should have the shape: `embedding_size` x 2.

  #           When you train the model, don't forget to also save your decoder in a pickle file. You can name it `text_decoder.pkl`.
  #         duration: 2

  #       # - name: Visualize the performance of your model
  #       #   id: 8df8fea8-9cfd-44f9-87df-96195ff5aa41
  #       #   description: |
  #       #     Use Tensorboard to measure your performance over time. Remember to run different experiments to enhance the model, and to prevent overfitting or underfitting.
  #       #   duration: 2
  #       #   prerequisites:
  #       #     - fa913a8a-f4c7-4ad8-8410-13cff6a4ad85 # 1 Tensorboard

  #       - name: Create a text processor script
  #         id: 5e112971-d6c7-46d8-ac3b-c1a1733aa431
  #         description: |
  #           Create a script called `text_processor.py` that will take in a text and will apply the transformations needed to be fed to the model.

  #           In this case, what you sent to the model was the embedded sentences, so the input shape was `(batch_size, embedding_dimension, max_length)`. However, now, you will be passing in a single sentence, so the input shape will be: `(embedding_dimension, max_length)`

  #           Depending on how you created the embeddings, you might need to add a dimension to the beginning of the text to make it a batch of size 1, so that the model can process it.
  #         duration: 1

  #       - name: Save the weights
  #         id: 7502f807-6962-437d-af1b-f1421e107a84
  #         description: |
  #           Once you are happy with the performance of your model, save the model to a file, so you can load it eventually.

  #           Save the `state_dict` of the model under the name `text_model.pt` by running `torch.save(model.state_dict(), 'text_model.pt')` in a new file.
  #         duration: 1
  #         prerequisites:
  #           - 0a6fc357-4b14-482a-a221-7f373169f163 # Saving and loading PyTorch models

  #       - name: Update your documentation
  #         id: eb7cf6e2-c16c-462c-88a3-a36071b1f516
  #         description: |
  #           Add a section to your README file that describes how you used the model, and its performance.
  #         duration: 2

  #     description: |
  #       Train a CNN to classify text data of the products.
  #     id: caabdc05-c3f2-42eb-b54f-e7041ba1e9d6

  #   - name: Combine the models
  #     tasks:
  #       - name: Create a PyTorch Dataset which creates an instance of both the image and text datasets
  #         id: 68f6aa6c-602f-4e1d-b9ce-2a7ea6327812
  #         description: |
  #           On previous tasks, you created separate Datasets for the image and text data.

  #           Now, they must be combined into a single multimodal PyTorch Dataset.

  #           Make sure that each image is associated with the correct text data.

  #           When indexed, the Dataset should return a tuple of (image, embedded text sequence) as the features, and the category classification as the target.

  #           The overall output should have the format: `((image, embedded text sequence), category classification)`.

  #           Similar to the image and text Datasets, you will need to create an encoder and decoder to know how to turn the text data into numerical data. Make sure you create them as attributes.

  #           Once you have created the Dataset, you can use it on a dataloader to train your model in next tasks.
  #         duration: 2

  #       - name: Add a linear layer onto the end of the model which combines the two output vectors
  #         id: d961adf6-bc04-4ca2-9d23-ac2a65f2000b
  #         description: |
  #           Create a model that concatenates the layers of the image and text models. Name this model `CombinedModel`

  #           The initialiser should instantiate an instance of the image model you trained earlier, and an instance of the text model you trained earlier. It should also initialise a linear layer that will combine those models' concatenated outputs to make a classification.

  #           Alternatively, you can recreate both structures within `CombinedModel` itself as two different attributes, as well as the linear layer that will process the outputs of both models.

  #           Regardless of how you create the architecture, the outputs of the image architecture and the text architecture should be different from the output that was generated by each model alone.

  #           Within the model's forward method, it should unpack the image and the text embeddings and pass them through the separate models you created earlier.

  #           Then it should concatenate the embeddings generated by each of these models, and pass that concatenated vector through the new linear layer.

  #           That new linear layer should have an input size equal to the sum of the size of the vectors output from the two previous models, and its output size should be equal to the number of categories.
  #         duration: 1

  #       - name: Train and save the model
  #         id: 7c3b9e4d-4e75-41c9-ae54-a1e2fbdec5da
  #         description: |
  #           Train the model you created in the previous task. Did the accuracy improve?

  #           Remember to note down the metrics and make screenshots to the Tensorboard plots to use them in the documentation.

  #           Save the model under the name `combined_model.pt` within the `final_models` folder. Also, when training the model, make sure to save the decoder as `combined_model.pkl`
  #         duration: 3

  #       - name: Update your documentation
  #         id: f44acce6-e8e0-48e8-8bda-5bc05198e718
  #         description: |
  #           Add a section to your README file that describes how you combined the models, and the embeddings you obtain out of the combined models
  #         duration: 2
  #     description: |
  #       Concatenate some of the image and text model activations together, so that they contain information about both modes of data. Then use this combined representation to train a model that can classify the product category.
  #     id: 1bb06d6e-4483-488a-9417-4a007a2e61af

  # - name: Building the search index
  #   description: |
  #     A search index is a set of dense vectors representing each item in a dataset. Now it's time to use your multimodal model to produce these vectors
  #   tasks:
  #       - name:
  #         description: |

  #         id: 942e6e9c-c6f0-4c86-843d-c99e2615aca1
  #         duration: 2
  #       - name:
  #         description: |

  #         id: 40c52281-f1a2-4e4f-9bcf-4176fd1e8e68
  #         duration: 2
  #   duration: 2
  #   id: bd04b9a2-b44a-4dde-8b23-9501d5421448

  # - name: Using FAISS to find similar matches locally
  #   tasks:
  #     - name: Load your search index vectors into FAISS
  #       description: |
  #         Take a look at the description of the milestone to see how to install FAISS on your computer.

  #         Then, load your embeddings into the FAISS library to create the index.
  #       id: 4de7267e-ce4e-4efe-b9fc-addc56940ff1
  #       duration: 4
  #     - name: Test it out for some vectors you have
  #       description: |
  #         Using the demographic information on this link and the index you generated in the previous task, give recommendation to the people in the dataset.
  #       id: 56617ee3-02cd-46c5-9a9c-441f07a46cc0
  #       duration: 3
  #   description: |
  #     For this platform, Facebook used FAISS, which is a system for similarity search.

  #     It firstly clusters the data obtained from the output of your model and then it can perform a similarity search with the demographic information of the users that want to buy an article through the Maketplace

  #     Read about FAISS [here](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)
  #   duration: 7
  #   id: 9cbc63dd-f1a4-4b6f-b1bc-bcfc748e8795

  # - name: Store the dataset in cloud storage
  #   description: |
  #     Decide on a storage format that makes sense for
  #   tasks:
  #       - name:
  #         description:
  #         id: b2379172-bdbd-4cc6-a7ba-dddf132216bb
  #   duration:
  #   id: 6faa0a70-80aa-4b5c-bec9-d2f9f4c36a5a

  # - name: Serving recommendations through an API
  #   description: |
  #     Create an API using FastAPI that returns the output of the FAISS model you created
  #   tasks:
  #     - name: Build the FastAPI and run it locally
  #       description: |
  #         It should have just one method, which you can define a response to GET requests on and name `/search`
  #       id: 297b7431-4b3c-4d37-8c24-289706785d8b
  #       duration: 4
  #     - name: Return the product details along with the API response
  #       description: |
  #         Of course your user doesn't just want a vector, they want a prouct, with images and a description. So you need to return all of this information with your API response.
  #         _Note: You should not actually return the image data, this will much more efficiently be done through a CDN which the client requests directly. Instead, you should send the S3 URIs for each image as part of the response _
  #       id: b9b87972-241b-400a-ae25-d6e1079aa0d7
  #       duration: 3
  #   duration: 7
  #   id: c972e067-139a-444c-bda8-84e197a84d0c

  - name: Configure and deploy the model serving API
    description: |
      Deploy the model serving API to a cloud provider so that it can be accessed by the client
    id: 0aae2ec5-88fb-4a5b-a608-75dfdbe25a6a
    tasks:
      - name: Setup the API endpoints
        id: e621b858-6c53-41c3-ae9d-7b755ef996d0
        description: |
          In the S3 bucket that was provided when you created the project, there are a few files that will help you create the API endpoints.

          - `api_template.py` is a template for the API code. Use this as the basis for your API code.

          - `requirements.txt` contains the dependencies for the API code. Use this to create a `requirements.txt` file for your API code. If you are using a library that is not in that file, add it to the file.

          - `Dockerfile` is a the Dockerfile that will be used to build the Docker image for the API.

          - `docker-compose.yml` is the docker-compose file that will be used to run the API in a Docker container.

          You can download the images into your EC2 instance using the `aws s3 cp` command. For example, to download the `api_template.py` file, you can run the following command:

          `aws s3 cp s3://<bucket-name>/api_template.py <local-file-name>`

          The `api_template.py`` file contains the shell for a FastAPI application with two endpoints, one for each model (image, FAISS).

          Complete each endpoint by creating the correct Pydantic data classes for each endpoint's request body as well as adding the relevant methods from each model to run predicitons on the relevant request body (images and other params) and then return the predictions/search result indexes.

          As mentioned, the `api_template.py` file is just a template that you have to fill in. Before defining the methods, you should initialize the models you want to use by defining the same architecture you used when training each model. Then, initialize an instance of the model, and load the weights you obtained from training and exporting a `.pt` file.

          Also, you should import the image_processing module you made earlier so that the API can make the necessary transformation to the inputs it receives.

          Additionally, load the decoders you saved as `pkl` files from previous tasks and the dictionary/json.

          You can check this [demo ](https://youtu.be/nBHwuni85C4) to know how to add methods to the classes you are defining in the API and how to use the decoder.
        
        prerequisites:
          - f0379838-450b-4d9e-8cf2-4b9178abe37e #  S3 and boto3
          - 91f4433e-c2e0-43eb-8052-9ee5b3127d31 #  Intro to FastAPI
          - d2c2ac9d-9d48-4c88-8cec-29169335fed1 #  Routing and Pydantic Model

        duration: 3

      - name: Deploying the API
        id: 0ee79da8-4c73-4bd8-a1f3-e1f530822b98
        description: |
          Now you have setup the endpoints, it's time to deploy!

          In the same API folder are three more files: dockerfile, docker-compose.yml and requirements.txt

          If you have added any additional libraries then please update requirements.txt with the relevant packages

          Before building your docker image, once you have finished editing the api.py file, use the following command to copy the api.py file to the app volume directory `sudo cp ~/api/api.py ~/app/api.py`

          You must copy the completed api.py file to the app volume directory, otherwise deploying the docker container will fail

          Make sure to also copy your model python file,model pickle binary adn other related files/objects to the same volume directory (`~/app`)

          Deploy your API using the following command: `docker-compose -d --env-file .env up`, the `-d` backgrounds the process (or detached mode), so to make sure it is working you can use `docker ps` to list all docker processes

          This will build your image before deploying it on port 8080 using docker compose. 

        prerequisites:
          - 06d96433-7b50-41bc-bbbc-227f1f70152b #  What is Docker?
          - b8873f7d-087f-479b-a2f5-b26dbf77dc7a #  Docker Essentials
          - fc11754a-1137-46ba-818f-0e03b76c1c26
          - ed1216b9-8db3-45d0-9940-e33cbc0beba1
        duration: 3

      - name: Test the API
        id: 2fce8115-d345-4b9c-820a-a0c52408da92
        description: |
          Now the API is deployed, send some requests to 2 endpoints with the relevant data and test that it is now working and returning *image embeddings* and __search results indexes__.

          You can also access a GUI test endpoint for the api by visting `http://<your ec2 ipv4 address>:8080/docs`

          Any updates that you make to the api.py file in the app volume directory will persist to the docker container however you must restart it to see the changes
        prerequisites:
          - bf5e1b7b-0556-4417-809a-85e544ba2e0e # 1 APIs and requests
        duration: 1

      - name: Update your documentation
        id: 40645b74-98b8-45ef-9865-e4d82d56dd5a
        description: |
          Add a section to your README file that describes how you built the API, how FastAPI works as well as model serving.

          Also, add some screenshots of the API working and it returning the correct response body.
        duration: 2
  # - name: Set up Kubeflow
  #   tasks:
  #     - name: Setup the EKS cluster
  #       id: 297b7431-4b3c-4d37-8c24-289706785d8b
  #       description: |
  #         Follow the instructions in the notebook (Kubeflow Setup) to set up the dependencies for the EC2 instance or your local machine, depending on whether you want to use the provided infrastructure or your own cluster.

  #         You will see that you can use the infrastructure provided in this scenario, or you can create your own in case you want to experiment. Remember that creating your own cluster will incur charges, so if you decide to go on your own, remember to close the cluster when you're done.

  #         Kubeflow is built on Kubernetes. You don't have to learn about Kubernetes, but in case you need want to know about it, you can take a look at the content to understand how Kubernetes works behind the scenes.
  #       prerequisites:
  #         - 3ce6effd-24a0-4bfb-b99e-91076ad6c60f # 1 Intro to Kubeflow
  #         - c88e2a40-3bb0-4898-8d7d-b8343c077afa # 2 Kubeflow Setup
  #         - f85e0465-0c0a-412f-bdf6-cfcd582f3fa9 # 1 Kubernetes Basics
  #         - 6b722428-500b-4d29-8894-06a1141fb857 # 2 Kubernetes Workloads
  #         - 92324888-b4a5-41f1-bd52-f159ef357212 # 3 Kubernetes Networking
  #         - b2e6956a-cd30-42f8-839e-db6f109cdfc1 # 4 Kubernetes Storage & StatefulSets
  #       duration: 3

  #     - name: Create the cluster and install Kubeflow
  #       id: b9b87972-241b-400a-ae25-d6e1079aa0d7
  #       description: |
  #         Follow the instructions in the notebook (Kubeflow Setup) to create the cluster and install Kubeflow.
  #       duration: 1
  #     - name: Create the Load Balancer
  #       id: 1317eee2-5651-4b64-a4af-e80df8c7e612
  #       description: |
  #         Follow the instructions in the notebook (Kubeflow Setup) to create the Load Balancer that will allow you to access the Kubeflow dashboard.

  #         The load balancer usually comes by default if you followed the instructions in for setting up the EKS cluster from your local machine.
  #       duration: 1
  #   description: |
  #     Create the Kubeflow architecture on AWS.
  #   duration: 3
  #   id: c972e067-139a-444c-bda8-84e197a84d0c

  # - name: Deploy the model to KubeFlow
  #   tasks:
  #     - name: Develop the section of the pipeline that cleans the data
  #       id: c1132f4a-4a3b-4c4e-9064-f2ac95db1d89
  #       description: |
  #         Create the pipeline that contains all the steps to clean the data you defined previously. Use the same files you created earlier in milestone 1 (`clean_tabular.py`, `clean_images.py`)

  #         Base this task on examples that you can find in the notebook, or on this [website](https://www.kubeflow.org/docs/components/pipelines/introduction/)
  #       prerequisites:
  #         - a024aba1-b258-460b-aad6-3856ecd44726 # 3 Kubeflow Pipelines
  #       duration: 5
  #     - name: Develop the section of the pipeline that trains the model
  #       id: 1cdad350-c2a1-4c55-a5c3-14d33ee92e50
  #       description: |
  #         Create the pipeline that contains the model you created earlier. Use the combined model you created (`combined_model.py`).

  #         Base this task on examples that you can find on the notebook, or on this [website](https://www.kubeflow.org/docs/components/pipelines/introduction/)
  #       duration: 4
  #     - name: Serve the model
  #       id: 595495fc-5999-431a-9149-450c9bf09714
  #       description: |
  #         Attention! This task might not be done using the EC2 instance we provided due to protocol reasons.

  #         However, this can be done with no issues if you deployed the model using your own EKS cluster and using KFserving.

  #         Check out this (video)[https://www.youtube.com/watch?v=lj_X2ND2BBI] to learn more about KFserving
  #       duration: 4

  #     - name: Update your documentation
  #       id: e7640f03-602c-48b7-beb3-81b8f709fad0
  #       description: |
  #         Add a section to your README file that describes how you deployed the model on Kubeflow

  #         Also, add some screenshots of the pipeline that you created.
  #       duration: 1
  #   description: |
  #     Deploy the model on Kubeflow and make it available to the public.
  #   duration: 12
  #   id: 02c6691d-46c8-4c9f-8e4b-b956a2d170ec
# - name: Containerising the application
#   description: |
#     Now it's time to wrap up the API, the model and FAISS into a Docker container which can serve predictions from the cloud
#   tasks:
#       - name: Create the Docker container
#         description: |
#           Containerise your API and test it locally. Remember to expose the corresponding port to see if the API works!
#         id: 7041673c-c332-4cff-8ee4-ad78289e8460
#         duration: 3
#       - name: Upload your container to an EC2 instance
#         description: |
#           Push your container to Dockerhub, spin up an EC2 instance, pull the image inside the instance, and edit the corresponding inbound rules so your application can be used anywhere
#         id: 1591e573-12fe-4fdf-8d02-5b458bad928a
#         duration: 3
#       - name: Implement a CI/CD pipeline
#         description: |
#           Your index needs to be re-generated eventually. Instead of generating it on the container, you can do it locally, and make a push to dockerhub.

#           The new image contains the updated search index, so it can serve requests more accurately.

#           Create a github action to automate this process without having to manually trigger the pipeline.

#           _Note: In industry you can use other tools such as Jenkins to create this process. Jenkins make everything easier, but it doens't show you how everything works behind the scences, and most importantly, it's not free!_
#         duration: 3
#         id: 1eddbc0f-4c44-42cd-ab5f-f3a95aec48ca
#   duration: 9
#   id: 19fa4c1a-106c-4217-9b11-709621267012
