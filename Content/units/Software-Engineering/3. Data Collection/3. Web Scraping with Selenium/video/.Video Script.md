# Webscraping

## Setup

### What do I need open?
- Chrome
    - You are going to check Chrome's version
    - You are going to download chromedriver
- Terminal
    - We will move chromedriver to PATH
- VSCode
- Keynote
    - Webscraping.key
    
- OBS

### What recording scenes do I need open?

- Screencapture of the mentioned disposition with your webcam capture in the corner
- Webcam capture in full screen
- When using Selenium, open VSCode and the Chrome window on the same scene. Let's call it Selenium Disposition

## Script

- __`[START SCENE] webcam in full screen`__
    - The vast amount of data you find online is a great resource for any field of research or for your personal interests
    - It would be helpful if we could systematize the collection of this data without fetching it manually
    - This task is usually known as webscraping and luckily, Python has libraries that help us with that task
    - Before diving into it, it is important to understand how a website is built
        - So that we can access to specific parts of its structure

- __`[SWITCH SCENE] OPEN CHROME`__
    - When you visit a website using your browser, you see a nice disposition of its contents
        - For example, when you use Google Chrome to visit "www.python.org" __`[VISIT www.python.org]`__
        - What happens behind the scenes is that, your computer sends a GET request to "python.org"
        - If the request is successful, the server returns an HTML code and Google Chrome renders the code to display 
    - We can take a look at the rendered HTML structure if we right click on any part of the website and click "Inspect" __`[RIGHT CLICK AND CLICK INSPECT]`__
        - This opens the Browser's developer tools, which is currently showing the HTML architecture. 
    - This HTML architecture is telling the browser how to display the content of the website
    - __`[POINT TO DIFFERENT PARTS OF THE HTML CODE]`__ If I hover over different things in this HTML code, its corresponding object in the website is highlighted
    - And the other way around can also be done. Once the developer tool window is open, I can right-click on an object in the website, and select "Inspect" __`[RIGHT-CLICK ON THE SEARCH BAR AND CLICK INSPECT]`__

    <p align=center><img src=.images/search_bar.png width=400></p>

    - This shows the position of the search bar in the HTML code
    - One thing to take into account about the HTML code
        - Notice that each element has a tag, for example, div, span, input. 
        - Each tag in turn might have different attributes, such as id, class, href
        - This will help us finding the right elements when we have to read the HTML code from Python
    - We have two different ways to access the elements in a HTML code,
        - One is by referring to the right tag
        - And another is by getting the XPath of the element
    - So, before digging in, let's take a look at XPaths and how the HTML structure works

<br>

- __`[SWITCH SCENE] Open Keynote with the Webscraping presentation`__
    
    - Let's imagine we have this HTML structure
        - We have the main body of the code __`[POINT TO THE body tag]`__
        - And INSIDE the body, we have two division tags, __`[POINT TO THE div tags]`__
        - Then, the second division tag, in turn, contains more tags, one division tag __`[POINT TO THE INNER DIV tag]`__
        - and three paragraph tags __`[POINT TO THE THREE P tags]`__
    - This inner division tag and the three paragraph tags are considered as siblings
    - And their parent is this division tag over here __`[POINT TO THE SECOND DIV TAG]`__
    - However, they are not DIRECT children of the body tag, because they are two levels below
    - Body has two direct children, which correspond to these DIVISION TAG __`[POINT TO THE DIV TAGS]`__

<br>

- __`[NEXT SLIDE] ABSOLUTE XPATH SLIDE (Part 1)`__
    - Taking that into account, we can start explaining XPaths. 
    - XPath can be used to navigate through elements and attributes in an HTML document.
    - By default, when we read an HTML code, we start from the top
        - And then, we can start going down.
        - If we add one single slash, that means that we are going to look at direct children
            - For example, __`[POINT TO body/div]`__, this XPath only points to direct children of body 
            - So, it will only point to the first two Division tags
        - On the other hand, two slashes means that it can be any descendant, no matter how many levels below
            - For example, __`[POINT TO //div]`__, this XPath points to any division tag, regardless of its level
        
<br>

- __`[NEXT SLIDE] ABSOLUTE XPATH SLIDE (Part 2)`__
    - You can be more specific when describing the XPath
    - As mentioned earlier, tags can also contain attributes such as id
    - So, for example, we can point to multiple paragraph tags using this XPath __`[POINT TO //p]`__
    - But, if we want to get more specific, we can include, in the XPath, an attribute that is included in the tag
        - For example, if we say that we want to point to the tag whose id attribute is equal to "par_1"
        - We can do it by pointing to any paragraph tag with "slash-slash-p" followed by the name of the attribute and its value

<br>

- __`[NEXT SLIDE] RELATIVE XPATH SLIDE`__
    - When specifying the XPath, we don't always have to start from the beginning 
    - If we are already pointing to an element in the HTML structure, we can use it as a starting point
    - For instance, let's say that we are already pointing to this Division tag __`[POINT TO THE THIRD div TAG]`__
        - If, from this element, we define a new XPath to define all the direct children
        - We only need to add a dot at the beginning of the XPath

<br>

- __`[SWITCH SCENE] webcam in full screen`__
    - So, now that we know how to navigate through the HTML code, how do apply it in Python?
    - As we know, when you make a GET request to a website, this will return the HTML code
    - But that is usually not enough to scrape the data in an automatic fashion
    - Luckily, Python has libraries that allows us to:
        - firstly, navigate through the HTML code of a website
        - and secondly, extract information such as the text or the attributes in an element or set of elements
    - The first library we'll see is BeautifulSoup, which, more than a scraper, is a parser
    - And the second library we'll see is Selenium, which is a library that allows us to automate actions in a web browser
    - Let's start with BeautifulSoup. 
        - We will see an example on how to see this library with a practical example where we will extract the price of graphic cards

<br>

- __`[SWITCH SCENE] In VSCode, open 'webscraping.ipynb'`__
    - If you haven't already, you can install BeautifulSoup running pip install BeautifulSoup4
    - A source of error here is that usually people forget about the number "4" at the end of the name

<br>

- __`[NEXT CELL] import requests`__
    - Let's see how to use it for extracting prices of graphic cards
        - We will take a look at the Currys' website to check the prices
    - First of all, if we make a GET request, we will get the whole HTML code __`[RUN THE CELL]`__
    - But this is not very convenient __`[CLICK ON THE THREE DOTS ON THE LEFT HAND SIDE AND CLEAR OUTPUT]`__
    - Luckily, BeautifulSoup can help us for parsing the HTML tree and navigate through it

<br>

- __`[NEXT CELL] from bs4 import BeautifulSoup`__
    - BeautifulSoup uses the text from requests and parses it 
    - First, it can be used to print out the HTML code in a much nicer way
    - Additionally, it identifies the tags in the code and allows us to easily navigate through it
    - So, let's create a soup out of the text from the response __`[RUN THE CELL]`__
    - Much better!
    - Ok, so now we have the HTML code, and we know how to navigate through it
        - Let's see how to get the prices of the graphic cards
    - __`[TO AVOID MOVING BETWEEN VSCODE AND CHROME, I WILL INCLUDE SCREENSHOOTS IN THE NOTEBOOK. LET ME KNOW IF YOU PREFER TO ACTUALLY SWITCH TO CHROME]`__

<br>

- __`[NEXT CELL] Image with the HTML code`__
    - We can visit the website and right click anywhere in the page and click on Inspect
    - If we do the same when the Inpector is open, we can see the position of the element we are looking for in the HTML code
    - So, in this case, first we right click anywhere in the window __`[RIGHT CLICK ANYWHERE]`__
    - And then, once the inspector is open, you can right click on the container that has all the list of all the items
    - Notice that all the elements containing a graphic card have a similar pattern:
        - They are in a Division tag, and the class starts with "ProductListItem"
    - So, we can use BeautifulSoup to access all the elements with those characteristics

<br>

- __`[NEXT CELL] from bs4 import BeautifulSoup`__
    - BeautifulSoup objects have a method called find_all, which accepts the name of the tag
    - And the attribute of the tags we want to find in the form of a dictionary
    - So, we will find div tags whose class attribute is "ProductListItem__"
    - __`[RUN THE CELL]`__
    - We can see that BeautifulSoup has found 20 elements, so it looks like its working fine!
    - The problem is that each item it found it finds everything in the graphic card, the image, the characteristics, some reviews...
    - But we only want to obtain the price of each graphic card

<br>

- __`[NEXT CELL] Image with the second HTML code`__
    - You can see that each division tag contains a span tag that contains the price
    - This span tag has a class attribute equal to something like "ProductPriceBlock"
    - So, we can iterate through the list of graphic cards, and for each item, we can extract the price accessing to its span tag
    - Then, for each span tag we can extract the text getting the "text" attribute
    - And finally append each price to a list
    - __`[RUN CELL]`__
    - We can see that the output is showing all the prices of the graphic cards stored in a list
    - BeautifulSoup has many more methods apart from what we just saw, make sure to check the documentation!

<br>

- __`[NEXT CELL] pip install selenium`__
    - The other library we are going to talk about is Selenium
    - Selenium can use a browser to automate actions that we programmed from Python
    - To use it, first, we need to install it by running pip install selenium
    - And we need to download the driver for controlling the browser we are going to use
    - In this video, we will use Chrome as the main driver
    - Let's see how to download and set the driver for using it with selenium

<br>

- __`[SWITCH SCENE] Open Chrome`__
    - The first thing we need to check is the Chrome version we currently have
    - We can check it on the application and clicking on the three dots on the top right corner __`[CLICK THE THREE DOTS]`__
        - And then Settings __`[CLICK Settings]`__
    - In the next window, click on About Chrome on the left-hand side __`[CLICK ON ABOUT CHROME]`__
    - At the time of recording, the version is 96.0
    - Now, we can download the driver. You can google download chromedriver and click on the first link __`[GOOGLE download chromedriver and CLICK THE FIRST LINK]`__
    - Click the link corresponding to the version you have in Chrome. In our case, we will download the 96 version
    - And then select the OS in your machine. In my case, I will download the one for Mac.
    - __`[IF YOU CAN, DOWNLOAD IT DIRECTLY INTO DESKTOP - OTHERWISE, MOVE IT LATER MANUALLY]`__
    - Once downloaded, it is useful to move the file into PATH

<br>

- __`[SWITCH SCENE] Open Terminal`__
    - chromedriver is now in Desktop, so we can "cd" to Desktop __`[RUN cd Desktop]`__
    - PATH is an environment variable that specifies a set of directories
    - To check the value of PATH we can run `echo $PATH`  __`[RUN echo $PATH]`__
    - Each path is separated by a colon, so we can move chromedriver to any of these files
    - __`[IN MY CASE IT LOOKS LIKE THIS]`__
        - <p align=center><img src=.images/path.png width=700></p>
    - One of the values is "/usr/local/bin", so we can move chromedriver to that path __`[RUN mv chromedriver /usr/local/bin]`__
    - The reason we move it to PATH is because by default selenium will look into PATH to look for driver
    - If we don't move it to PATH, we would need to specify the path to find chromedriver

<br>

- __`[SWITCH SCENE] Open VSCode`__
- __`[NEXT CELL] from selenium import webdriver`__
    - Let's say that we are still looking for graphic cards.
    - When you finish extracting the first 20 graphic cards, that's it, you need to click on the Next button
    - However, BeautifulSoup doesn't allow us to do so
    - Luckily, Selenium has some methods to interact with the elements of a website
    - First of all, we need to create an instance of the webdriver
    - __`[RUN THE CELL]`__
    - A new Chrome window has popped up
    - Let's change the disposition of the screen, so we can see all the actions we code in VSCode reflected in Chrome

<br>

- __`[SWITCH SCENE] Selenium Disposition`__
- __`[NEXT CELL] driver.get(...)`__
    - Then, we will tell the instance to connect to the website
    - __`[RUN THE CELL]`__
    - We can see that Chrome has connected to the Curry's website
    - But one problem has appeared, now we have to accept the cookies
    - Selenium allows us to click on that button

<br>

- __`[NEXT CELL] driver.find_element_by_xpath("//button")`__
    - We can use the Xpath of the "Accept All Cookies" button
    - We can observe the xpath of this button right clicking on it and inspecting the element
    - __`[RIGHT CLICK ON "ACCEPT ALL COOKIES" and Click on Inspect]`__
    - Notice that the XPath can be described as a button tag whose id attribute is something like "onetrust-accept"
    - And once we locate the element, we can click on it using the click() method. 
    - __`[RUN THE CELL]`__
    - We can see that the accept cookies method has dissapeared because selenium has clicked on it for us

<br>

- __`[NEXT CELL] driver.find_element_by_xpath("//div")`__
    - We can use the same tags and attributes that we used with BeautifulSoup to find the price of all the items
    - Notice that the xpath for the container is absolute
    - But the prices correspond to relative xpaths
    - Additionally, the first method is find elementS, plural
        - So this will find all elements that meet the requirements we specify
    - But the second one is find element, singular
        - And this will find the first element that meets the requirement
    - I also had to add a try except statement to extract those prices that have a slight different xpath
    - __`[RUN CELL]`__
    - And we can see that selenium has extracted all the prices

<br>

- __`[NEXT CELL] search_bar = driver.find_element_by_xpath('.//input[@class="Input__InputElement-kaiOny fQGpYl"]')`__
    - Now that we extracted all the items in this page, we can click the button for moving to the next page
    - __`[IN CHROME SCROLL DOWN UNTIL YOU FIND THE PAGE SELECTOR AND RIGHT CLICK ON THE NEXT BUTTON TO SEE ITS POSITION IN THE HTML CODE]`__
    - So, let's scroll down until we find the Next button to see its XPath
    - __`[YOU SHOULD SEE SOMETHING LIKE THIS]`__
        - <p align=center><img src=.images/curry_3.png width=600></p>
    - The Xpath can be specified if we look for an "a" tag whose title is "next"
    - And that gives us the Xpath we can see in the code
    - So after clickin on it, __`[RUN THE CELL]`__
    - Chrome goes to the next page, where we can keep extracting data

<br>

- __`[NEXT CELL] search_bar.send_keys("SD card")`__
    - We can use the send_keys method to specify the keys we want to write inside the search bar
    - __`[RUN THE CELL]`__

<br>

- __`[NEXT CELL] from selenium.webdriver.common.keys import Keys`__
    - And if we want to press RETURN, we need to import special keys
    - And then send them to the search bar
    - __`[RUN THE CELL]`__
    - Now Chrome is on another page, meaning that everything worked fine!

<br>

- __`[SWITCH SCENE] webcam in full screen`__
    - You have seen how powerful webscaping can be
    - You can navigate through the HTML code using tags and the attributes of each tag
    - And you can leverage these attributes to extract specific parts of the website
    - You can get the HTML code using a GET request
    - But only that might not be enough for easily finding certain elements in the website
    - So you can use two Python libraries
        - BeautifulSoup can help you parse the HTML tree and scrape some parts of the website
        - Selenium allows to interact with the website
    - Both libraries have many methods that we haven't covered during the video
    - So make sure to check the documentation to create a great scraper!

