# Big O Notation

- Welcome everyone to a very important concept in computer programming, Big O Notation or algorithmic efficiency. 

## The Why

- While programming one of your concerns will be improving the efficiency of your algorithms. 
- But how do we compare algorithms and how do we choose which one runs faster and which takes up the least space?
- Some functions might run quickly on some hardware but slower on others so how do we determine which algorithm would be best to use? 
- This is where we can use big O notation to examine algorithm efficiency in time and space.
-  Big O notation is mathematical notation which can describe the limiting behaviour of a function as it tends to infinity or a particular value, this is known as asymptotic analysis.

- In particular for computer programming, big O Notation can be used to classify algorithms according to how their time or space complexity requirements grow based on the functions input size. 
- With this definition, we can classify algorithms based on their efficiency so we know which is best for our program. 

__`[SHOW ON SLIDES]`__ Show big O(n) notation graphic.
- This is how you write big O notation.
- The n term here represents the runtime of the algorithm as n gets very large. 


## Measuring time Complexity

- Let's look at an example to understand how we might define the big O notation of a function for time complexity.

__`[SHOW IN VS CODE]`__ for i in range(n):
- We have defined a simple loop that will print out consecutive numbers for a given n.

__`[SHOW ON SLIDES]`__ Graphic animation of looping through the list.
- Imagine the loop we created iterating through a list of elements of size n. We can see that the amount of time this would take would be proportional to the length n of the list.

__`[SHOW IN VS CODE]`__ Now image looping through this nested loop.
    - Here the loop will print out the number consecutively n * n times. 

__`[SHOW ON SLIDES]`__ Graphic looping through the nested loop
- Now for a nested for loop, we can see it would take each element in a list of size n, n times to iterate through the second list. So it would take the whole algorithm n*n times to iterate through both lists or n^2.
-  Now we can define these two complexities in big O notation as O(n) (pronounced big O of n) for the first algorithm and O(n^2) (pronounced big O squared) for the second. 

## Worst case scenario

- Now you might wonder what about the cases for small values of n where the algorithms might have similar runtime? Yes in this case the algorithms would have similar time complexity but remember we are concerned with how the function performs when n is large or growing.
-  When we're talking about big O notation we are interested in the worst case scenario. 
-  Knowing this it is easy to see as n grows the second function will have a much larger time complexity than the first. Now you know why generally you would like to avoid using nested loops!

## Rules to follow

- There are some simple rules to follow when analysing algorithms with big O notation. The first rule when defining an algorithm in big O notation is that constant terms in your function can be omitted.

__`[SHOW IN VS CODE]`__ Show example of multiplying by constant

- Imagine we have this function that returns the product of multiplying all elements in the list by 5.  
- Intuitively you would think that this function would be represented by O(5n) in big O notation.
- But remember we're concerned with the worse case scenario so as n gives sufficiently large then the 5 term will no longer matter so we represent this complexity as O(n). 
- It doesn't matter how large the constant term just follow the rule that the constant can be dropped when representing the complexity.

- Rule number two and three help you determine big O when your algorithm is structured in the form do this then do that. Here is a example of an algorithm performing two loops one after another.

__`[SHOW ON SLIDES]`__ Example of seperated loops

- If your algorithm is in this form then based on our rule the resulting complexity would be O(m+n). 
- Another way to say this is if you have two algorithms that take f(n) and g(n) steps where the functions are performing the same action then their total performance will be O(f(n) + g(n)).

- Rule number three is similar to rule two but there is a slight difference let's have a look.

__`[SHOW ON SLIDES]`__ Separated loops where one loop has a higher time complexity.

- In this example the first function let's call it f(n) as we know from before will have O(n) complexity and the second let's call it g(n) will have O(n^2) complexity. 
- We would expect that the big O for this function would be O(f(n) + g(n)) or O(n + n^2).
-  In this case, we would just simplify big O to O(n^2) since the n^2 term will dominate the n term as n gets very large. 
-  We can think of this rule as saying if our algorithm O(f(n) + g(n)) steps and g(n) is greater than f(n) then we can simplify it to O(f(n)).



__`[SHOW ON SLIDES]`__ Example of nested loops
- Lastly, if we have an algorithm that takes f(n) steps, and for every step in f(n) we take g(n) steps then we can calculate the complexity as O(f(n) x g(n)). 
- We have seen this example before when creating the nested loops from before. 

## Space Complexity

- So far we have only been concerned with algorithmic time complexity, how fast our algorithms run. We should also be concerned with how much space in memory our algorithms need to run. 

- Like time complexity we can use big O to measure the space complexity of our algorithms. 
- Though the n in big O notation for space complexity represents something a little bit different. 
- With space complexity for each input element to our function, there will be a fixed number of bytes allocated this is what n represents. 

 - So why do we need to measure space complexity surely we just care that our program running as fast as possible? 
 - In a perfect world, all of our algorithms would run fast and in as small a space as possible in memory. 
 - Imagine we had a dataset and it was increasing with a space complexity of O(n) then eventually the dataset would get so large we might not even be able to fit it into memory on our computer!
 - So we need to consider space complexity when creating our algorithms or our program might not even run. 

- In the simplest and most common case for a list of elements of size n. There will be k bytes allocated for each element in the list of size n. So this would be represented by O(n) in terms of space complexity. 

- Generators to the rescue, in Python one great way to resolve this problem, is to use generators! 
- Since generators only return one value every time they are run then we can reduce our space complexity to take a constant amount of space, the best possible scenario. 
- This can be represented by O(1) in big O notation.

## Visualising Complexity

- Now that we know the rules for determining space and time complexity and how to define them using big O notation let's look at how time complexity looks visually. 

__`[SHOW ON SLIDES]`__ Graph of time complexities from O(1) -> O(n^2).

- From the graph, you can see the differences in some of the common complexities O(1) to O(n^2). Notice that for small values of n some of the complexities that perform poorly for a large value of n actually perform better.
- But remember we want to know the worst case scenario so we are only interested in the result if n grows very large.
- We can see from the graph that O(n^2) performs better than O(n) for small values but as n grows large it performs significantly worse.  

- So how does this look in numbers? 
  
__`[SHOW ON SLIDES]`__ table of time complexity for n = 1 -> 64 for different complexities.
- You can see there is a huge difference between even O(n) and O(n^2). Even for such a small value like n = 64 there is quite the difference between 64 and 4,069 and it will only get exponentially worse as n gets larger.
-  Imagine your function had to process a dataset with over 1000 entries this would be 1000^2 or 1,000,000!

- Now that we have seen the performance differences between the most common complexities let's have get an idea of what they mean in practice.

## Constant O(1)

__`[SHOW ON SLIDES]`__ Graphs of all the complexities

- O(1) is the best case scenario we can expect in terms of complexity and it known as constant time.
-  It's called this because for time complexity that no matter the value of n the function will have a constant runtime. For space complexity, the function would require a constant amount of space in memory.
-   Some examples of these types of functions would be accessing an item in an array or checking if a number is even or odd.

## Linear O(n)

__`[SHOW ON SLIDES]`__ Graphs of all the complexities

 - O(n) is known as linear time complexity, it means that the complexity will increase linearly with the length of n. 
 - If we consider the runtime of our function as 1 unit of time then we could say this function takes 1 * n units of time to run. 
 - This simplest example would be iterating through a list. For space complexity this would mean our function takes k bytes * n bytes of space in memory. Again this could be the length of a list of of size n.

## O(log(n)) Logarithmic running time

__`[SHOW ON SLIDES]`__ Graphs of all the complexities

- An algorithm is said to have O(log n) complexity when it reduces the size of it's data with each step. It is usually applied to algorithms that divide data in half with each time step.

 - Imagine you're searching a dictionary for a word. You could just look through every word in the dictionary until you find it but this would be very time consuming.
 - A better way might be to check the middle of the book and see if you word is in the left half or right half of the book. 
 - You can then repeat this process until you get to the word you were looking for.
So we're reducing the size of the dictionary each step we perform in our search to find the word. 
- This is an indication that we have O(log n) complexity.

- The most common example of this complexity is a binary search which works like our dictionary problem.

- We can see the search finds the middle of the list and compare it to our target value. It will then search the right or left hand side of the list depending on our target value. 

## O(n^2) Quadratic runtime

__`[SHOW ON SLIDES]`__ Graphs of all the complexities

- O(n^2) is called Quadratic complexity we have seen an example of this with our nested loops. 
- Think of it as for every item in a list perform go through a whole other list.
-  We want to try and avoid this kind of complexity, it is normally never acceptable to be used and it can usually be avoided with a clever implementation of another algorithm.
-  So have a good think about how you can avoid this.

### Wrapping up

- I hope this video has cleared up the mystery of big O notation and you can now see how it is a great tool in estimating how our functions might perform when their input size grows. 
- Always keep in mind what the big O of your functions are when coding them and with some practice, you will be able to implement better performing code in no time!