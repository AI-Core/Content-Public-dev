{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Overview of ChatGPT\n",
    "\n",
    "ChatGPT is a complicated system that builds on top of many previous techniques. The sum of this amounts to a breakthrough that has truly democratised AI by making it available in an intuitive interface that anyone can use.\n",
    "\n",
    "ChatGPT is certainly capable of making mistakes, including:\n",
    "\n",
    "As trained, language models like GPT-3 are typically tasked with completing the input text. \n",
    "This might be directly useful for a long-range autocomplete, but in most use cases, the human user doesnt want a completion, they want a response to an instruction. If a vanilla language model is prompted with an instruction like \"\n",
    "\n",
    "![](./images/GPT-3%20Instruction%20Response.png)\n",
    "\n",
    "So how can we make language models that do what we tell them, rather than just continuing on from it?\n",
    "\n",
    "## Previously, Prompt Engineering was Important\n",
    "\n",
    "The initial way that this was accomplished was by using _prompt engineering_. \n",
    "Prompt engineering is the technique of writing your instruction in a way that makes your desired output the most likely completion that would follow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"How heavy is the earth?\"\n",
    "\n",
    "engineered_prompt = f\"\"\"\n",
    "Here is a question: {user_input}\n",
    "Here is an answer:\n",
    "\"\"\"\n",
    "\n",
    "print(\"Final prompt:\")\n",
    "print(engineered_prompt)\n",
    "\n",
    "# Send this prompt to the model and get a completion back as a response, which is most likely an answer to the question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Nowadays, there are techniques that seem to work better than prompt engineering for making language models respond appropriately to instructions\n",
    "\n",
    "Prompt engineering is still widely used, but not so much for framing an instruction as a text to be completed, but rather to add additional constraints around the instruction, such as \n",
    "- Reply in a professional and polite tone\n",
    "- Do not express political opinions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"How heavy is the earth?\"\n",
    "\n",
    "engineered_prompt = f\"\"\"\n",
    "\n",
    "You are a chatbot trained to respond in a professional manner.\n",
    "\n",
    "{user_input}\n",
    "\"\"\"\n",
    "\n",
    "print(\"Final prompt:\")\n",
    "print(engineered_prompt)\n",
    "\n",
    "# Send this prompt to the model and get a completion back as a response, which is most likely an answer to the question.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## InstructGPT\n",
    "\n",
    "> InstructGPT is a large language model trained to produce a single text response to an instruction provided by the human user. \n",
    "\n",
    "![](./images/GPT-3%20vs%20InstructGPT.png)\n",
    "\n",
    "- InstructGPT is produced by fine-tuning GPT-3 using human feedback.\n",
    "- The InstructGPT models are able to generate preferred responses to GPT-3, despite having less than 100x fewer parameters (1.3B compared to 175B).\n",
    "- The InstructGPT models are now the default models deployed on the OpenAI API, so when you provide a prompt the models use it as an instruction, rather than as a prompt to be completed.\n",
    "\n",
    "ChatGPT is trained using a similar approach to InstructGPT.\n",
    "- Collect suitable responses written by human labellers\n",
    "\n",
    "\n",
    "\n",
    "So how does it work under the hood?\n",
    "\n",
    "ChatGPT is implemented in 3 steps, as shown below:\n",
    "\n",
    "![](./images/steps.png)\n",
    "\n",
    "- Step 1: Fine tune a pre-trained language model to act like a chatbot\n",
    "- Step 2: Train a new _reward model_ to identify which responses generated by the chatbot are better than others\n",
    "- Step 3: Use the reward model to score generated responses, and update the language model to prefer responses with a higher score\n",
    "\n",
    "Interestingly, there are likely other machine learning models used in the system too. For example: \n",
    "- To identify whether the prompt is inappropriate.\n",
    "- To identify whether the final responses is inappropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
