{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to import the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataset import CitiesDataset\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we will define our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferLearning(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = resnet50(weights=ResNet50_Weights)\n",
    "        for param in self.layers.parameters():\n",
    "            param.grad_required = False\n",
    "        linear_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2048, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 10),\n",
    "        )\n",
    "        self.layers.fc = linear_layers\n",
    "        # print(self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset. This has been created for you in this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    lr=0.1,\n",
    "    epochs=20,\n",
    "    optimiser=torch.optim.SGD\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a neural network on a dataset and returns the trained model\n",
    "\n",
    "    Parameters:\n",
    "    - model: a pytorch model\n",
    "    - dataloader: a pytorch dataloader\n",
    "\n",
    "    Returns:\n",
    "    - model: a trained pytorch model\n",
    "    \"\"\"\n",
    "\n",
    "    # components of a ml algortithms\n",
    "    # 1. data\n",
    "    # 2. model\n",
    "    # 3. criterion (loss function)\n",
    "    # 4. optimiser\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # initialise an optimiser\n",
    "    optimiser = optimiser(model.parameters(), lr=lr, weight_decay=0.001)\n",
    "    scheduler = lr_scheduler.MultiStepLR(optimiser, milestones=[5,15], gamma=0.1,verbose=True)\n",
    "    batch_idx = 0\n",
    "    epoch_idx= 0\n",
    "    for epoch in range(epochs):  # for each epoch\n",
    "        # \n",
    "        \n",
    "        print('Epoch:', epoch_idx,'LR:', scheduler.get_lr())\n",
    "        epoch_idx +=1\n",
    "        \n",
    "        for batch in train_loader:  # for each batch in the dataloader\n",
    "            features, labels = batch\n",
    "            prediction = model(features)  # make a prediction\n",
    "            # compare the prediction to the label to calculate the loss (how bad is the model)\n",
    "            loss = F.cross_entropy(prediction, labels)\n",
    "            loss.backward()  # calculate the gradient of the loss with respect to each model parameter\n",
    "            optimiser.step()  # use the optimiser to update the model parameters using those gradients\n",
    "            print(\"Epoch:\", epoch, \"Batch:\", batch_idx,\n",
    "                  \"Loss:\", loss.item())  # log the loss\n",
    "            optimiser.zero_grad()  # zero grad\n",
    "            writer.add_scalar(\"Loss/Train\", loss.item(), batch_idx)\n",
    "            batch_idx += 1\n",
    "            if batch_idx % 25 == 0:\n",
    "                print('Evaluating on valiudation set')\n",
    "                # evaluate the validation set performance\n",
    "                val_loss, val_acc = evaluate(model, val_loader)\n",
    "                writer.add_scalar(\"Loss/Val\", val_loss, batch_idx)\n",
    "                writer.add_scalar(\"Accuracy/Val\", val_acc, batch_idx)\n",
    "\n",
    "        scheduler.step()\n",
    "    # evaluate the final test set performance\n",
    "    \n",
    "    print('Evaluating on test set')\n",
    "    test_loss = evaluate(model, test_loader)\n",
    "    # writer.add_scalar(\"Loss/Test\", test_loss, batch_idx)\n",
    "    model.test_loss = test_loss\n",
    "    \n",
    "    return model   # return trained model\n",
    "    \n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    n_examples = 0\n",
    "    for batch in dataloader:\n",
    "        features, labels = batch\n",
    "        prediction = model(features)\n",
    "        loss = F.cross_entropy(prediction, labels)\n",
    "        losses.append(loss.detach())\n",
    "        correct += torch.sum(torch.argmax(prediction, dim=1) == labels)\n",
    "        n_examples += len(labels)\n",
    "    avg_loss = np.mean(losses)\n",
    "    accuracy = correct / n_examples\n",
    "    print(\"Loss:\", avg_loss, \"Accuracy:\", accuracy.detach().numpy())\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 0 LR: [0.0001]\n",
      "Epoch: 0 Batch: 0 Loss: 2.304605722427368\n",
      "Epoch: 0 Batch: 1 Loss: 2.3003504276275635\n",
      "Epoch: 0 Batch: 2 Loss: 2.2844228744506836\n",
      "Epoch: 0 Batch: 3 Loss: 2.298607110977173\n",
      "Epoch: 0 Batch: 4 Loss: 2.282902717590332\n",
      "Epoch: 0 Batch: 5 Loss: 2.305506706237793\n",
      "Epoch: 0 Batch: 6 Loss: 2.2964835166931152\n",
      "Epoch: 0 Batch: 7 Loss: 2.2878315448760986\n",
      "Epoch: 0 Batch: 8 Loss: 2.272757053375244\n",
      "Epoch: 0 Batch: 9 Loss: 2.2604007720947266\n",
      "Epoch: 0 Batch: 10 Loss: 2.2550315856933594\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 1 LR: [0.0001]\n",
      "Epoch: 1 Batch: 11 Loss: 2.147705078125\n",
      "Epoch: 1 Batch: 12 Loss: 2.156311511993408\n",
      "Epoch: 1 Batch: 13 Loss: 2.2031359672546387\n",
      "Epoch: 1 Batch: 14 Loss: 2.119562864303589\n",
      "Epoch: 1 Batch: 15 Loss: 2.148904323577881\n",
      "Epoch: 1 Batch: 16 Loss: 2.069739580154419\n",
      "Epoch: 1 Batch: 17 Loss: 2.1123878955841064\n",
      "Epoch: 1 Batch: 18 Loss: 2.06619930267334\n",
      "Epoch: 1 Batch: 19 Loss: 1.9806499481201172\n",
      "Epoch: 1 Batch: 20 Loss: 2.031989812850952\n",
      "Epoch: 1 Batch: 21 Loss: 1.9863547086715698\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 2 LR: [0.0001]\n",
      "Epoch: 2 Batch: 22 Loss: 1.978411316871643\n",
      "Epoch: 2 Batch: 23 Loss: 1.8181508779525757\n",
      "Epoch: 2 Batch: 24 Loss: 1.9501899480819702\n",
      "Evaluating on valiudation set\n",
      "Loss: 2.139334 Accuracy: 0.20289855\n",
      "Epoch: 2 Batch: 25 Loss: 1.8515973091125488\n",
      "Epoch: 2 Batch: 26 Loss: 1.8473098278045654\n",
      "Epoch: 2 Batch: 27 Loss: 1.7663841247558594\n",
      "Epoch: 2 Batch: 28 Loss: 1.8109393119812012\n",
      "Epoch: 2 Batch: 29 Loss: 1.7876402139663696\n",
      "Epoch: 2 Batch: 30 Loss: 1.670736312866211\n",
      "Epoch: 2 Batch: 31 Loss: 1.6501144170761108\n",
      "Epoch: 2 Batch: 32 Loss: 2.1397950649261475\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 3 LR: [0.0001]\n",
      "Epoch: 3 Batch: 33 Loss: 1.4786843061447144\n",
      "Epoch: 3 Batch: 34 Loss: 1.497117519378662\n",
      "Epoch: 3 Batch: 35 Loss: 1.4608694314956665\n",
      "Epoch: 3 Batch: 36 Loss: 1.5827841758728027\n",
      "Epoch: 3 Batch: 37 Loss: 1.404533863067627\n",
      "Epoch: 3 Batch: 38 Loss: 1.4112333059310913\n",
      "Epoch: 3 Batch: 39 Loss: 1.3779338598251343\n",
      "Epoch: 3 Batch: 40 Loss: 1.4331127405166626\n",
      "Epoch: 3 Batch: 41 Loss: 1.2389508485794067\n",
      "Epoch: 3 Batch: 42 Loss: 1.3625445365905762\n",
      "Epoch: 3 Batch: 43 Loss: 1.6317201852798462\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 4 LR: [0.0001]\n",
      "Epoch: 4 Batch: 44 Loss: 1.1613374948501587\n",
      "Epoch: 4 Batch: 45 Loss: 1.1720057725906372\n",
      "Epoch: 4 Batch: 46 Loss: 1.0100128650665283\n",
      "Epoch: 4 Batch: 47 Loss: 1.1090857982635498\n",
      "Epoch: 4 Batch: 48 Loss: 1.0781195163726807\n",
      "Epoch: 4 Batch: 49 Loss: 1.184799075126648\n",
      "Evaluating on valiudation set\n",
      "Loss: 1.7512722 Accuracy: 0.33333334\n",
      "Epoch: 4 Batch: 50 Loss: 0.9106115102767944\n",
      "Epoch: 4 Batch: 51 Loss: 0.9067680835723877\n",
      "Epoch: 4 Batch: 52 Loss: 0.792253315448761\n",
      "Epoch: 4 Batch: 53 Loss: 0.8037228584289551\n",
      "Epoch: 4 Batch: 54 Loss: 2.24714732170105\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 5 LR: [1.0000000000000002e-06]\n",
      "Epoch: 5 Batch: 55 Loss: 0.7502117156982422\n",
      "Epoch: 5 Batch: 56 Loss: 0.8558361530303955\n",
      "Epoch: 5 Batch: 57 Loss: 0.7340482473373413\n",
      "Epoch: 5 Batch: 58 Loss: 0.7787428498268127\n",
      "Epoch: 5 Batch: 59 Loss: 0.7378460764884949\n",
      "Epoch: 5 Batch: 60 Loss: 0.6933658123016357\n",
      "Epoch: 5 Batch: 61 Loss: 0.7338816523551941\n",
      "Epoch: 5 Batch: 62 Loss: 0.67079097032547\n",
      "Epoch: 5 Batch: 63 Loss: 0.7130088806152344\n",
      "Epoch: 5 Batch: 64 Loss: 0.7908768057823181\n",
      "Epoch: 5 Batch: 65 Loss: 0.8750178217887878\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 6 LR: [1e-05]\n",
      "Epoch: 6 Batch: 66 Loss: 0.7609021663665771\n",
      "Epoch: 6 Batch: 67 Loss: 0.7464163303375244\n",
      "Epoch: 6 Batch: 68 Loss: 0.6968879699707031\n",
      "Epoch: 6 Batch: 69 Loss: 0.6718863844871521\n",
      "Epoch: 6 Batch: 70 Loss: 0.7624093890190125\n",
      "Epoch: 6 Batch: 71 Loss: 0.5534433126449585\n",
      "Epoch: 6 Batch: 72 Loss: 0.6976972818374634\n",
      "Epoch: 6 Batch: 73 Loss: 0.6346972584724426\n",
      "Epoch: 6 Batch: 74 Loss: 0.6552746295928955\n",
      "Evaluating on valiudation set\n",
      "Loss: 1.7382884 Accuracy: 0.4347826\n",
      "Epoch: 6 Batch: 75 Loss: 0.7174339294433594\n",
      "Epoch: 6 Batch: 76 Loss: 1.181564450263977\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 7 LR: [1e-05]\n",
      "Epoch: 7 Batch: 77 Loss: 0.6159186959266663\n",
      "Epoch: 7 Batch: 78 Loss: 0.8996412754058838\n",
      "Epoch: 7 Batch: 79 Loss: 0.6024043560028076\n",
      "Epoch: 7 Batch: 80 Loss: 0.6084747314453125\n",
      "Epoch: 7 Batch: 81 Loss: 0.5740434527397156\n",
      "Epoch: 7 Batch: 82 Loss: 0.5891739726066589\n",
      "Epoch: 7 Batch: 83 Loss: 0.744312584400177\n",
      "Epoch: 7 Batch: 84 Loss: 0.7140803933143616\n",
      "Epoch: 7 Batch: 85 Loss: 0.5420132279396057\n",
      "Epoch: 7 Batch: 86 Loss: 0.6538859009742737\n",
      "Epoch: 7 Batch: 87 Loss: 1.309815526008606\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 8 LR: [1e-05]\n",
      "Epoch: 8 Batch: 88 Loss: 0.6084685325622559\n",
      "Epoch: 8 Batch: 89 Loss: 0.6668601036071777\n",
      "Epoch: 8 Batch: 90 Loss: 0.5937536358833313\n",
      "Epoch: 8 Batch: 91 Loss: 0.7073348164558411\n",
      "Epoch: 8 Batch: 92 Loss: 0.5858173966407776\n",
      "Epoch: 8 Batch: 93 Loss: 0.6699502468109131\n",
      "Epoch: 8 Batch: 94 Loss: 0.583714485168457\n",
      "Epoch: 8 Batch: 95 Loss: 0.594222903251648\n",
      "Epoch: 8 Batch: 96 Loss: 0.6620904207229614\n",
      "Epoch: 8 Batch: 97 Loss: 0.6120599508285522\n",
      "Epoch: 8 Batch: 98 Loss: 1.6103323698043823\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 9 LR: [1e-05]\n",
      "Epoch: 9 Batch: 99 Loss: 0.7871241569519043\n",
      "Evaluating on valiudation set\n",
      "Loss: 1.6896745 Accuracy: 0.4347826\n",
      "Epoch: 9 Batch: 100 Loss: 0.6899092793464661\n",
      "Epoch: 9 Batch: 101 Loss: 0.5406280159950256\n",
      "Epoch: 9 Batch: 102 Loss: 0.5377792716026306\n",
      "Epoch: 9 Batch: 103 Loss: 0.6022205352783203\n",
      "Epoch: 9 Batch: 104 Loss: 0.5593247413635254\n",
      "Epoch: 9 Batch: 105 Loss: 0.5559364557266235\n",
      "Epoch: 9 Batch: 106 Loss: 0.4841805696487427\n",
      "Epoch: 9 Batch: 107 Loss: 0.5753133893013\n",
      "Epoch: 9 Batch: 108 Loss: 0.5721566677093506\n",
      "Epoch: 9 Batch: 109 Loss: 1.86878502368927\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 10 LR: [1e-05]\n",
      "Epoch: 10 Batch: 110 Loss: 0.49788612127304077\n",
      "Epoch: 10 Batch: 111 Loss: 0.5180565714836121\n",
      "Epoch: 10 Batch: 112 Loss: 0.5562915205955505\n",
      "Epoch: 10 Batch: 113 Loss: 0.5907689332962036\n",
      "Epoch: 10 Batch: 114 Loss: 0.5849855542182922\n",
      "Epoch: 10 Batch: 115 Loss: 0.6773903965950012\n",
      "Epoch: 10 Batch: 116 Loss: 0.6221907138824463\n",
      "Epoch: 10 Batch: 117 Loss: 0.5169898271560669\n",
      "Epoch: 10 Batch: 118 Loss: 0.6356116533279419\n",
      "Epoch: 10 Batch: 119 Loss: 0.5177986025810242\n",
      "Epoch: 10 Batch: 120 Loss: 1.2328804731369019\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 11 LR: [1e-05]\n",
      "Epoch: 11 Batch: 121 Loss: 0.5086195468902588\n",
      "Epoch: 11 Batch: 122 Loss: 0.4346146285533905\n",
      "Epoch: 11 Batch: 123 Loss: 0.5163715481758118\n",
      "Epoch: 11 Batch: 124 Loss: 0.5024726986885071\n",
      "Evaluating on valiudation set\n",
      "Loss: 1.6054683 Accuracy: 0.4057971\n",
      "Epoch: 11 Batch: 125 Loss: 0.565920352935791\n",
      "Epoch: 11 Batch: 126 Loss: 0.5393664836883545\n",
      "Epoch: 11 Batch: 127 Loss: 0.6119500398635864\n",
      "Epoch: 11 Batch: 128 Loss: 0.5394459962844849\n",
      "Epoch: 11 Batch: 129 Loss: 0.4243164360523224\n",
      "Epoch: 11 Batch: 130 Loss: 0.509557843208313\n",
      "Epoch: 11 Batch: 131 Loss: 1.7055444717407227\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 12 LR: [1e-05]\n",
      "Epoch: 12 Batch: 132 Loss: 0.48201972246170044\n",
      "Epoch: 12 Batch: 133 Loss: 0.5741481781005859\n",
      "Epoch: 12 Batch: 134 Loss: 0.5405123233795166\n",
      "Epoch: 12 Batch: 135 Loss: 0.38916265964508057\n",
      "Epoch: 12 Batch: 136 Loss: 0.4277573525905609\n",
      "Epoch: 12 Batch: 137 Loss: 0.5006677508354187\n",
      "Epoch: 12 Batch: 138 Loss: 0.5030438899993896\n",
      "Epoch: 12 Batch: 139 Loss: 0.5558953881263733\n",
      "Epoch: 12 Batch: 140 Loss: 0.5397490859031677\n",
      "Epoch: 12 Batch: 141 Loss: 0.5231673121452332\n",
      "Epoch: 12 Batch: 142 Loss: 0.7162553668022156\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 13 LR: [1e-05]\n",
      "Epoch: 13 Batch: 143 Loss: 0.43449151515960693\n",
      "Epoch: 13 Batch: 144 Loss: 0.48375585675239563\n",
      "Epoch: 13 Batch: 145 Loss: 0.4494534432888031\n",
      "Epoch: 13 Batch: 146 Loss: 0.4152965247631073\n",
      "Epoch: 13 Batch: 147 Loss: 0.42987358570098877\n",
      "Epoch: 13 Batch: 148 Loss: 0.5846045017242432\n",
      "Epoch: 13 Batch: 149 Loss: 0.46700483560562134\n",
      "Evaluating on valiudation set\n",
      "Loss: 1.6229382 Accuracy: 0.46376812\n",
      "Epoch: 13 Batch: 150 Loss: 0.4697425365447998\n",
      "Epoch: 13 Batch: 151 Loss: 0.536303699016571\n",
      "Epoch: 13 Batch: 152 Loss: 0.449244886636734\n",
      "Epoch: 13 Batch: 153 Loss: 1.2257143259048462\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 14 LR: [1e-05]\n",
      "Epoch: 14 Batch: 154 Loss: 0.3821107745170593\n",
      "Epoch: 14 Batch: 155 Loss: 0.6141747236251831\n",
      "Epoch: 14 Batch: 156 Loss: 0.47362539172172546\n",
      "Epoch: 14 Batch: 157 Loss: 0.3949570059776306\n",
      "Epoch: 14 Batch: 158 Loss: 0.508978545665741\n",
      "Epoch: 14 Batch: 159 Loss: 0.4086953103542328\n",
      "Epoch: 14 Batch: 160 Loss: 0.43154025077819824\n",
      "Epoch: 14 Batch: 161 Loss: 0.4058831036090851\n",
      "Epoch: 14 Batch: 162 Loss: 0.4810055196285248\n",
      "Epoch: 14 Batch: 163 Loss: 0.5748245716094971\n",
      "Epoch: 14 Batch: 164 Loss: 0.5534548163414001\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 15 LR: [1e-05]\n",
      "Epoch: 15 Batch: 165 Loss: 0.42527878284454346\n",
      "Epoch: 15 Batch: 166 Loss: 0.35893014073371887\n",
      "Epoch: 15 Batch: 167 Loss: 0.4230082035064697\n",
      "Epoch: 15 Batch: 168 Loss: 0.4151882231235504\n",
      "Epoch: 15 Batch: 169 Loss: 0.662875771522522\n",
      "Epoch: 15 Batch: 170 Loss: 0.392642617225647\n",
      "Epoch: 15 Batch: 171 Loss: 0.34662944078445435\n",
      "Epoch: 15 Batch: 172 Loss: 0.3985360860824585\n",
      "Epoch: 15 Batch: 173 Loss: 0.35044631361961365\n",
      "Epoch: 15 Batch: 174 Loss: 0.3504236936569214\n",
      "Evaluating on valiudation set\n",
      "Loss: 1.6624831 Accuracy: 0.46376812\n",
      "Epoch: 15 Batch: 175 Loss: 0.9830244183540344\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 16 LR: [1e-05]\n",
      "Epoch: 16 Batch: 176 Loss: 0.4705108106136322\n",
      "Epoch: 16 Batch: 177 Loss: 0.37930676341056824\n",
      "Epoch: 16 Batch: 178 Loss: 0.3600051999092102\n",
      "Epoch: 16 Batch: 179 Loss: 0.48777562379837036\n",
      "Epoch: 16 Batch: 180 Loss: 0.3946884274482727\n",
      "Epoch: 16 Batch: 181 Loss: 0.38912177085876465\n",
      "Epoch: 16 Batch: 182 Loss: 0.5548071265220642\n",
      "Epoch: 16 Batch: 183 Loss: 0.35682213306427\n",
      "Epoch: 16 Batch: 184 Loss: 0.40035003423690796\n",
      "Epoch: 16 Batch: 185 Loss: 0.36054527759552\n",
      "Epoch: 16 Batch: 186 Loss: 2.2589309215545654\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 17 LR: [1e-05]\n",
      "Epoch: 17 Batch: 187 Loss: 0.38643163442611694\n",
      "Epoch: 17 Batch: 188 Loss: 0.34587615728378296\n",
      "Epoch: 17 Batch: 189 Loss: 0.26363125443458557\n",
      "Epoch: 17 Batch: 190 Loss: 0.4238647520542145\n",
      "Epoch: 17 Batch: 191 Loss: 0.39131081104278564\n",
      "Epoch: 17 Batch: 192 Loss: 0.33380475640296936\n",
      "Epoch: 17 Batch: 193 Loss: 0.4492763876914978\n",
      "Epoch: 17 Batch: 194 Loss: 0.4497253894805908\n",
      "Epoch: 17 Batch: 195 Loss: 0.3272426724433899\n",
      "Epoch: 17 Batch: 196 Loss: 0.42365533113479614\n",
      "Epoch: 17 Batch: 197 Loss: 1.8733086585998535\n",
      "Adjusting learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 18 LR: [1e-05]\n",
      "Epoch: 18 Batch: 198 Loss: 0.40039029717445374\n",
      "Epoch: 18 Batch: 199 Loss: 0.36613520979881287\n",
      "Evaluating on valiudation set\n",
      "Loss: 1.5436182 Accuracy: 0.47826087\n",
      "Epoch: 18 Batch: 200 Loss: 0.3069804012775421\n",
      "Epoch: 18 Batch: 201 Loss: 0.42927080392837524\n",
      "Epoch: 18 Batch: 202 Loss: 0.4308152496814728\n",
      "Epoch: 18 Batch: 203 Loss: 0.3021623492240906\n",
      "Epoch: 18 Batch: 204 Loss: 0.3560609817504883\n",
      "Epoch: 18 Batch: 205 Loss: 0.2894766628742218\n",
      "Epoch: 18 Batch: 206 Loss: 0.30671706795692444\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# nn = NeuralNetworkClassifier()\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m# cnn = CNN()\u001b[39;00m\n\u001b[1;32m     24\u001b[0m model \u001b[39m=\u001b[39m TransferLearning()\n\u001b[0;32m---> 26\u001b[0m train(\n\u001b[1;32m     27\u001b[0m     model,\n\u001b[1;32m     28\u001b[0m     train_loader,\n\u001b[1;32m     29\u001b[0m     val_loader,\n\u001b[1;32m     30\u001b[0m     test_loader,\n\u001b[1;32m     31\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     32\u001b[0m     lr\u001b[39m=\u001b[39;49m\u001b[39m0.0001\u001b[39;49m,\n\u001b[1;32m     33\u001b[0m     optimiser\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdamW\n\u001b[1;32m     34\u001b[0m     )\n",
      "Cell \u001b[0;32mIn [10], line 45\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, test_loader, lr, epochs, optimiser)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m# compare the prediction to the label to calculate the loss (how bad is the model)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(prediction, labels)\n\u001b[0;32m---> 45\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# calculate the gradient of the loss with respect to each model parameter\u001b[39;00m\n\u001b[1;32m     46\u001b[0m optimiser\u001b[39m.\u001b[39mstep()  \u001b[39m# use the optimiser to update the model parameters using those gradients\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch:\u001b[39m\u001b[39m\"\u001b[39m, epoch, \u001b[39m\"\u001b[39m\u001b[39mBatch:\u001b[39m\u001b[39m\"\u001b[39m, batch_idx,\n\u001b[1;32m     48\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mLoss:\u001b[39m\u001b[39m\"\u001b[39m, loss\u001b[39m.\u001b[39mitem())  \u001b[39m# log the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface1/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/huggingface1/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "size = 128\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size),\n",
    "    transforms.RandomCrop((size, size), pad_if_needed=True),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "dataset = CitiesDataset(transform=transform)\n",
    "train_set_len = round(0.7*len(dataset))\n",
    "val_set_len = round(0.15*len(dataset))\n",
    "test_set_len = len(dataset) - val_set_len - train_set_len\n",
    "split_lengths = [train_set_len, val_set_len, test_set_len]\n",
    "# split the data to get validation and test sets\n",
    "train_set, val_set, test_set = random_split(dataset, split_lengths)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)\n",
    "# nn = NeuralNetworkClassifier()\n",
    "# cnn = CNN()\n",
    "model = TransferLearning()\n",
    "\n",
    "trained_model=train(\n",
    "                model,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                test_loader,\n",
    "                epochs=100,\n",
    "                lr=0.0001,\n",
    "                optimiser=torch.optim.AdamW\n",
    "                )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e36d4b688d7e3685ae8ad6703c0e99019531dd9f05b6e8f8c82292a1f759bcdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
