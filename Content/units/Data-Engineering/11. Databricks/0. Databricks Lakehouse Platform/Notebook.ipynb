{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks Lakehouse Platform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [*Databricks Lakehouse Platform*](https://www.databricks.com/product/data-lakehouse) is a powerful solution for managing data at scale, seamlessly integrating with cloud storage and security across multiple cloud providers. Databricks is built upon *Apache Spark*, as a unified analytics engine for big data processing, enabling users to build, deploy, share, and maintain enterprise-grade data solutions with ease.\n",
    "\n",
    "Databricks is a true multi-cloud data lake platform, meaning it extends its capabilities across various cloud environments, allowing organization to leverage the advantages of different cloud providers. Whether it's AWS, Azure, or Google Cloud, Databricks ensures a consistent and efficient experience, providing flexibility and avoiding **vendor lock-in**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks as a Lakehouse Platform\n",
    "\n",
    "To comprehend the innovation behind Databricks, let's explore the concept of a *Lakehouse*. Traditionally, data management involved the use of *Data Lakes* and *Data Warehouses*:\n",
    "\n",
    "- A **Data Lake** is a storage system that allows for the ingestion of large amounts of raw and diverse data without predefined structures. It provides flexibility, enabling storage of data in its raw format, making it suitable for a variety of data types.\n",
    "\n",
    "- A **Data Warehouse** is a structured, high-performance database optimized for analytical queries. Unlike a Data Lake, it requires predefined schemas, organizing data neatly into tables. It excels at handling structured data, making it ideal for analytical tasks.\n",
    "\n",
    "Now, envision a **Lakehouse** – Databricks' innovative approach that combines the best of both worlds. It seamlessly blends the flexibility and scalability of a Data Lake with the structure and efficiency of a Data Warehouse. In essence, Databricks acts as a unified platform capable of handling a spectrum of data types – from raw and unstructured to processed and organized.\n",
    "\n",
    "This unique synthesis positions Databricks as a comprehensive solution for modern data analytics, empowering organizations to derive insights from diverse data sources efficiently. With its multi-cloud compatibility and Lakehouse architecture, Databricks stands at the forefront of data management, providing a unified environment for analytics, collaboration, and innovation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks Lakehouse Platform Architecture\n",
    "\n",
    "### Workspace\n",
    "\n",
    "The *Workspace* serves as the heart of the platform, offering a unified environment for data engineers, data scientists, and analysts to collaborate. Within the Workspace, teams can collectively develop code, create notebooks, and share insights. This collaborative hub streamlines workflows, promoting agility and innovation.\n",
    "\n",
    "Additionally, the workspace can also refer to the UI for the Databricks persona-based environments, as seen below:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Databricks Workspace.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "We will look in more detail on how to access and utilize the workspace UI in a later section.\n",
    "\n",
    "### Runtime\n",
    "\n",
    "The *Runtime* is the powerhouse behind data processing tasks within Databricks. It optimizes efficiency and scalability by incorporating Apache Spark and other libraries. This unified runtime environment caters to a spectrum of tasks, from large-scale ETL operations to intricate machine learning algorithms, ensuring organizations can derive meaningful insights from their data at scale.\n",
    "\n",
    "### Cloud Services\n",
    "\n",
    "As seen earlier, the cloud service acts as the bridge between Databricks and the cloud infrastructure, integrating with providers such as AWS, Azure, or Google Cloud. This integration allows organizations to leverage the scalability, storage, and computational capabilities of the cloud, providing a dynamic and responsive foundation for the platform.\n",
    "\n",
    "Next, we'll explore how Databricks deploys this architecture in the cloud, introducing the *Control Plane* and *Data Plane* concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Architecture in the Cloud using Databricks\n",
    "\n",
    "### Control Plane\n",
    "\n",
    "The **Control Plane** serves as the command center for managing the Databricks infrastructure. Databricks houses the Control Plane. Here, configurations are set, access controls are defined, and security policies are established. It takes responsibility for creating and managing clusters, ensuring optimal resource utilization, and governing the overall lifecycle of the Databricks platform.\n",
    "\n",
    "### Data Plane\n",
    "\n",
    "In contrast, the **Data Plane** is where the actual data processing and storage activities occur, and it resides in the cloud provider's environment. Databricks leverages cloud-native storage solutions like AWS S3, Azure Data Lake Storage, or Google Cloud Storage for storing both raw and processed data. Within the Data Plane, computational tasks are distributed across clusters, which are essentially virtual machines stored on the desired cloud provider, optimizing data processing performance and scalability. This separation ensures that the computational workload is managed efficiently in the cloud provider's infrastructure.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/Databricks Architecture.png\" width=\"450\" height=\"400\"/>\n",
    "</p>\n",
    "\n",
    "### Deployment Workflow\n",
    "\n",
    "To make use of the capabilities of the Databricks Lakehouse Platform, teams have to use a streamlined deployment workflow, that starts with the creation of a Databricks cluster. This cluster will be the heart of all computational tasks in the Databricks. We will talk in more detail about clusters in a later section, but for now let's have a look at the deployment workflow:\n",
    "\n",
    "- **Configuration through the Workspace**\n",
    "\n",
    "  - Administrators start the process by utilizing the Databricks Workspace to configure important aspects of the platform through the Control Plane. These include configurations, runtime options, and storage locations fine-tuned to align with the unique needs and demands of the organization.<br><br>\n",
    "\n",
    "- **Cluster Creation with Control Plane**\n",
    "\n",
    "  - Using web-based cluster management tools within the Control Plane, administrators can create and manage clusters in the cloud environment. These clusters, will be hosted in the Data Plane, and will be used for different data processing tasks. <br><br>\n",
    "\n",
    "- **Workspace Interaction for Collaboration**\n",
    "\n",
    "  - Data scientists and engineers actively engage with the Databricks Workspace, benefiting from collaborative features facilitated by the Control Plane. *Workflow Notebooks* within the Workspace serve as a collaborative canvas for code development, query execution, and analyses, fostering teamwork and innovation. We will discuss about notebooks in greater detail later in this lesson. <br><br>\n",
    "\n",
    "- **Data Processing in the Cloud**\n",
    "\n",
    "  - Activation of the Data Plane unfolds as cluster VMs within the cloud provider's infrastructure efficiently process and store data\n",
    "  - Utilizing cloud-native storage solutions, such as AWS S3, Azure Data Lake Storage, or Google Cloud Storage, the Data Plane orchestrates the storage of both raw and processed data. <br><br>\n",
    "\n",
    "- **Sharing Insights and Collaboration**\n",
    "\n",
    "  - Insights and results derived from data processing are shared within the collaborative Workspace, becoming the focal point for knowledge exchange among team members. Team members collaboratively analyze results within the Workspace, interpreting findings and refining approaches iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Databricks Account\n",
    "\n",
    "To explore in depth the different features Databricks has to offer, the first step is to create a Databricks account. Depending on your cloud provider, there are different types of Databricks accounts available. To explore the platform without the need for a separate cloud provider account, Databricks Community Edition offers a free and accessible option, making it ideal for individuals and small teams.\n",
    "\n",
    "Follow these steps to sign up for a Databricks Community Edition account:\n",
    "\n",
    "- Visit the [Databricks Community Edition](https://docs.databricks.com/en/getting-started/community-edition.html) page using your web browser\n",
    "- Click on the **Try Databricks** button at top-right of the page to begin the registration process\n",
    "- Fill in essential details, including your name, phone number, and country\n",
    "- Click **Continue** to proceed. When prompted to choose a cloud provider, opt for **Get started with Community Edition** at the bottom of the page to create a Databricks account without a cloud provider account.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/CommunityEdition.png\" width=\"250\" height=\"400\"/>\n",
    "</p>\n",
    "\n",
    "- After selecting Community Edition, you will be redirected to a page instructing you to check your email\n",
    "- Look for an email from Databricks and follow the link provided to confirm your email address\n",
    "- Once verified, set a password for your Databricks Community Edition account\n",
    "- You will now be redirected to the Databricks Workspace. Congratulations you have set up a Databricks account!\n",
    "\n",
    "Now that your account is ready, let's familiarize ourselves with the various workspace options in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating the Databricks Workspace\n",
    "\n",
    "Upon logging into your Databricks account, you'll find yourself in the default *persona* - the Data Science and Engineering persona.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/DefaultPersona.png\" width=\"850\" height=\"400\"/>\n",
    "</p>\n",
    "\n",
    "This persona is designed to cater to the needs of both data engineers and data scientists. However, Databricks offers flexibility by allowing you to switch between different personas, such as Machine Learning and SQL (the SQL person is not present in the Community Edition), based on your specific tasks and preferences.\n",
    "\n",
    "### Changing Personas\n",
    "\n",
    "To switch personas, you can use the persona switcher located in the upper-left corner of the workspace.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/SwitchPersona.png\" width=\"850\" height=\"400\"/>\n",
    "</p>\n",
    "\n",
    "Depending on your needs, you can choose between the following personas:\n",
    "\n",
    "- **Data Science and Engineering**\n",
    "\n",
    "  - Ideal for tasks involving data exploration, analysis, and engineering\n",
    "  - Combines features essential for both data engineers and data scientists <br><br>\n",
    "\n",
    "- **Machine Learning**\n",
    "\n",
    "  - Tailored for developing and deploying machine learning models\n",
    "  - Provides tools for model training, tuning, and deployment <br><br>\n",
    "\n",
    "- **SQL**\n",
    "\n",
    "  - Not present in Community Edition\n",
    "  - Focused on executing SQL queries for data analysis and reporting \n",
    "\n",
    "### Main Tabs in Data Science and Engineering Persona\n",
    "\n",
    "We will only use the Data Science and Engineering persona going forward, so let's understand the main tabs and functionality offered by this persona. Let's start by creating an expanded view of the menu options for quick access. To do this, click on the **Menu options** at the bottom-left of the menu bar, then select **Expand**.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/ExpandMenu.png\" width=\"850\" height=\"450\"/>\n",
    "</p>\n",
    "\n",
    "This will keep the menu options expanded, providing easier navigation and access to the tools and features you use frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Workspace\n",
    "\n",
    "The **Workspace** tab serves as the central hub for managing your projects. This is the place where you can create a structured and organized environment for your various data science and engineering projects.\n",
    "\n",
    "Here, you'll notice two main folders: **Shared** and **Users**.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/WorkspaceFolders.png\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "The Shared folder is designed for collaborative efforts within your team. It's a shared space where team members can collaborate on projects, share notebooks, and work together on tasks. Notebooks and projects placed in this folder are accessible to everyone within your team, fostering a collaborative workflow.\n",
    "\n",
    "The Users folder is personalized for each individual user in the Databricks environment. Within this folder, you can organize your work in a way that suits your preferences. It provides a private space for your notebooks and projects. Anything stored here is only accessible to you.\n",
    "\n",
    "Within the Workspace, you have the ability to create folders, notebooks, and experiments to structure your work effectively.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/CreateNotebook.png\" width=\"850\" height=\"350\"/>\n",
    "</p>\n",
    "\n",
    "Alternatively, you can also create Notebooks using the **Create** button in the menu options. We will discuss in more detail notebooks later in this lesson.\n",
    "\n",
    "#### 2. Data\n",
    "\n",
    "The **Data** tab can be used to effectively manage tables, databases, and data storage. Please note that this tab will appear empty if no clusters are created yet. However, normally this tab allows you to visualize your data for analysis. It provides a seamless way to interact with your data, making data exploration easier.\n",
    "\n",
    "#### 3. Compute\n",
    "\n",
    "In the **Compute** tab, you can manage clusters for data processing. This involves creating, configuring, and monitoring clusters based on your specific processing needs. Clusters are essential for running large-scale data processing task effectively.\n",
    "\n",
    "> Clusters are the fundamental units providing computational resources for running code, executing queries, and processing data. When connected to a cloud provider, Databricks clusters represent a collection of Virtual Machines (VMs) within the cloud provider account.\n",
    "\n",
    "There are two primary types of clusters: *All-Purpose* and *Job*.\n",
    "\n",
    "**All-Purpose** clusters are versatile and suitable for a variety of data processing tasks. They offer a balanced combination of CPU and memory resources. They are designed to run continuously, making them ideal for persistent tasks like ongoing data exploration and analysis. They are also suited for interactive tasks where users need continuous access to a computing environment for running queries, experimenting with code, and exploring data interactively.\n",
    "\n",
    "**Job** clusters are created dynamically for the duration of a specific job and terminate automatically once the job is completed. They are efficient for running specific tasks or jobs, especially those scheduled at intervals. Job clusters provide resource isolation for each job, ensuring that the job's execution does not interfere with other ongoing tasks in the environment.\n",
    "\n",
    "Let's create an all-purpose cluster that we will use throughout this pathway to work with data within Databricks. From the **Compute** tab, select **Create compute** under the **All-purpose compute** tab. Give your cluster a name and leave the Databricks runtime version as the default one. Click **Create compute** to begin the process of creating a cluster. Creating the cluster might take a couple of minutes. Once the cluster has been created, you should be able to see it in the **All-purpose compute** tab:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/TrainingCluster.png\" width=\"850\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "Now that you have a cluster in your workspace, notice the presence of a default database in the **Data** tab. This default database serves as the initial location for your data within Databricks. It's a pre-configured storage space where you can organize and manage your datasets.\n",
    "\n",
    "> It's important to highlight that clusters in Databricks run Apache Spark. Apache Spark is a powerful open-source distributed computing system that enables parallel processing of large datasets. Spark provides a unified analytics engine for big data processing, offering high-level APIs in multiple programming languages like Scala, Python, and SQL. We will talk in more detail about Spark and its architecture in a later lesson.\n",
    "\n",
    "> IMPORTANT: Note that in Databricks Community Edition, your cluster might automatically shut down after periods of inactivity (typically around 30 minutes of inactivity). You also won't have the possibility of restarting an idle cluster, so you will need to create a new one every time this happens.\n",
    "\n",
    "#### 4. Workflows\n",
    "\n",
    "The **Workflows** tab is dedicated to managing and monitoring data workflows and jobs. Here, you can schedule and track the progress of your data processing tasks. We will not cover workflows in this pathway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks Notebooks\n",
    "\n",
    "> **Databricks Notebooks** are interactive, web-based documents that enable data scientists and data engineers to integrate code, queries, visualizations and narrative text in a collaborative environment. These notebooks provide a unified platform for end-to-end data solutions, allowing users to work with different programming languages and technologies within a single interface.\n",
    "\n",
    "Key features of Databricks notebooks include:\n",
    "\n",
    "- **Multi-Language Support**: Databricks Notebooks support multiple programming languages, including Python, Scala, SQL, and R. This flexibility allows users to choose the language that best suits their data processing and analysis needs.\n",
    "\n",
    "- **Collaborative Environment**: Notebooks promote collaboration among team members by providing a shared workspace where multiple users can contribute, edit, and comment on the same documents\n",
    "\n",
    "- **Interactive Data Exploration**: Users can perform interactive data exploration and analysis by writing and executing code cells in real-time. Notebooks offer an iterative development process, allowing users to experiment with code and immediately see the results.\n",
    "\n",
    "- **Version Control**: Notebooks automatically track history, enabling users to review and revert to previous states. This version control features provides a safety net for experimentation.\n",
    "\n",
    "To create a new notebook in Databricks, navigate to the **Workspace** tab. Select the location where you want to create the notebook, in this case I will select my own user folder, and click on the **Create** button then select **Notebook**.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/CreateNotebook2.png\" width=\"850\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "You can change the notebook name by click on the notebook name at the top (**Untitled Notebook ...**) and then entering a new name, for example **My first notebook**.\n",
    "\n",
    "In order to be able to run different commands in a notebook, we first have to attach the notebook to a cluster, that will provide the compute power for our tasks. To do so, click on the **Connect** button at top-right of the page, and then select the cluster you have previously created:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AttachNotebook.png\" width=\"850\" height=\"250\"/>\n",
    "</p>\n",
    "\n",
    "Once attached, the **Connect** button should now display the name of the cluster you've attached your notebook to.\n",
    "\n",
    "The default language for a notebook can be set from the drop-down next to the notebook name. Noticed right now the default language for your notebook is Python, but you can change this to SQL, Scala or R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Cells\n",
    "\n",
    "> In Databricks Notebooks, a cell is a fundamental unit of content that can contain code, queries, or text. Cells enable users to organize and structure their work in a modular and interactive manner. \n",
    "\n",
    "There are primarily three types of cells in Databricks Notebooks: code cells, SQL cells, and Markdown cells. Noticed that when we created our first notebook, by default we have an empty code cell. \n",
    "\n",
    "> **Code cells** are used to write and execute code snippets in various programming languages such as Python, Scala and R. Our existing code cell has Python set as the desired programming language. This can be changed using the programming language drop-down menu:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/CellLanguage.png\" width=\"850\" height=\"250\"/>\n",
    "</p>\n",
    "\n",
    "Code cells can be executed individually, allowing users to interactively run and test code in a step-by-step manner. For example let's right a simple Python code in our code cell:\n",
    "\n",
    "```python\n",
    "print(\"Hello, Databricks!\")\n",
    "```\n",
    "\n",
    "> To run a cell press the play button or press **Shift + Enter**.\n",
    "\n",
    "Once the cell has run, you should see the following successful output:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/FirstCell.png\" width=\"750\" height=\"150\"/>\n",
    "</p>\n",
    "\n",
    "You can create new cells using the **+** button above or below an existing cell. By default, this will create the same type of the cell as the previous one, but you can switch the cell type (code, SQL, Markdown) from the toolbar.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/InsertCell.png\" width=\"750\" height=\"200\"/>\n",
    "</p>\n",
    "\n",
    "> SQL cells are specifically designed for writing and executing SQL queries against structured data. They can interact with tables and datasets, making them well-suited for relational data analysis. \n",
    "\n",
    "An example of an SQL cell command would be:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM table_name\n",
    "```\n",
    "\n",
    "> Markdown cells allow users to write formatted text, create headings, lists, links, or embed images to provide rich documentation. They support Markdown syntax, enabling users to structure content in a visually appealing way. They are often used to provide explanations, instructions, or narrative alongside code.\n",
    "\n",
    "For example, the following Markdown code:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/MarkdownCode.png\" width=\"750\" height=\"325\"/>\n",
    "</p>\n",
    "\n",
    "Would be displayed like this, once the cell is run:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/MarkdownRan.png\" width=\"750\" height=\"300\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Magic Commands\n",
    "\n",
    "Databricks Notebooks offer a multitude of powerful features, and among them, *magic commands* stand out as a versatile tool for improving interactive data analysis and overall coding experience. **Magic commands** in Databricks are prefixed with a percentage sign (`%`) and provide a wide range of functionalities, from switching programming languages to interacting with the Databricks File System (DBFS) and even running different notebooks.\n",
    "\n",
    "#### 1. Switching Programming Languages\n",
    "\n",
    "You can use magic commands to switch between different programming languages within the same notebook. While the entire notebook may have a default programming language, which can be set using the toogle next to the notebook name, you have the flexibility to use different programming languages in different notebook cells using magic commands.\n",
    "\n",
    "For instance, in a Python notebook, you can use `%scala` or `%sql` magic commands within specific cells to introduce Scala or SQL code. This flexibility allows you to combine the strength of multiple languages within a single notebook, catering to the diverse requirements of your data analysis and processing tasks.\n",
    "\n",
    "#### 2. Markdown Magic Command\n",
    "\n",
    "The `%md` magic command allows you to incorporate Markdown cells directly within a code cell. This feature facilitates the integration of narrative text, headers, links, and images alongside your code and query cells. Observe in the example above, that when you select a Markdown cell using the toogle menu, it automatically adds the `%md` command to the newly created cell.\n",
    "\n",
    "#### 3. Running Different Notebooks\n",
    "\n",
    "With the `%run` magic command, you can execute cells from different notebooks with the same workspace. This promotes modularity and code reuse, enabling you to build a library of functions or code snippets and easily share them across notebooks.\n",
    "\n",
    "Let's look at an example of using the `%run` magic command. Start by creating a new notebook in your workspace. Call this notebook **New Notebook**. In the first cell of this notebook, write the following Python code to define a function:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/NewNotebook.png\" width=\"850\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "Navigate back to your first notebook. In a new cell in this notebook, use the `%run` magic command to execute the cells from the **New Notebook**. You can find the file path of the notebook using the Workspace navigator as such:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/CopyFilePath.png\" width=\"850\" height=\"350\"/>\n",
    "</p>\n",
    "\n",
    "You can then use this full file path within the `%run` magic command. In a subsequent cell, call the function defined in the new notebook and print the result. Run the cell, and you should see the output, as the cells calculates the square of the given number using the function from the new notebook.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/RunMagicCommand.png\" width=\"850\" height=\"400\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. File System (fs) Magic Commands\n",
    "\n",
    "Magic command prefixed with `%fs` provide a way of interacting with the *Databricks File System (DBFS)*.\n",
    "\n",
    "> **DBFS** is a distributed file system mounted on top of existing cloud storage solutions (such as AWS S3, Azure Data Lake Storage, or Google Cloud Storage) to provide a unified and scalable storage layer for Databricks.\n",
    "\n",
    "DBFS provides a hierarchical file system, and its structure is similar to that of a traditional file system:\n",
    "\n",
    "- `dbfs:/`: The root of DBFS\n",
    "- `dbfs:/mnt/`: Mount point for cloud storage\n",
    "- `dbfs:/mnt/data/`: Example directory within DBFS\n",
    "\n",
    "DBFS is accessible to all clusters in a Databricks workspace, making it a centralized and shared storage layer. It supports various file formats and is seamlessly integrated with Databricks notebooks.\n",
    "\n",
    "A common `%fs` magic commands is the list files `%fs ls` command. This command is used to list files and directories within DBFS. When you run this command you will typically see output similar to the following:\n",
    "\n",
    "```javascript\n",
    "dbfs:/databricks-datasets/\n",
    "dbfs:/databricks-results/\n",
    "mnt/\n",
    "```\n",
    "  - `dbfs:/databricks-datasets/`: This directory contains curated datasets provided by Databricks for exploration and learning purposes\n",
    "  - `dbfs:/databricks-results/`: This directory is used to store temporary results generated during the execution of notebooks or jobs\n",
    "  - `mnt/`: The `mnt/` directory is a common mount point for cloud storage, allowing you to access external data sources. This won't be present in Databricks Community Edition as this workspace is not directly associate with your cloud provider.\n",
    "\n",
    "Other commands include:\n",
    "\n",
    "- Reading file contents using the `%fs head` command\n",
    "\n",
    "  - Use `%fs head` to view the contents of a file in DBFS without reading the entire file. For example: `%fs head \"dbfs:/databricks-datasets/README.md\"`. <br><br>\n",
    "\n",
    "- Writing to files using the `%fs cp` command\n",
    "\n",
    "  - The `%fs cp` command copies files between locations, such as from the local file system to DBFS or between DBFS directories. Example: `%fs cp localfile.txt dbfs:/mnt/data/localfile.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Databricks Utilities (`dbutils`)\n",
    "\n",
    "`dbutils` is a versatile module in Databricks that offers a range of utilities for performing tasks related to file management, collaboration and notebook customization. It provides an interactive set of tools to enhance your interactive data analysis and coding sessions within Databricks.\n",
    "\n",
    "Key magic commands you can run using `dbutils` include:\n",
    "\n",
    "- `%dbutils fs cp` to upload files from a local file system to DBFS. This is particularly useful for bringing external data into your Databricks environment. For example: `%dbutils fs cp localfile.txt dbfs:/mnt/data/localfile.txt`.\n",
    "\n",
    "- `%dbutils fs ls` to list files and directories in DBFS. It provides an overview of the file structure within a specified directory. For example: `%dbutils fs ls \"dbfs:/mnt/data/\"`.\n",
    "\n",
    "- `%dbutils notebook run` to execute other notebooks within the same workspace. This promotes modularity and code reuse across different notebooks. For example: `%dbutils notebook run \"/Workspace/OtherNotebook\"`.\n",
    "\n",
    "While both `dbutils` and `%fs` provide functionality for file system operations in Databricks, the key distinction lies in the broader capabilities offered by `dbutils`. `dbutils` not only facilitates file manipulation but also allows you to execute other notebooks, create interactive widgets, and manage libraries. This extended functionality enhance the versatility of `dbutils`, making it a preferred choice for a comprehensive set of tasks within Databricks notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks Repos\n",
    "\n",
    "We've briefly discussed before that Databricks provides version control functionality within notebooks, allowing users to track changes, revert to previous versions, and collaborate effectively.\n",
    "\n",
    "To access version history navigate to the desired notebook. In the notebook toolbar at the top of the page, locate the section that says **Last edit was made ...**. Click on this to access the version history panel.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/VersionHistory.png\" width=\"850\" height=\"450\"/>\n",
    "</p>\n",
    "\n",
    "While Databricks provides built-in version control for notebooks, it has limitations. Different Notebook version can easily be deleted by users, and there is no possibility of working with branches. Databricks propose an alternative solution, *Databricks Repos* - a solution that extends version control capabilities and introduces a more robust collaboration environment.\n",
    "\n",
    "> Note: Databricks Repos are not part of the Databricks Community Edition. We will introduce the workflow to create and use a Databricks Repo here, but you won't be able to follow along at this point. Later when you get to your specialisation project you will get access to a full-feature Databricks account where you will be able to use Databricks Repos. Make sure to come back to this section then.\n",
    "\n",
    "The paid-version of Databricks UI will look a bit different than the Community Edition:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/DatabricksAccount.png\" width=\"850\" height=\"450\"/>\n",
    "</p>\n",
    "\n",
    "Notice that here rather than having a toogle bar to switch between the different personas, all the features from the different personas are present in the left-hand side menu under their specific tab. Now let's go back to understanding what is Databricks Repos and the features it provides.\n",
    "\n",
    "> **Databricks Repos** is a version control system integrated into Databricks, addressing the shortcomings of standard notebook versioning. It leverages Git for efficient version control, enabling users to manage changes, create branches, and collaborate seamlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure GitHub-Databricks Integration\n",
    "\n",
    "Databricks seamlessly integrates with various Git providers, enabling the functionality of Databricks Repos and facilitating collaborative workflows for efficient notebook management. To enable this integration, follow these steps to configure a connection to GitHub:\n",
    "\n",
    "- In the Databricks workspace, begin by navigating to your user settings\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/UserSettings.png\" width=\"650\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "- Within your user settings, locate and select the **Linked accounts** option. Here, choose GitHub as your preferred Git provider.\n",
    "\n",
    "- Use the **Link Git Account** feature to establish a connection GitHub account, eliminating the need for a personal access token. This action will redirect you to the following authorization page:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/DatabricksAuth.png\" width=\"450\" height=\"500\"/>\n",
    "</p>\n",
    "\n",
    "- Click **Authorize Databricks** to grant the necessary permissions. Upon successful authorization you will be redirected to Databricks.\n",
    "\n",
    "- If the operation was successful you should see a message indicating that you have successfully linked your GitHub account. Additionally, verify the linked GitHub account status under the **Linked accounts** section.\n",
    "\n",
    "### Cloning a GitHub Repository in Databricks Repos\n",
    "\n",
    "In this section, we'll walk through the process of creating a new repository in GitHub and cloning it into Databricks Repos for seamless collaboration. Begin by creating a new GitHub repository. Initialize this repository with a `README` file. Once created, make a note of the HTTPS URL of the repo.\n",
    "\n",
    "To clone the newly created repository in Databricks Repos, follow these steps:\n",
    "\n",
    "- Navigate to your Databricks workspace. In the workspace, click on the **New** button and select **Repo**.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/AddNewRepo.png\" width=\"450\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "- This will open a new page, where you need to provide the previously copied repository URL, select the Git provider as GitHub, and enter a repository name for Databricks Repos\n",
    "\n",
    "- Once you have filled in the details, click the **Create Repo** button to initiate the cloning process\n",
    "\n",
    "- Once the process is complete, you should be able to see a new folder called **Repos** in the Databricks workspace. In this folder, under your user folder, you should see the folder belonging to the newly created Databricks Repo.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/DatabricksRepos.png\" width=\"950\" height=\"250\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Branches in Databricks Repos\n",
    "\n",
    "Databricks Repos offer robust version control features, including the ability to work with branches, as mentioned before. Branches allow you to isolate your work, experiment with new features, and collaborate more effectively. Here's a step-by-step guide on how to work with branches:\n",
    "\n",
    "#### 1. Understanding Branches\n",
    "\n",
    "- In your Databricks workspace, navigate to the **Repos** folder where your cloned repository is hosted\n",
    "\n",
    "- Next to the repository name, you can see the current branch (usually set to `main`). Click on the branch name, which will redirect you to the following page, displaying the existing branches:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/BranchPage.png\" width=\"850\" height=\"450\"/>\n",
    "</p>\n",
    "\n",
    "- In this window, you can view existing branches using the toggle dropdown next to the current branch name. As you can see above, our current repository only has one branch, the `main` branch. \n",
    "\n",
    "- Let's create a new branch called `dev` using the **Create Branch** button. At this point you might be meet with the following error message:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/ErrorMessage.png\" width=\"550\" height=\"450\"/>\n",
    "</p>\n",
    "\n",
    "- Follow the link to the **Databricks GitHub app installation page**. This will redirect you to the following page:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/InstallDatabricksApp.png\" width=\"350\" height=\"450\"/>\n",
    "</p>\n",
    "\n",
    "- You can choose to allow access to all repositories in your GitHub account or only one/few selected ones. In this example, we give access only to the repository that has been cloned in Databricks Repos. Click **Install** to continue. At this point, you might be asked to log in to GitHub to confirm access.\n",
    "\n",
    "- Return to Databricks and try creating the `dev` branch again, this time you should be successful\n",
    "\n",
    "#### 2. Making Changes\n",
    "\n",
    "- Create a new file or clone an existing notebook within the `dev` branch. Before proceeding, make sure you are on the correct branch using the toggle next to the repo name.\n",
    "\n",
    "- To add a new file you can use the **Add** button and choose the file type (folder, notebook, or file)\n",
    "\n",
    "- To clone an existing file, navigate to the desired file in your workspace, click on the three dots on the right-hand side of the file name, and then select **Clone**\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/CloneNotebook.png\" width=\"650\" height=\"450\"/>\n",
    "</p>\n",
    "\n",
    "- On the clone page, navigate to the desired location for the cloned notebook. Typically, you'd move back to the Workspace directory, then into the Repos directory, selecting your cloned Databricks Repo folder before clicking **Clone**.\n",
    "\n",
    "- Once that's done you should see the newly cloned file in the Databricks Repo:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/RepoClonedFile.png\" width=\"850\" height=\"250\"/>\n",
    "</p>\n",
    "\n",
    "- After making these changes, commit them by clicking on the branch name next to the repository name. In this page, view the changes, add a descriptive commit message, then commit and push the changes using the **Commit & Push** button. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/ReposPush.png\" width=\"850\" height=\"450\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Merging Branches\n",
    "\n",
    "To merge the changes from the `dev` branch into `main`, navigate to GitHub. Create a pull request on the GitHub repository to merge the changes from your branch into the `main` branches.\n",
    "\n",
    "> It's important to note that Databricks Repos don't support merging branches directly. GitHub is the preferred platform for this operation.\n",
    "\n",
    "#### 4. Pulling Changes\n",
    "\n",
    "- To retrieve the recently merged changes on the `main` branch, switch to the `main` branch in Databricks Repos\n",
    "\n",
    "- Use the **Pull** button located on top-right side of the page to fetch and merge changes from the remote repository\n",
    "\n",
    "By following these steps, you've successfully worked with branches in Databricks Repos, made changes, pushed them to the remote repository, and even managed to merge changes on GitHub."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2 (default, Mar  3 2021, 11:58:52) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d25a140d753dbbce5959f3e993c340d725ba39c38e4259359e51030082d2593"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
