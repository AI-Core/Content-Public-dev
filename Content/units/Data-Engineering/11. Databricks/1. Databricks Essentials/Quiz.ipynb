{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks Essentials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Databricks?\n",
    "\n",
    "- A) A data warehouse solution\n",
    "- B) A big data processing and analytics platform ***\n",
    "- C) A programming language for data analysis\n",
    "- D) A data visualization tool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What AWS service is used to provision and manage the underlying infrastructure for Databricks workloads on AWS?\n",
    "\n",
    "- A. Amazon S3\n",
    "- B. Amazon EC2 ***\n",
    "- C. Amazon RDS\n",
    "- D. Amazon Redshift\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Databricks account?\n",
    "\n",
    "- A. A user account that allows access to the Databricks platform ***\n",
    "- B. A data storage service provided by Databricks\n",
    "- C. A data visualization tool for Databricks users\n",
    "- D. A type of Databricks workspace used for collaborative data analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Databricks workspace?\n",
    "\n",
    "- A. An environment for developing and running Apache Spark applications ***\n",
    "- B. A data storage service provided by Databricks\n",
    "- C. A data visualization tool for Databricks users\n",
    "- D. A user account that allows access to the Databricks platform\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Databricks notebook?\n",
    "\n",
    "- A. An interactive document that combines code, text, and visualizations ***\n",
    "- B. A data storage service provided by Databricks\n",
    "- C. A data visualization tool for Databricks users\n",
    "- D. A type of Databricks cluster used for distributed data processing\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between an All-purpose cluster and a Job cluster in Databricks?\n",
    "\n",
    "- A. All-purpose clusters are used for interactive workloads, while Job clusters are used for batch processing.\n",
    "- B. All-purpose clusters have a fixed set of nodes, while Job clusters are dynamically provisioned.\n",
    "- C. All-purpose clusters are optimized for CPU-intensive workloads, while Job clusters are optimized for memory-intensive workloads.\n",
    "- D. All-purpose clusters support concurrent users and multiple workloads, while Job clusters are designed for single-use jobs. ***\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Databricks Runtime?\n",
    "\n",
    "- A. An open-source runtime environment for running Apache Spark and other big data frameworks\n",
    "- B. A cloud-based service for building, training, and deploying machine learning models\n",
    "- C. A managed, versioned, and optimized runtime environment for running Apache Spark and other big data frameworks on Databricks ***\n",
    "- D. A data storage service provided by Databricks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Databricks workflow?\n",
    "\n",
    "- A. A set of rules and policies that govern the access and usage of Databricks resources\n",
    "- B. A collection of jobs, notebooks, and data pipelines that automate data processing and analysis in Databricks ***\n",
    "- C. A type of Databricks cluster used for machine learning and deep learning workloads\n",
    "- D. A data storage service provided by Databricks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Delta Table in Databricks?\n",
    "\n",
    "- A. A data storage format for structured data that supports ACID transactions and time-travel queries ***\n",
    "- B. A type of Databricks cluster used for running Apache Spark streaming applications\n",
    "- C. A visualization tool for exploring and analyzing data stored in Databricks\n",
    "- D. A type of Databricks notebook that supports advanced data science and machine learning workflows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are some key features of Unity Catalog in Databricks?\n",
    "\n",
    "- A. Searchable metadata store, customizable schemas, data lineage tracking, data quality monitoring, data access policies ***\n",
    "- B. Real-time data processing, stream processing, graph processing, machine learning workflows\n",
    "- C. Automated data discovery, data profiling, data classification, data masking, data encryption\n",
    "- D. Interactive notebooks, dashboards, collaboration tools, version control\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between Unity Catalog, Hive Metastore, and External Metastore in Databricks?\n",
    "\n",
    "- A. Unity Catalog is a Databricks-native data catalog that provides a unified view of data assets across different data sources and platforms, while Hive Metastore and External Metastore are third-party metadata stores that can be integrated with Databricks. ***\n",
    "\n",
    "- B. Unity Catalog is used for managing data pipelines and workflows, while Hive Metastore and External Metastore are used for storing and managing metadata related to Hive tables and databases.\n",
    "\n",
    "- C. Unity Catalog supports only Databricks-specific data formats, while Hive Metastore and External Metastore support a wider range of data formats.\n",
    "\n",
    "- D. Unity Catalog provides a centralized metadata store for all data assets in Databricks, while Hive Metastore and External Metastore provide separate metadata stores for different data sources and platforms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Databricks, what is the correct order of hierarchy for catalogs, schemas, tables, views, and functions?\n",
    "\n",
    "- A. Catalog > Schema > View > Table > Function\n",
    "- B. Catalog > Schema > Table > View > Function ***\n",
    "- C. Schema > Catalog > Table > View > Function\n",
    "- D. Schema > Catalog > View > Table > Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between the Databricks control plane and data plane?\n",
    "\n",
    "- A. The control plane is responsible for managing user access to Databricks resources, while the data plane is responsible for storing and processing data.\n",
    "\n",
    "- B. The control plane is responsible for scaling and managing Databricks clusters, while the data plane is responsible for running user workloads.\n",
    "\n",
    "- C. The control plane is responsible for managing Databricks infrastructure and resources, while the data plane is responsible for data processing and storage. ***\n",
    "\n",
    "- D. The control plane is responsible for providing a user interface for interacting with Databricks resources, while the data plane is responsible for running automated tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
