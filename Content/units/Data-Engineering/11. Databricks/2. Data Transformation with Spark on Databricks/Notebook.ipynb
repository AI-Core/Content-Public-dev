{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation with Spark on Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transformation is a critical step in the journey from raw data to actionable insights. It involves the process of cleaning, enriching, and structuring data to make it suitable for analysis. The importance of data transformation lies in its ability to:\n",
    "\n",
    "- **Enhance Data Quality**: Transformations help in cleaning and validating data, ensuring that it is accurate and reliable for downstream analysis\n",
    "\n",
    "- **Enable Analysis**: Well-transformed data is easier to analyze, allowing data scientists and analysts to derive meaningful patterns, trends, and insights\n",
    "\n",
    "- **Support Decision-Making**: Businesses rely on high-quality, transformed data to make informed decisions, optimize processes, and gain a competitive edge\n",
    "\n",
    "> Databricks leverages the power of Spark to ensure that data undergoes these transformations efficiently, providing a seamless and collaborative environment where the full potential of transformed data can be realized for robust analytics and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark Architecture\n",
    "\n",
    "Before we start leveraging Spark to perform data transformation, let's first understand the architecture underlying Apache Spark. Remember, Spark is a unified engine for large-scale distributed data processing on computer clusters.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/SparkArchitecture.png\" width=\"700\" height=\"350\"/>\n",
    "</p>\n",
    "\n",
    "### Cluster Manager\n",
    "\n",
    "> Apache Spark's architecture revolves around a *Cluster Manager*, a central entity that coordinates the distribution of tasks across a computing cluster. The Cluster Manager is responsible for resource allocation and task scheduling.\n",
    " \n",
    "In Databricks, users are abstracted from direct interaction with the Cluster Manager. Users only interact with Databricks to create and configure clusters through an interface, but the platform automatically manages cluster resources, handling tasks like resource allocation and task scheduling.\n",
    "\n",
    "### Spark Application\n",
    "\n",
    "> A *Spark Application* represents the entire computation process performed using Spark. It consists of the driver program (*Spark Driver*) and a set of executor programs (*Spark Executors*). The Spark Application defines the tasks to be executed on the Spark cluster, and it is submitted to the Cluster Manager for execution.\n",
    "\n",
    "Databricks facilitates the submission and management of Spark Applications. Users define and execute Spark Applications through Databricks notebooks, allowing them to seamlessly work with Spark's computation processes. Databrcisk manages the Spark Application lifecycle, including job submission and execution.\n",
    "\n",
    "### Spark Executors\n",
    "\n",
    "> **Spark Executors** are worker nodes within the cluster responsible for executing tasks. Executors manage data partitions in memory and store intermediate results. They enhance performance by processing data close to where it is stored, minimizing data movement across the network.\n",
    "\n",
    "Databricks transparently managed Spark Executors. When users execute Spark jobs, the Databricks platform dynamically allocates and oversees Spark Executors on the underlying infrastructure. Users do not need to manually configure or monitor individual Executors.\n",
    "\n",
    "### Spark Driver\n",
    "\n",
    "> The **Spark Driver** is a central control program that manages the overall execution of a Spark job. It communicates with the Cluster Manager to acquire resources and coordinates tasks across Spark Executors. The Spark Driver is responsible for overseeing the execution flow and collection final results.\n",
    "\n",
    "Users initiate the Spark Driver through Databricks notebooks or jobs. Code is written and executed in notebooks, and Databricks  coordinates with the Spark Driver.\n",
    "\n",
    "### Spark Session\n",
    "\n",
    "> The *Spark Session* serves as the entry point for interacting with Spark. It manages configuration settings and provides a unified interface for executing various operations. Spark Session facilitate working with different Spark components to offer a user-friendly experience.\n",
    "\n",
    "Databricks abstract the concept of Spark Session for users. Starting a Spark Session is implicit when running code in a notebook cell. Databricks handles the creation of the Spark Session behind scenes, ensuring users can switch between Spark SQL, Python, and other components without explicit session management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL vs PySpark\n",
    "\n",
    "Throughout this lesson, we will use examples of both Spark SQL and PySpark, as most data transformation can be achieved using either of them. This flexibility allows users to choose the approach that best aligns with their preference, expertise, and specific data processing requirements. Whether you will favour the declarative nature of SQL or the programmatic flexibility of Python, Databricks accommodates both.\n",
    "\n",
    "> **Spark SQL** is a module in Spark designed for structured data processing. It allows users to execute SQL queries on Spark data, providing a high-level interface for working with structure and semi-structured data.\n",
    "\n",
    "Spark SQL is ideal for scenarios where you want to leverage the familiarity and expressiveness of SQL for querying and analyzing data. It's particularly well-suited for structured datasets and situations where SQL-like operations are preferable.\n",
    "\n",
    "> **PySpark** is the Python API for Spark. It enables Python developers to harness the power of Spark for distributed data processing. PySpark provides a programmatic interface for working with Spark, allowing more flexibility in expressing complex data transformations and analytics.\n",
    "\n",
    "PySpark is versatile and can be employed when you need more control and customization in your data processing tasks. It's suitable for scenarios where Python is the preferred language, or when you need to integrate Spark with Python-based libraries and tools.\n",
    "\n",
    "As we've seen before, Databricks provides a unified platform where you can seamlessly switch between Spark SQL and PySpark within the same notebook environment. This is because both Spark SQL and PySpark operate on the underlying concept of *DataFrames*. This serves as a bridge, allowing you to transition between the two of them using the DataFrame API.\n",
    "\n",
    "In the previous lesson, we focus our attention on relational entities in Databricks. While effective, traditional data structures such as databases and tables are more limited to SQL-based operations. With DataFrames, you can perform data manipulation and transformation using Spark SQL operations, benefiting from SQL-like syntax, then switch to PySpark to apply more programmatic and customized transformations, all on the same DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames in Apache Spark\n",
    "\n",
    "> A **DataFrame** is a distributed collection of data organized into named columns, similar to a table in a relational database. DataFrames serve as a fundamental abstraction, providing a structured and tabular representation o data.\n",
    "\n",
    "The key features of DataFrames are:\n",
    "\n",
    "- **Distributed Nature**: DataFrames in Spark are distributed across a cluster of machines, allowing for parallel processing. This distribution enables Spark to handle large-scale datasets by dividing them into smaller partitions and processing them in parallel.\n",
    "\n",
    "- **Immutable Structure**: DataFrames are immutable, meaning their structure cannot be changed once created. However, you can perform transformations on a DataFrame to create a new DataFrame with the desired changes. This immutability ensures data consistency and facilitates the construction of a lineage of transformations.\n",
    "\n",
    "- **Lazy Evaluation**: Spark employs lazy evaluation, meaning that transformations on DataFrames are not executed immediately. Instead, Spark builds a logical execution plan, and the actual computation is deferred until an action is triggered. This optimization enhances performance by minimizing unnecessary computations.\n",
    "\n",
    "- **Schema**: DataFrames have a well-defined schema that specifies the names and types of columns. The schema provides structure to the data, allowing Spark to optimize query execution and enabling users to express complex transformations in a declarative manner.\n",
    "\n",
    "DataFrames in Spark can be created from various data sources, including structured data formats like `CSV` and `JSON`, or external databases. We will look in details at this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Loading Data\n",
    "\n",
    "In the data transformation process, reading and loading data play a pivotal role. In this section, we will learn how to read and load data from various file types and examine practical examples.\n",
    "\n",
    "### Reading `JSON` Data\n",
    "\n",
    "#### Reading a Single `JSON` File\n",
    "\n",
    "Let's start by exploring how to read `JSON` data. We'll cover different scenarios, including reading a single `JSON` file, a directory of `JSON` files, and multiple `JSON` files using a wildcard.\n",
    "\n",
    "When dealing with a single `JSON` file, PySpark DataFrames provides a straightforward method for reading and interacting with the data. The syntax is as it follows: \n",
    "\n",
    "```python\n",
    "json_data_single_file = spark.read.json(\"path/to/single/json/file\")\n",
    "```\n",
    "\n",
    "Let's see how we would use this in Databricks. Begin by downloading [this example `JSON` file](). Next, import it into Databricks using the **Data** explorer. In the **Data** explorer tab, use the **Create Table** button. Utilize the drag-and-drop functionality to upload the previously downloaded file. Note and copy the path at which the file will be uploaded within Databricks.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/CreateTable.png\" width=\"700\" height=\"550\"/>\n",
    "</p>\n",
    "\n",
    "Click the **Create Table with UI** button, select a cluster to preview the table, and click **Preview Table**. This should dislay the table preview and the inferred file type. Finally, click **Create Table** to finish.\n",
    "\n",
    "Now, navigate to a Databricks Notebook and run the following PySpark code to read in the uploaded `JSON` file:\n",
    "\n",
    "`json_data_from_dbfs1 = spark.read.json('/FileStore/tables/single_json_file_1.json')`\n",
    "\n",
    "Replacing the file path with your specific file path.\n",
    "\n",
    "#### Checking DataFrame Contents\n",
    "\n",
    "After creating the DataFrame, you might want to inspects its contents. You can use the `show()` method to display the first few rows of the DataFrame: `json_data_from_dbfs1.show()`. \n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/ShowCommand.png\" width=\"700\" height=\"200\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "This command prints a tabular representation of the DataFrame, providing an overview of the data's structure. You can adjust the number of rows displayed by specifying the desired value within the `show()` method (e.g., `show(10)` for the first 10 rows).\n",
    "\n",
    "Alternatively, for a more interactive exploration, you can use the `display()` method:\n",
    "\n",
    "`display(json_data_from_dbfs1)`\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/DisplayTable.png\" width=\"700\" height=\"200\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "The `display()` method provides a feature-rich interface for exploring and interacting with the DataFrame, including filtering, sorting, and visualizations. It's a powerful tool for a comprehensive examination of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading a Directory of `JSON` Files\n",
    "\n",
    "To read a directory of `JSON` files, you can use a similar approach by specifying the directory path:\n",
    "\n",
    "`json_data_directory = spark.read.json(\"path/to/json/files/directory\")`\n",
    "\n",
    "This command reads all the `JSON` files in the specified directory into a PySpark DataFrame. Note, this command assumes that the entire directory contains only `JSON` files.\n",
    "\n",
    "Let's look at an example to illustrate the process. Follow these steps:\n",
    "\n",
    "- Begin by downloading [this file](), and [this file](), representing two different `JSON` files\n",
    "\n",
    "- Import the files into Databricks using the **Data** explorer. In the **Data** explorer tab, make sure to modify the **DBFS target directory** to create a new folder where you can store your `JSON` files. See the example below for updating the first file:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/NewDirectory.png\" width=\"700\" height=\"550\"/>\n",
    "</p>\n",
    "\n",
    "- Upload both files to the same directory\n",
    "\n",
    "- Run the following command to read in all the files in the directory: `json_directory = spark.read.json('/FileStore/tables/json_files')`\n",
    "\n",
    "- Visualize the output of this command use the `display()` command\n",
    "\n",
    "#### Reading Multiple `JSON` Files Using a Wildcard\n",
    "\n",
    "If you have multiple `JSON` files in a directory, that also contains other data types, and you want to read only the `JSON` files you can use the following wildcard (`*`) syntax:\n",
    "\n",
    "`json_data_multiple_files = spark.read.json(\"path/to/json/files/*.json\")`\n",
    "\n",
    "This command reads all `JSON` files matching the wildcard pattern into a PySpark DataFrame.\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading `CSV` Data\n",
    "\n",
    "When dealing with tabular data, especially in scenarios where structured data is stored in `CSV` format, Spark provides efficient methods for reading and loading such data into Spark DataFrames.\n",
    "\n",
    "#### Reading a Single CSV File\n",
    "\n",
    "To read a single `CSV` file into a DataFrame, you can use the following syntax:\n",
    "\n",
    "```python\n",
    "# Read a single CSV file into a PySpark DataFrame\n",
    "csv_data_single_file = spark.read.csv(\"path/to/single/csv/file.csv\", \n",
    "                                     header=True, \n",
    "                                     inferSchema=True, \n",
    "                                     sep=\";\")\n",
    "```\n",
    "\n",
    "This command reads a `CSV` file into a PySpark DataFrame. Here are some key parameters:\n",
    "\n",
    "- `header=True`: Indicates that the first row contains column headers\n",
    "- `inferSchema=True`: Attempts to infer the schema of the data\n",
    "- `sep=\";\"`: Specifies the column delimiter. The default value is a comma (`,`), but in this example, it's set to a semicolon (`;`).\n",
    "\n",
    "Other possible values for `sep` include, but are not limited to: `\\t` for tab, `\" \"` for space, etc.\n",
    "\n",
    "Let's look at an example to illustrate the process. Follow these steps:\n",
    "\n",
    "- Begin by downloading [this CSV file]()\n",
    "\n",
    "- Import the file into Databricks using the **Data** explorer. In the preview table tab, make sure to enable **First row is header** and the **Infer schema** options before creating the table.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/CreateCSVTable.png\" width=\"600\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "- Run the following command in a Databricks Notebook to read in the `CSV` file: `csv_data_single_file = spark.read.csv(\"dbfs:/FileStore/tables/username.csv\", header=True, inferSchema=True, sep=\";\")`\n",
    "\n",
    "- Visualize the output of this command use the `display()` command\n",
    "\n",
    "#### Reading `CSV` Files from a Directory\n",
    "\n",
    "If you have multiple `CSV` files in a directory, you can read them all into a DataFrame using a similar approach:\n",
    "\n",
    "`csv_data_directory = spark.read.csv(\"path/to/csv/files/directory\", header=True, inferSchema=True)`\n",
    "\n",
    "This command reads all CSV files in the specified directory into a DataFrame.\n",
    "\n",
    "#### Handling `CSV` Files with Custom Schema\n",
    "\n",
    "In some cases, you might want to specify a custom schema for your `CSV` data. You can achieve this by defining a schema and using it during the read operation:\n",
    "\n",
    "``` python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define a custom schema\n",
    "custom_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    # Add more fields as needed\n",
    "])\n",
    "\n",
    "# Read CSV data with the custom schema\n",
    "csv_data_custom_schema = spark.read.csv(\"path/to/csv/file.csv\", header=True, schema=custom_schema)\n",
    "\n",
    "````\n",
    "This command reads a `CSV` file into a PySpark DataFrame, applying the specified custom schema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data from Cloud Storage\n",
    "\n",
    "Data stored in cloud storage systems such as Amazon S3, Google Cloud Storage (GCS), or Azure Storage can be easily accessed with PySpark. Here's a general approach:\n",
    "\n",
    "- **Store Credentials Safely**: Avoid hardcoding credentials directly in code. Leverage secure methods such as environment variables or secure key storage services.\n",
    "- **Mount Storage to Databricks**: Mount your cloud storage to Databricks, providing a secure way to access data. This involves configuring storage-specific credentials within Databricks.\n",
    "\n",
    "We will look at this workflow in more detail in a future lesson, using Amazon S3 as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL for Reading Data\n",
    "\n",
    "In addition to using PySpark DataFrames, you can leverage Spark SQL to query and interact with your data using SQL commands. Let's explore how you can use Spark SQL to read `JSON` and `CSV` files.\n",
    "\n",
    "You can use Spark SQL to query `JSON` files directly. The following example illustrates how to select all columns from a `JSON` file:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM json.`path/to/json/file`\n",
    "```\n",
    "This SQL command allows you to query the contents of the `JSON` file using Spark SQL.\n",
    "\n",
    "Similarly, Spark SQL enables you to query `CSV` files using SQL commands. For instance, you can use the following SQL command to select all columns from a `CSV` file:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM csv.`path/to/csv/file`\n",
    "```\n",
    "This SQL command provides an alternative way to interact with your `CSV` data using Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Transformation\n",
    "\n",
    "In the data analysis process, ensuring that the data is clean and appropriately formatted is crucial. Raw data often comes with inconsistencies, missing values, or formats that are not conducive to analysis. Cleaning and transforming data involve preparing it for further analysis, enhancing its quality, and making it suitable for downstream processes. In this section we will look at different techniques for cleaning and transforming data using PySpark.\n",
    "\n",
    "Let's start by considering the following example DataFrame representing information about individuals:\n",
    "\n",
    "```python\n",
    "# Create an example DataFrame\n",
    "data = [\n",
    "    [\"John\", \"Doe\", 30, \"Male\", \"$500.00\", \"2022-01-01 08:30:00\", [\"Street1\", \"New York\", \"12345\", \"USA\"], \"john.doe@example.com\", \"Married\"],\n",
    "    [\"Alice\", \"Smith\", 25, \"Female\", \"$700.50\", \"2022-01-02 15:45:30\", [\"Street2\", \"San Francisco\", \"54321\", \"USA\"], \"alice.smith@example.com\", \"Single\"],\n",
    "    [\"Bob\", \"Jones\", 28, \"Male\", \"$650.00\", \"2022-01-03 12:15:00\", [\"Street3\", \"Los Angeles\", \"67890\", \"USA\"], \"Unknown\", \"Single\"],\n",
    "    [\"Eve\", \"White\", 35, \"Female\", \"$600.75\", \"2022-01-04 10:00:45\", [\"Street4\", \"New Haven\", \"00000\", \"USA\"], \"eve.white@example.com\", \"User Info Error\"]\n",
    "]\n",
    "columns = [\"First Name\", \"Last Name\", \"Age\", \"Gender\", \"Salary\", \"Timestamp\", \"Location\", \"Email\", \"Status\"]\n",
    "\n",
    "example_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show the original DataFrame\n",
    "print(\"Original DataFrame:\")\n",
    "example_df.show()\n",
    "```\n",
    "\n",
    "This DataFrame will serve as our example throughout this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Replacing Missing Values\n",
    "\n",
    "Handling missing values is a common use case in data cleaning. The `replace` method allows us to replace specific values with designated replacements. In the example DataFrame, suppose we want to replace all occurrences of `'User Info Error'` with `None` in the `Status` column:\n",
    "\n",
    "```python\n",
    "cleaned_df = example_df.replace({'User Info Error': None}, subset=['Status'])\n",
    "```\n",
    "\n",
    "In this command, `replace` initiates the replacement operation. `{'User Info Error': None}` defines the replacement rule, indicating that occurrences of `'User Info Error'` should be replaced with `None`. `subset=['Status']` specifies the column where the replacement should occur.\n",
    "\n",
    "### 2. Updating Data Points\n",
    "\n",
    "The `replace` method is not limited to handling null values; it can also be employed to update existing values. For instance, suppose we want to update all occurrences of `'Unknown'` in the `Email` column to `'Pending'`:\n",
    "\n",
    "```python\n",
    "cleaned_df = cleaned_df.replace({'Unknown': 'Pending'}, subset=['Email'])\n",
    "```\n",
    "\n",
    "In this example, `{'Unknown': 'Pending'}` defines the replacement rule, indicating that occurrences of `'Unknown'` should be replaced with `'Pending'` in the `Email` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using `regexp_replace` for Column Transformations\n",
    "\n",
    "Column transformations are essential for manipulating text-based columns. The `regexp_replace` function enables us to apply regular expression patterns to modify or clean column values.\n",
    "\n",
    "The default syntax is as follows:\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"ColumnName\", regexp_replace(\"ColumnName\", \"pattern\", \"replacement\")\n",
    "```\n",
    "Let's break down the components:\n",
    "\n",
    "- `.withColumn(\"ColumnName\", ...)` : This method is used to add or replace a column in the DataFrame. In this case, it specifies that the operation is targeting a column named `\"ColumnName\"`.\n",
    "\n",
    "- `regexp_replace(\"ColumnName\", \"pattern\", \"replacement\")`: This is the PySpark `regexp_replace` function. It takes three arguments:\n",
    "  - `\"ColumnName\"`: The name of the column to which the replacement will be applied\n",
    "  - `\"pattern\"`: The regular expression pattern to search for in the values of the specified column\n",
    "  - `\"replacement\"`: The string that will replace the matched pattern in the column values\n",
    "\n",
    "Let's look at an example in our `cleaned_df` DataFrame:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "cleaned_df = cleaned_df.withColumn(\"Salary\", regexp_replace(\"Salary\", \"\\\\$\", \"\")\n",
    "```\n",
    "In the example above, the `regexp_replace` function removes dollar signs from the `\"Salary\"` column in the PySpark DataFrame. The regex pattern `\\\\$` represents the dollar sign, and it is replaced with an empty string `\"\"`.\n",
    "\n",
    "### 4. Casting Columns to Different Data Types\n",
    "\n",
    "Casting columns to different data types is a common operation, especially when the inferred schema needs adjustment. We can cast columns to ensure they are of the correct data type. The default syntax is as it follows:\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"ColumnName\", df[\"ColumnName\"].cast(\"desired_type\"))\n",
    "```\n",
    "The `cast` function is applied to convert the specified column to the desired data type. The argument, `\"desired_type\"`, represents the target data type to which you want to cast the column. There are different types of data casting, including:\n",
    "\n",
    "- **Numeric Types**: Casting to numeric types, such as `integer`, `double`, `float`, etc., is common when dealing with numerical data\n",
    "\n",
    "- **String Type**: You can cast a column to the `string` type if you want to treat it as text\n",
    "\n",
    "- **Boolean Type**: Casting to `boolean` is suitable for columns representing true/false or binary data\n",
    "\n",
    "- **Timestamp Type**: For columns containing timestamp or date data, casting to `timestamp` is useful\n",
    "\n",
    "> Before casting different columns to new data types, it is useful to see exactly which data type is assigned to each column. You can do so using the `printSchema()` command.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/printSchema.png\" width=\"700\" height=\"300\"/>\n",
    "</p>\n",
    "\n",
    "In the example above, we can observe that the `Salary` column in our `cleaned_df` DataFrame is of type string, but as we removed the `$` sign from each value in the column in the previous step, we can now change the data type for this column to a numeric one:\n",
    "\n",
    "```python\n",
    "cleaned_df = cleaned_df.withColumn(\"Salary\", cleaned_df[\"Salary\"].cast(\"float\"))\n",
    "```\n",
    "\n",
    "If we know rerun the `printSchema()` command we should see the `Salary` column being of type float:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/NewDataType.png\" width=\"700\" height=\"300\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Transforming Columns to Timestamp Type\n",
    "\n",
    "Transforming columns to `timestamp` type is crucial for handling temporal data. The `to_timestamp` function is used to convert a `string` representation of a timestamp into the `timestamp` type. While the `cast` function allows casting to different data types, including timestamps, if you need to handle timestamp or date-related data, `to_timestamp` is the appropriate choice. This is because `to_timestamp` is specific to timestamp-related transformations and ensures the correct interpretation of time-related data.\n",
    "\n",
    "The default syntax is:\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"NewTimestampColumn\", to_timestamp(\"ExistingTimestampColumn\"))\n",
    "```\n",
    "\n",
    "Let's break down the components of this syntax:\n",
    "\n",
    "- `\"NewTimestampColumn\"`: Specifies the name of the new column that will store the converted timestamps\n",
    "- `to_timestamp(\"ExistingTimestampColumn\")`: This is the PySpark `to_timestamp` function. It takes one argument:\n",
    "  - `\"ExistingTimestampColumn\"`: The name of the existing column containing string representations of timestamps that you want to convert\n",
    "\n",
    "If we take a look at the previously ran `printSchema()` command for our `cleaned_df` DataFrame we can see that the `Timestamp` column is of type string. We can change this using the following command:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "cleaned_df = cleaned_df.withColumn(\"Timestamp\", to_timestamp(\"Timestamp\"))\n",
    "```\n",
    "In this example, the `\"Timestamp\"` column is transformed to a timestamp type using the `to_timestamp` function.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/ToTimestamp.png\" width=\"700\" height=\"300\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Creating New Columns Using Array Functions\n",
    "\n",
    "Generating new insights and integrating data often requires the creation of new columns. In PySpark, we leverage *array functions** and concatenation to derive meaningful information.\n",
    "\n",
    "> **Array functions** operate on arrays, which are ordered collections of elements. These functions become invaluable when working with columns that contain arrays of values. Let's delve into some common array functions in PySpark:\n",
    "\n",
    "#### `array`: Creating a new Array Column\n",
    "\n",
    "The `array` function creates a new array column. In the example below, we create a column named `\"new_array_column\"` by combining values from `\"column1\"` and `\"column2\"`:\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"new_array_column\", array(\"column1\", \"column2\"))\n",
    "```\n",
    "\n",
    "The output of such command would look like this:\n",
    "\n",
    "```markdown\n",
    "| column1 | column2 | new_array_column |\n",
    "|---------|---------|------------------|\n",
    "|   val1  |   val2  | [val1, val2]     |\n",
    "|   val3  |   val4  | [val3, val4]     |\n",
    "|   val5  |   val6  | [val5, val6]     |\n",
    "```\n",
    "\n",
    "We can use the `array` function to create a new column called `Full Name` in our `clean_df` DataFrame from the `First Name` and `Last Name` columns:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import array\n",
    "cleaned_df = cleaned_df.withColumn(\"Full Name Array\", array(\"First Name\", \"Last Name\"))\n",
    "```\n",
    "\n",
    "#### `array_contains`: Checking Array for a Value\n",
    "\n",
    "The `array_contains` function checks if an array contains a specific value. In the example below, we check if the `\"array_column\"` contains the value `\"value\"`:\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"contains_value\", array_contains(\"array_column\", \"value\"))\n",
    "```\n",
    "\n",
    "The output of this command would look like this:\n",
    "\n",
    "```markdown\n",
    "| array_column    | contains_value  |\n",
    "|-----------------|-----------------|\n",
    "| [val1, val2]    | False           |\n",
    "| [val3, val4]    | True            |\n",
    "| [val5, val6]    | False           |\n",
    "```\n",
    "\n",
    "#### `size`: Getting Size of an Array\n",
    "\n",
    "The `size` function returns the size (length) of an array. In the example below, we create a column named `\"array_size\"` to store the size of the `\"array_column\"`:\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"array_size\", size(\"array_column\"))\n",
    "```\n",
    "The output of this command would look like this:\n",
    "\n",
    "```markdown\n",
    "| array_column    | array_size |\n",
    "|-----------------|------------|\n",
    "| [val1, val2]    | 2          |\n",
    "| [val3, val4]    | 2          |\n",
    "| [val5, val6]    | 2          |\n",
    "```\n",
    "\n",
    "#### `concat`: Concatenating Multiple Arrays/Values\n",
    "\n",
    "The `concat` function can concatenate multiple arrays into a single array. In the example below, we create a column named `\"concatenated_arrays\"` by combining values from `\"array1\"` and `\"array2\"`:\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"concatenated_arrays\", concat(\"array1\", \"array2\"))\n",
    "```\n",
    "The output of this command would look like this:\n",
    "\n",
    "```markdown\n",
    "| array1    | array2    | concatenated_arrays  |\n",
    "|-----------|-----------|-----------------------|\n",
    "| [v1, v2]  | [v3, v4]  | [v1, v2, v3, v4]      |\n",
    "| [v5, v6]  | [v7, v8]  | [v5, v6, v7, v8]      |\n",
    "| [v9, v10] | [v11, v12]| [v9, v10, v11, v12]   |\n",
    "```\n",
    "\n",
    "> The `concat` function in PySpark is not limited to concatenating arrays only; it can be employed to concatenate various elements, including strings and literals. Here's a breakdown of its utility:\n",
    "\n",
    "```python\n",
    "# Example: Concatenating strings from two columns\n",
    "df = df.withColumn(\"concatenated_strings\", concat(\"column1\", \"column2\"))\n",
    "\n",
    "# Example: Concatenating strings and literals\n",
    "df = df.withColumn(\"combined_values\", concat(\"column1\", lit(\" - \"), \"column2\"))\n",
    "```\n",
    "\n",
    "Let's explore the examples and their corresponding output:\n",
    "\n",
    "- Example 1: Concatenating Strings from Two Columns:\n",
    "\n",
    "```markdown\n",
    "| column1 | column2 | concatenated_strings |\n",
    "|---------|---------|----------------------|\n",
    "|   val1  |   val2  |       val1val2       |\n",
    "|   val3  |   val4  |       val3val4       |\n",
    "|   val5  |   val6  |       val5val6       |\n",
    "```\n",
    "\n",
    "- Example 2: Concatenating Strings and Literals\n",
    "\n",
    "```markdown\n",
    "| column1 | column2 | combined_values  |\n",
    "|---------|---------|-------------------|\n",
    "|   val1  |   val2  |   val1 - val2     |\n",
    "|   val3  |   val4  |   val3 - val4     |\n",
    "|   val5  |   val6  |   val5 - val6     |\n",
    "```\n",
    "\n",
    "In the first example, we concatenate the strings from two columns (`column1` and `column2`). In the second example, we combine strings with a literal hyphen (`\" - \"`) between them.\n",
    "\n",
    "Before, we have created a new column `Full Name Array` in our `cleaned_df` DataFrame. Alternatively to that approach, we can create a new column called `Full Name` that will use the `concat` function to concatenate the information from the `First Name` and `Last Name` columns. Instead of being a type array as with the previous example, this new column will simply be a string.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import concat, lit\n",
    "cleaned_df = cleaned_df.withColumn(\"Full Name\", concat(\"First Name\", lit(\" \"), \"Last Name\"))\n",
    "```\n",
    "\n",
    "#### `explode`: Transforming Array into Rows\n",
    "\n",
    "The `explode` function in PySpark is specifically designed for transforming columns containing arrays into rows. It duplicates the other columns for each element in the array, creating a new row for each array element.\n",
    "\n",
    "Let's consider the following DataFrame:\n",
    "\n",
    "```markdown\n",
    "| column1 |       array_column        |\n",
    "|---------|---------------------------|\n",
    "|   val1  |   [\"item1\", \"item2\"]      |\n",
    "|   val2  |   [\"item3\", \"item4\"]      |\n",
    "|   val3  |   [\"item5\", \"item6\"]      |\n",
    "```\n",
    "\n",
    "If we now apply the following transformation to this DataFrame:\n",
    "\n",
    "```python\n",
    "df = df.select(\"column1\", explode(\"array_column\").alias(\"exploded_values\"))\n",
    "```\n",
    "\n",
    "After applying the `explode` function:\n",
    "\n",
    "```markdown\n",
    "| column1 |  exploded_values  |\n",
    "|---------|-------------------|\n",
    "|   val1  |      \"item1\"      |\n",
    "|   val1  |      \"item2\"      |\n",
    "|   val2  |      \"item3\"      |\n",
    "|   val2  |      \"item4\"      |\n",
    "|   val3  |      \"item5\"      |\n",
    "|   val3  |      \"item6\"      |\n",
    "```\n",
    "\n",
    "In this example, the `explode` function creates new rows for each element in the `array_column`, duplicating the values in the `column1` for each exploded row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Unfolding Information from Arrays\n",
    "\n",
    "When working with arrays in PySpark, you may encounter scenarios where information is stored in array columns, and you want to unfold or extract that information into separate columns. The `withColumn` method, combined with array indexing, enables you to achieve this transformation. The general syntax is as follows:\n",
    "\n",
    "```python\n",
    "# Unfolding array information into separate columns\n",
    "df = df.withColumn(\"NewColumn1\", col(\"ArrayColumn\")[index1]) \\\n",
    "       .withColumn(\"NewColumn2\", col(\"ArrayColumn\")[index2]) \\\n",
    "       .withColumn(\"NewColumn3\", col(\"ArrayColumn\")[index3])\n",
    "# Repeat for each desired column\n",
    "```\n",
    "\n",
    "In the syntax above:\n",
    "\n",
    "- `\"NewColumn1\"`, `\"NewColumn2\"`, ...: The names of the new columns you want to create\n",
    "- `\"ArrayColumn\"`: The name of the column containing the array\n",
    "- `index1`, `index2`, ...: The indices indicating which elements from the array should populate the new columns\n",
    "\n",
    "Let's take a look now at our example `cleaned_df`, which has an array column named `\"Location\"` containing street, city, postcode, and country information:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Unfolding the Address array into separate columns\n",
    "cleaned_df = cleaned_df.withColumn(\"Street\", col(\"Location\")[0]) \\\n",
    "            .withColumn(\"City\", col(\"Location\")[1]) \\\n",
    "            .withColumn(\"Postcode\", col(\"Location\")[2]) \\\n",
    "            .withColumn(\"Country\", col(\"Location\")[3])\n",
    "```\n",
    "After applying this transformation, the DataFrame will have new columns: `\"Street\"`, `\"City\"`, `\"Postcode\"`, and `\"Country\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Renaming and Dropping Columns\n",
    "\n",
    "Renaming and dropping columns are common operations to enhance DataFrame clarity and exclude unnecessary information. In PySpark, you can achieve this using the `withColumnRenamed` method for renaming and the `drop` method for dropping columns.\n",
    "\n",
    "The general syntax for renaming columns is:\n",
    "\n",
    "```python\n",
    "# Renaming a single column\n",
    "df = df.withColumnRenamed(\"OldColumnName\", \"NewColumnName\")\n",
    "\n",
    "# Renaming multiple columns\n",
    "df = df.withColumnRenamed(\"OldColumnName1\", \"NewColumnName1\") \\\n",
    "       .withColumnRenamed(\"OldColumnName2\", \"NewColumnName2\")\n",
    "# Repeat for each column pair\n",
    "\n",
    "```\n",
    "The general syntax for dropping columns is:\n",
    "\n",
    "```python\n",
    "# Dropping a single column\n",
    "df = df.drop(\"ColumnName\")\n",
    "\n",
    "# Dropping multiple columns\n",
    "df = df.drop(\"ColumnName1\", \"ColumnName2\", ...)\n",
    "# List all columns you want to drop\n",
    "```\n",
    "\n",
    "Let's drop some redundant columns in our example `cleaned_df` DataFrame:\n",
    "\n",
    "```python\n",
    "cleaned_df = cleaned_df.drop(\"Location\", \"Full Name Array\", \"First Name\", \"Last Name\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Reordering Columns\n",
    "\n",
    "Reordering columns can enhance DataFrame structure, providing better organization and simplifying data exploration. The syntax is as follows:\n",
    "\n",
    "```python\n",
    "df = df.select(\"Column1\", \"Column2\", ...)\n",
    "```\n",
    "Let's apply this to our example `cleaned_df` DataFrame. You can begin by observing the current column ordering using the `printSchema()` function, then decide on the desired ordering:\n",
    "\n",
    "```python\n",
    "cleaned_df = cleaned_df.select(\"Full Name\", \"Age\", \"Gender\", \"Email\", \"Salary\", \"Street\", \"City\", \"Postcode\", \"Country\", \"Status\", \"Timestamp\")\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
