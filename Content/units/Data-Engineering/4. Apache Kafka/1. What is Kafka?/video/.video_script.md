- as any software company grows, it is going to develop more systems that require data from other systems

_Show diagram of many different systems passing data between each other_

- if you have 4 systems generating data, and 5 systems that consume that data, then that means that you will need to build and maintain 20 different integrations, and when I add another target I am going to need to add another 4 more integrations from the other sources
- this can get out of hand really quickly
- let alone the fact that all of those integrations need to be highly scalable and fail-safe
- instead, it would be much better if we could simply push all data of a specific type to a particular place in a standardised format, and then pull from there when we need it
- that's what kafka is for

_Show network of integrations in between replaced by Kafka_

- kafka is a high throughput, distributed message queue

_Show kafka homepage_

- it can be used to move data between systems in a standardised way
- it can move millions of messages between systems in real time
- it can be used for streaming data in and out in real time, or it can be used to store data which will be processed in batch later

## Components and terminology

I want to show you a high level diagram of the components in the kafka ecosystem before I go into more detail on each of them

### Streams of events

- kafka is going to consume and distribute messages which are streams of events
- events might be something like a JSON object of someone making an order containing the details such as their user id, order id and timestamp
- kafka events could contain any kind of information really, they could be plaintext, simple numerical values, or even image data

### Brokers

- and kafka is able to ingest billions of these events at a very high throughput because it is a distributed system
- that means that it works by spreading out the work over a cluster of many different computers networked together
- we might call those computers within the cluster nodes or in the language of kafka, _brokers_
- kafka clusters could have as few as 3 brokers, or as many as thousands
- because the brokers are networked together, once you're connected to one of them, you're connected to the entire cluster
- all together, the cluster acts as a single machine, that can scale up simply by adding more brokers

### Topics

- these brokers store the streams of events in what are called topics
- each topic has a name, typically describing what kind of data it contains
- so you might have a topic called orders for storing events that represent orders made by users
- and you might have another topic called signups for storing signup events
- topics are _logs_ of data. Logs are a very simple data structure that are append only

### Producers and consumers

- once we've created kafka topics, we can create kafka producers to write to any one of them
- and then we can create kafka consumers to read from any of them

# Distributed data storage on kafka

- How is data stored on kafka brokers?

## Partitions

- topics are split into partitions, which store events in order
- each partion is storing the same type of event - whatever type of data the topic contains
- and each event is stored with an automatically incrementing integer id, called an offset
- a topic can contain multiple partitions, as many as you want
<!-- TODO WHY? -->
- and by default data is assigned to a random partition of a topic unless
<!-- WHY? -->
- the order within a partition is always maintained
- if an event has a higher offset, that means it came after than any other event in that partition with a lower offset
- but only within the same partition
- i could put the first 10 events into one partition where they would have offsets 0-9
- but then if i put the next event into another partition, it would have an offset equal to zero
- partitions are immutable
- that means that once data is put into a partition, it cannot be changed
-

## Data replication

- data is replicated across brokers by copying partitions

- only one broker can be the leader for a partition
- the leader can send and recieve data for a partition

- you might hear of ISR partitions which stands for (in-sync replica)
