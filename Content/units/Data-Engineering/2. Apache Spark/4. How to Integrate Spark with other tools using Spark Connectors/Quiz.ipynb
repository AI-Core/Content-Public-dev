{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to integrate Spark with other tools using Spark Connectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Apache Maven?\n",
    "\n",
    "- A) A web server for hosting Java applications\n",
    "- B) A build automation tool for Java projects ***\n",
    "- C) A programming language for building web applications\n",
    "- D) A version control system for managing source code\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the purpose of Maven Coordinates in a project file?\n",
    "\n",
    "- A) To specify the name of the project.\n",
    "- B) To specify the location of the source code.\n",
    "- C) To identify the project's dependencies. ***\n",
    "- D) To define the project's build settings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the three components of Maven Coordinates?\n",
    "\n",
    "- A) Group ID, Artifact Type, and Version\n",
    "- B) Artifact ID, Packaging Type, and Version\n",
    "- C) Group ID, Artifact ID, and Version ***\n",
    "- D) Group Name, Artifact Type, and Version Number"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the purpose of Group ID in Maven Coordinates?\n",
    "\n",
    "- A) To identify the version of the artifact.\n",
    "- B) To specify the packaging type of the artifact.\n",
    "- C) To identify the specific module within the organization that produces the artifact.\n",
    "- D) To identify the group or organization that owns the artifact. ***\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the purpose of Artifact ID in Maven Coordinates?\n",
    "\n",
    "- A) To identify the specific module within the organization that produces the artifact. ***\n",
    "- B) To specify the packaging type of the artifact.\n",
    "- C) To identify the group or organization that owns the artifact.\n",
    "- D) To identify the version of the artifact."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the purpose of Version in Maven Coordinates?\n",
    "\n",
    "- A) To identify the specific module within the organization that produces the artifact.\n",
    "- B) To specify the packaging type of the artifact.\n",
    "- C) To identify the group or organization that owns the artifact.\n",
    "- D) To identify the version of the artifact. ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Maven Central Repository?\n",
    "\n",
    "- A) A central location for storing and sharing Maven artifacts.***\n",
    "- B) A tool for automating the build process in Maven.\n",
    "- C) A plugin used for deploying Maven artifacts.\n",
    "- D) A plugin used for generating documentation for Maven artifacts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you submit external packages to PySpark?\n",
    "\n",
    "- A) Use the `pyspark-submit` command with the `--packages` option. ***\n",
    "- B) Add the package name to the requirements.txt file.\n",
    "- C) Download and install the package manually on each worker node.\n",
    "- D) Add the package name to the pyspark-shell command line arguments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the purpose of the `PYSPARK_SUBMIT_ARGS` environment variable in PySpark?\n",
    "\n",
    "- A) To specify the configuration settings for the Spark driver program.\n",
    "- B) To specify the location of the PySpark installation.\n",
    "- C) To specify the configuration settings for the Spark worker nodes.\n",
    "- D) To specify the command-line arguments for the pyspark-submit command. ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the recommended way to access data stored in AWS S3 from PySpark?\n",
    "\n",
    "- A) Download the data to a local file system and load it into PySpark.\n",
    "- B) Use the AWS CLI to copy the data to the local file system and load it into PySpark.\n",
    "- C) Use the Hadoop S3A connector to directly access the data from PySpark. ***\n",
    "- D) Use the AWS SDK to read the data into PySpark."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
