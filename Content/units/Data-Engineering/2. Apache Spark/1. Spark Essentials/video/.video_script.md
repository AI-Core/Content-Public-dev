_Webcam view_

- a common way to use Spark is through it's python library, pyspark

_Switch to screen capture_

- you can download it from the official website or using a package manager

_Download the package and then uncompress it_

- you can check that spark has installed correctly by using the findspark library

_Show conda install -c conda-forge findspark_

- if you haven't configured spark correctly, this will throw some error

- one of the most common issues relates to an environment variable `spark*HOME* not being set correctly

_Open up bashrc_

- so you should export that environment variable in your ~/.bashrc file which runs every time you open a terminal

_Add the export SPARK_HOME... to your bashrc_

- make sure to close and open the terminal or notebook you're working from so that the environment updates

## Setting config with Spark

- you can configure Spark using pyspark's sparkconf class to set things like the resources available to each compute node, the application name, and logging

_Type out the spark config_

```
cfg = (
    pyspark.SparkConf()
    .setMaster(f"local[{multiprocessing.cpu_count()}]")
    .setExecutorEnv(pairs=[("VAR3", "value3"), ("VAR4", "value4")])
    .setIfMissing("spark.executor.memory", "1g")
    .setAppName("TestApp")
    .set("spark.eventLog.enabled", False)
)
```

## Contexts

- previously,

# TODO

- you can only have one spark context in an application
- but you can have many sessions
- the typical use case would be that each user has their own Spark session
- each spark session uses the same underlying context
- if we stop the context from one session, it will then prevent the other sessions from using the context

_Try to run a spark session after killing the spark context_

## Session

- We should use the `builder` attribute to build the obtain appropriate `SparkSession`

_Write builder and return spark session as `session`_

- We should firstly set the config

_Set config_

- and then run `getOrCreate`

_Call getorcreate_

- This wll create a new global `Session` if none exists
<!-- # TODO what is a global session? -->
- If a global `Session` does exist, it will get an instance of it and apply a new configuration to it
- This SparkSession can be used just like the other context objects were historically.

## Data types

- to process data as efficiently as possible, spark has it's own data types

### RDDs

- the fundamental datatype which others are built on top of is the RDD, or the resilient distributed dataset
- they represent data
- that data can be stored and processed on many different machines
- they are resilient in the sense that they are immutable
- that means that instead of changing the RDD on a transformation, a new one is created

<!-- #### Compile-time type safety
- RDDs are what you call compile-time type-safe
- that means that they will alert you of errors before  -->
<!-- TODO -->

5. RDDs are computed lazily

- what that means, that they are not evaluated until you need them
- because of this, Spark can look at the chain of spark transformations and look at where they can rearranged to optimise the execution
- instead of evaluating every transformation as the line that defines them is reached, Spark builds an acyclic graph showing the order and relationship between operations
- for example, if you compute a new value from each row, and then filter them on another column which already existed, spark can swap the order so that the filter comes first, and we have to compute the new value for less rows
- they are only evaluated when we perform what is called an "action"

- an action is something like producing a histrogram, aggregating results together
- a transformation is something like a sort or a filter
<!-- TODo explain difference -->

- we refer to a combination of transformations and actions as an "operation"

- the RDD is the lowest level of abstraction
- as such they allow you very fine grained control over exactly what Spark does
  <!-- TODO distinguish between spark operations and custom code sent to RDDs -->
  <!-- - spark treats the custom code which runs on an RDD as a black box
  <!-- - this means that... -->

- because RDDs don't treat the data as having a particular structure, it means that they can handle processing unstructured data
- however, it also means that it's up to you to specify where exactly within each piece of the dataset Spark will find what it is looking for
- the problem with RDDs is that because they allow such control, it's easy for developers to accidentally introduce inefficiencies or write code that has a chance of failing in production because the types are incorrect, for example
- RDDs also didn't have methods to make common operations like table joins easy

- it's important to understand RDDs because they are the lowest level of abstraction of data in spark, which allow it to run so fast and provide data lineage

- as you use spark, you'll find yourself in the docs, where you can explore all the functionality of RDDs

_Show the RDDs docs_

## Dataframes

- to address this, in Spark 2.0 a new data type called the dataframe was introduced
- dataframes represent structured data dataframes much more like a SQL table or a pandas dataframe than RDDs

- you can create a dataframe from an RDD by doing .toDF on an RDD

_Turn RDD into Dataframe_

- of course, to do this, the RDD has to be a type that makes sense to case into a dataframe
- that is, it needs to be table like with rows and columns
- you can also pass in the column names as the arguments

- you can specify the structure of the data they contain by giving names to columns, which then again makes them easy to use
- dataframes also have methods to perform common methods like joins
- you'll probably recognise lots of these from pandas - in a lot of cases, the methods are the same

- as usual, you can find all of these methods in the docs

_Open the docs_

- having methods of the dataframe, rather than passing a custom function to an RDD is far more delarative
- Meaning that you are telling Spark what to do, not how to do it
- and because it recognises these declarative methods internally, it means that it's easy for Spark to optimise internally, for example by rearranging transformations
- if you tell spark to sort then filter, it will realise that this is an inefficient thing to do as you'll spend time sorting things that you immediately filter out, so it will swap the order of those operations

- the limitation of these dataframes is that the datasets are not strictly typed
- if a value is not the expected type then the processing is going to fail at runtime

- to address this, the dataset type was also introduced
- datasets allow you to specify the type of the records in your dataframe
- this means that spark can detect potential errors at compile time, in advance, rather than at runtime

- When you develop Spark applications, you typically use DataFrames and Datasets, rather than RDDs

- unfortunately, Spark datasets do not exist in python as it is not a strongly typed language
- so we have to use dataframes

## SQL queries on dataframes

- you can use spark to run distributed SQL queries
- to give dataframes a name you can reference in SQL, just call createOrReplaceTempView with that name

_Show creating a temp view_

- then you can call the sql method of the session and pass in usual SQL to run
- this returns you a spark dataframe

- then when you run show

## Spark submit

- In a production product, it is likely that you will not be running the spark code interactively by clicking "run cell"
- instead, you will have all of that code in a file, and you will submit this file to the spark cluster to perform it all together
- to do that, we use a command line utility called _`spark-submit`_

<!-- TODO what is a standard number of executors? -->
