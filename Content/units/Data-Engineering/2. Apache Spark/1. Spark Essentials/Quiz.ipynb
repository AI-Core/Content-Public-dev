{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Essentials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PySpark in Apache Spark and how is it useful?\n",
    "\n",
    "- A) PySpark is a Python library for parallel processing of data, and it is useful because it provides an easy-to-use programming interface for data processing and analysis\n",
    "- B) PySpark is a Python package for distributed computing, and it is useful because it allows Python developers to write Spark jobs in Python instead of Java or Scala ***\n",
    "- C) PySpark is a Python-based cluster manager for Apache Spark, and it is useful because it simplifies the deployment and management of Spark clusters\n",
    "- D) PySpark is a Python-based machine learning library, and it is useful because it provides a powerful set of tools for machine learning and data analysis in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is `findspark` in PySpark and how is it useful?\n",
    "\n",
    "- A) `findspark` is a package that allows users to locate and download Spark libraries, and it is useful because it simplifies the setup process for Spark on local machines\n",
    "- B) `findspark` is a module that automatically finds the location of Spark and sets the necessary environment variables, and it is useful because it makes it easier to integrate PySpark with existing Python environments ***\n",
    "- C) `findspark` is a Python-based cluster manager for Apache Spark, and it is useful because it simplifies the deployment and management of Spark clusters\n",
    "- D) `findspark` is a machine learning library for PySpark, and it is useful because it provides a powerful set of tools for machine learning and data analysis in PySpark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following code snippets correctly shows how to create and configure a SparkConf object in PySpark?\n",
    "\n",
    "- A)\n",
    "\n",
    "`from pyspark import SparkContext`\n",
    "\n",
    "`conf = SparkConf().setAppName(\"MyApp\")`\n",
    "`sc = SparkContext(conf=conf)` ***\n",
    "\n",
    "- B)\n",
    "\n",
    "`from pyspark import SparkConf, SparkContext`\n",
    "\n",
    "`sc = SparkContext(\"local\", \"MyApp\")`\n",
    "\n",
    "- C)\n",
    "\n",
    "`from pyspark import SparkConf, SparkContext`\n",
    "\n",
    "`conf = SparkConf().setAppName(\"MyApp\")`\n",
    "`sc = SparkContext.getOrCreate(conf=conf)`\n",
    "\n",
    "- D)\n",
    "\n",
    "`from pyspark import SparkConf`\n",
    "\n",
    "`conf = SparkConf().setAppName(\"MyApp\").setMaster(\"local\")`\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the purpose of `SparkContext` in PySpark?\n",
    "\n",
    "- A) To provide a connection to a Spark cluster and coordinate the execution of Spark jobs. ***\n",
    "- B) To define a Spark application's configuration settings, such as the application name and the location of input data.\n",
    "- C) To provide a high-level API for working with distributed data in PySpark.\n",
    "- D) To manage the execution of tasks within a Spark job and handle failures and retries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In PySpark, what is the purpose of the `getOrCreate()` method of `SparkSession`?\n",
    "\n",
    "- A) To create a new `SparkSession `instance if one does not already exist, or to return an existing `SparkSession` instance if one has already been created. ***\n",
    "- B) To create a new RDD from a given data source, such as a file or database table.\n",
    "- C) To create a new DataFrame by applying a schema to a given data source, such as a file or database table.\n",
    "- D) To create a new `StreamingContext` that can consume data from various sources, such as Kafka or HDFS."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the main purpose of `SparkSession` in PySpark?\n",
    "\n",
    "- A) To create a connection to a Spark cluster and coordinate the execution of Spark jobs.\n",
    "- B) To define a Spark application's configuration settings, such as the application name and the location of input data.\n",
    "- C) To provide a high-level API for working with distributed data in PySpark. ***\n",
    "- D) To manage the execution of tasks within a Spark job and handle failures and retries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Which of the following data structures are available in Apache Spark?\n",
    "\n",
    "- A) RDDs\n",
    "- B) DataFrames\n",
    "- C) Datasets\n",
    "- D) All of the above ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What is an RDD in Apache Spark?\n",
    "\n",
    "- A) A type of database that stores structured data in a distributed manner.\n",
    "- B) A distributed collection of objects that can be processed in parallel. ***\n",
    "- C) A type of machine learning algorithm used in Spark.\n",
    "- D) A tool used to manage Spark clusters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a DataFrame in Apache Spark?\n",
    "\n",
    "- A) A database management system used for storing and querying large-scale data.\n",
    "- B) A distributed collection of objects that can be processed in parallel.\n",
    "- C) A tabular view of data with named columns, similar to a relational database table. ***\n",
    "- D) A machine learning algorithm used for clustering data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a DataSet in Apache Spark?\n",
    "\n",
    "- A) A distributed collection of objects that can be processed in parallel.\n",
    "- B) A collection of data organized into named columns, similar to a relational database table.\n",
    "- C) A distributed collection of objects that are strongly typed, providing a more efficient and convenient API for working with structured data. ***\n",
    "- D) A type of machine learning algorithm used in Spark.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following code snippets creates an RDD in PySpark?\n",
    "\n",
    "- A) `rdd = sc.parallelize([\"apple\", \"banana\", \"orange\"])` ***\n",
    "- B) `rdd = spark.read.text(\"data.txt\")`\n",
    "- C) \n",
    "`rdd = Seq((\"apple\", 2), (\"banana\", 4), (\"orange\", 1))`\n",
    "\n",
    "`spark.createDataFrame(rdd)`\n",
    "- D) \n",
    "`rdd = [1, 2, 3, 4, 5]`\n",
    "\n",
    "`sc.parallelize(rdd)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is lazy evaluation in PySpark?\n",
    "\n",
    "- A) Lazy evaluation is a feature in PySpark that allows us to chain transformations on an RDD or DataFrame without actually executing them until an action is called. ***\n",
    "\n",
    "- B) Lazy evaluation is a feature in PySpark that allows us to execute transformations on an RDD or DataFrame without chaining them.\n",
    "\n",
    "- C) Lazy evaluation is a feature in PySpark that allows us to execute actions on an RDD or DataFrame without transforming them.\n",
    "\n",
    "- D) Lazy evaluation is a feature in PySpark that automatically optimizes transformations and actions for faster execution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the `persist()` method in PySpark do?\n",
    "\n",
    "- A) The `persist()` method allows you to store a PySpark RDD or DataFrame in memory or on disk for faster access. ***\n",
    "\n",
    "- B) The `persist()` method allows you to delete a PySpark RDD or DataFrame from memory or disk.\n",
    "\n",
    "- C) The `persist()` method allows you to rename a PySpark RDD or DataFrame.\n",
    "\n",
    "- D) The `persist()` method allows you to filter a PySpark RDD or DataFrame."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following code snippets can be used to create a DataFrame using Spark SQL in PySpark?\n",
    "\n",
    "- A)\n",
    "\n",
    "`from pyspark.sql import SparkSession`\n",
    "\n",
    "`spark = SparkSession.builder.appName(\"example\").getOrCreate()`\n",
    "`data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]`\n",
    "`df = spark.createDataFrame(data, [\"Name\", \"Age\"])`\n",
    "`df.show()`***\n",
    "\n",
    "- B)\n",
    "\n",
    "`from pyspark.sql import SparkSession`\n",
    "\n",
    "`spark = SparkSession.builder.appName(\"example\").getOrCreate()`\n",
    "`data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]`\n",
    "`rdd = spark.sparkContext.parallelize(data)`\n",
    "`df = spark.createDataFrame(rdd, [\"Name\", \"Age\"])`\n",
    "`df.show()`\n",
    "\n",
    "- C)\n",
    "\n",
    "`from pyspark.sql import SparkSession`\n",
    "\n",
    "`spark = SparkSession.builder.appName(\"example\").getOrCreate()`\n",
    "`data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]`\n",
    "`df = spark.sql(\"SELECT _1 as Name, _2 as Age FROM VALUES %s\" % str(data))`\n",
    "`df.show()`\n",
    "\n",
    "- D)\n",
    "\n",
    "`from pyspark.sql import SparkSession`\n",
    "\n",
    "`spark = SparkSession.builder.appName(\"example\").getOrCreate()`\n",
    "`data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]`\n",
    "`rdd = spark.sparkContext.parallelize(data)`\n",
    "`df = rdd.toDF([\"Name\", \"Age\"])`\n",
    "`df.show()`\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements is true about Spark SQL?\n",
    "\n",
    "- A) Spark SQL allows you to execute SQL queries and perform analysis on structured data within Spark. ***\n",
    "\n",
    "- B) Spark SQL is only compatible with relational databases like MySQL and PostgreSQL.\n",
    "\n",
    "- C) Spark SQL is a separate engine from Spark and requires separate installation.\n",
    "\n",
    "- D) Spark SQL is used only for processing unstructured data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the `map` function in PySpark do?\n",
    "\n",
    "- A) It applies a function to each element of an RDD and returns a new RDD. ***\n",
    "\n",
    "- B) It returns the first element of an RDD.\n",
    "\n",
    "- C) It removes all the duplicates from an RDD.\n",
    "\n",
    "- D) It sorts the elements of an RDD in descending order."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the `filter` function in PySpark do?\n",
    "\n",
    "- A) It removes all the duplicates from an RDD.\n",
    "\n",
    "- B) It returns the first element of an RDD.\n",
    "\n",
    "- C) It applies a function to each element of an RDD and returns a new RDD.\n",
    "\n",
    "- D) It selects the elements of an RDD that satisfy a given condition and returns a new RDD. ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the `sortBy` function in PySpark do?\n",
    "\n",
    "- A) It sorts an RDD in ascending order based on a key function.***\n",
    "\n",
    "- B) It returns the first element of an RDD.\n",
    "\n",
    "- C) It applies a function to each element of an RDD and returns a new RDD.\n",
    "\n",
    "- D) It selects the elements of an RDD that satisfy a given condition and returns a new RDD."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
