{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Essentials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PySpark in Apache Spark and how is it useful?\n",
    "\n",
    "- PySpark is a Python library for parallel processing of data, and it is useful because it provides an easy-to-use programming interface for data processing and analysis\n",
    "- PySpark is a Python package for distributed computing, and it is useful because it allows Python developers to write Spark jobs in Python instead of Java or Scala ***\n",
    "- PySpark is a Python-based cluster manager for Apache Spark, and it is useful because it simplifies the deployment and management of Spark clusters\n",
    "- PySpark is a Python-based machine learning library, and it is useful because it provides a powerful set of tools for machine learning and data analysis in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is `findspark` in PySpark and how is it useful?\n",
    "\n",
    "- `findspark` is a package that allows users to locate and download Spark libraries, and it is useful because it simplifies the setup process for Spark on local machines\n",
    "- `findspark` is a module that automatically finds the location of Spark and sets the necessary environment variables, and it is useful because it makes it easier to integrate PySpark with existing Python environments ***\n",
    "- `findspark` is a Python-based cluster manager for Apache Spark, and it is useful because it simplifies the deployment and management of Spark clusters\n",
    "- `findspark` is a machine learning library for PySpark, and it is useful because it provides a powerful set of tools for machine learning and data analysis in PySpark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following code snippets correctly shows how to create and configure a SparkConf object in PySpark?\n",
    "\n",
    "- \n",
    "\n",
    "``` python\n",
    "from pyspark import SparkContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"MyApp\")\n",
    "sc = SparkContext(conf=conf)` ***\n",
    "```\n",
    "\n",
    "- \n",
    "``` python\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"MyApp\")\n",
    "```\n",
    "\n",
    "- \n",
    "``` python\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setAppName(\"MyApp\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "```\n",
    "\n",
    "- \n",
    "``` python\n",
    "\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"MyApp\").setMaster(\"local\") \n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the purpose of `SparkContext` in PySpark?\n",
    "\n",
    "- To provide a connection to a Spark cluster and coordinate the execution of Spark jobs ***\n",
    "- To define a Spark application's configuration settings, such as the application name and the location of input data\n",
    "- To provide a high-level API for working with distributed data in PySpark\n",
    "- To manage the execution of tasks within a Spark job and handle failures and retries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In PySpark, what is the purpose of the `getOrCreate()` method of `SparkSession`?\n",
    "\n",
    "- A) To create a new `SparkSession `instance if one does not already exist, or to return an existing `SparkSession` instance if one has already been created. ***\n",
    "- B) To create a new RDD from a given data source, such as a file or database table.\n",
    "- C) To create a new DataFrame by applying a schema to a given data source, such as a file or database table.\n",
    "- D) To create a new `StreamingContext` that can consume data from various sources, such as Kafka or HDFS."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the main purpose of `SparkSession` in PySpark?\n",
    "\n",
    "- To create a connection to a Spark cluster and coordinate the execution of Spark jobs\n",
    "- To define a Spark application's configuration settings, such as the application name and the location of input data\n",
    "- To provide a high-level API for working with distributed data in PySpark ***\n",
    "- To manage the execution of tasks within a Spark job and handle failures and retries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Which of the following data structures are available in Apache Spark?\n",
    "\n",
    "- RDDs\n",
    "- DataFrames\n",
    "- Datasets\n",
    "- All answers are correct ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What is an RDD in Apache Spark?\n",
    "\n",
    "- A type of database that stores structured data in a distributed manner\n",
    "- A distributed collection of objects that can be processed in parallel ***\n",
    "- A type of machine learning algorithm used in Spark\n",
    "- A tool used to manage Spark clusters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a DataFrame in Apache Spark?\n",
    "\n",
    "- A database management system used for storing and querying large-scale data\n",
    "- A distributed collection of objects that can be processed in parallel\n",
    "- A tabular view of data with named columns, similar to a relational database table ***\n",
    "- A machine learning algorithm used for clustering data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a DataSet in Apache Spark?\n",
    "\n",
    "- A distributed collection of objects that can be processed in parallel\n",
    "- A collection of data organized into named columns, similar to a relational database table\n",
    "- A distributed collection of objects that are strongly typed, providing a more efficient and convenient API for working with structured data ***\n",
    "- A type of machine learning algorithm used in Spark\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following code snippets creates an RDD in PySpark?\n",
    "\n",
    "- A) `rdd = sc.parallelize([\"apple\", \"banana\", \"orange\"])` ***\n",
    "- B) `rdd = spark.read.text(\"data.txt\")`\n",
    "- C) \n",
    "``` python\n",
    "rdd = Seq((\"apple\", 2), (\"banana\", 4), (\"orange\", 1))\n",
    "spark.createDataFrame(rdd)\n",
    "```\n",
    "- D) \n",
    "``` python\n",
    "rdd = [1, 2, 3, 4, 5]\n",
    "sc.parallelize(rdd)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is lazy evaluation in PySpark?\n",
    "\n",
    "- Lazy evaluation is a feature in PySpark that allows us to chain transformations on an RDD or DataFrame without actually executing them until an action is called ***\n",
    "\n",
    "- Lazy evaluation is a feature in PySpark that allows us to execute transformations on an RDD or DataFrame without chaining them\n",
    "\n",
    "- Lazy evaluation is a feature in PySpark that allows us to execute actions on an RDD or DataFrame without transforming them\n",
    "\n",
    "- Lazy evaluation is a feature in PySpark that automatically optimizes transformations and actions for faster execution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the `persist()` method in PySpark do?\n",
    "\n",
    "- The `persist()` method allows you to store a PySpark RDD or DataFrame in memory or on disk for faster access ***\n",
    "\n",
    "- The `persist()` method allows you to delete a PySpark RDD or DataFrame from memory or disk\n",
    "\n",
    "- The `persist()` method allows you to rename a PySpark RDD or DataFrame\n",
    "\n",
    "- The `persist()` method allows you to filter a PySpark RDD or DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following code snippets can be used to create a DataFrame using Spark SQL in PySpark?\n",
    "\n",
    "- A)\n",
    "``` python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show() ***\n",
    "```\n",
    "\n",
    "- B)\n",
    "``` python \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "df = spark.createDataFrame(rdd, [\"Name\", \"Age\"])\n",
    "df.show()\n",
    "```\n",
    "\n",
    "- C)\n",
    "``` python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.sql(\"SELECT _1 as Name, _2 as Age FROM VALUES %s\" % str(data))\n",
    "df.show()\n",
    "```\n",
    "\n",
    "- D)\n",
    "``` python\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "df = rdd.toDF([\"Name\", \"Age\"])\n",
    "df.show()\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements is true about Spark SQL?\n",
    "\n",
    "- Spark SQL allows you to execute SQL queries and perform analysis on structured data within Spark ***\n",
    "\n",
    "- Spark SQL is only compatible with relational databases like MySQL and PostgreSQL\n",
    "\n",
    "- Spark SQL is a separate engine from Spark and requires separate installation\n",
    "\n",
    "- Spark SQL is used only for processing unstructured data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the `map` function in PySpark do?\n",
    "\n",
    "- It applies a function to each element of an RDD and returns a new RDD ***\n",
    "\n",
    "- It returns the first element of an RDD\n",
    "\n",
    "- It removes all the duplicates from an RDD\n",
    "\n",
    "- It sorts the elements of an RDD in descending order"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the `filter` function in PySpark do?\n",
    "\n",
    "- It removes all the duplicates from an RDD\n",
    "\n",
    "- It returns the first element of an RDD\n",
    "\n",
    "- It applies a function to each element of an RDD and returns a new RDD\n",
    "\n",
    "- It selects the elements of an RDD that satisfy a given condition and returns a new RDD ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the `sortBy` function in PySpark do?\n",
    "\n",
    "- It sorts an RDD in ascending order based on a key function ***\n",
    "\n",
    "- It returns the first element of an RDD\n",
    "\n",
    "- It applies a function to each element of an RDD and returns a new RDD\n",
    "\n",
    "- It selects the elements of an RDD that satisfy a given condition and returns a new RDD"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
