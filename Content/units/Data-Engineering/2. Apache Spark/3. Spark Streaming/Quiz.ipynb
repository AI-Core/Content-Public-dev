{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements accurately describes Spark Streaming?\n",
    "\n",
    "- A) Spark Streaming is a batch processing framework for processing large volumes of data.\n",
    "- B) Spark Streaming is a real-time data processing framework for processing streaming data. ***\n",
    "- C) Spark Streaming is a relational database management system.\n",
    "- D) Spark Streaming is a machine learning framework."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a DStream in Spark Streaming and how does it differ from an RDD in Apache Spark?\n",
    "\n",
    "- A DStream is a sequence of static datasets, while an RDD is a continuous stream of data\n",
    "- A DStream is a continuous stream of data, while an RDD represents static datasets ***\n",
    "- A DStream and an RDD are both continuous streams of data, but a DStream is optimized for real-time processing\n",
    "- A DStream and an RDD are both static datasets, but a DStream is optimized for parallel processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the typical process of Spark Streaming?\n",
    "\n",
    "- Spark Streaming reads data from a file, processes it in real-time, and writes the output to a file\n",
    "- Spark Streaming reads data from a database, processes it in real-time, and writes the output to a database\n",
    "- Spark Streaming reads data from a streaming source, processes it in real-time, and writes the output to a streaming sink ***\n",
    "- Spark Streaming reads data from a batch source, processes it in near-real-time, and writes the output to a batch sink"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a receiver in Spark Streaming and how does it work?\n",
    "\n",
    "- A receiver is a component that is responsible for processing data in real-time, and it works by pulling data from various sources into Spark for processing\n",
    "- A receiver is a component that is responsible for receiving data from various sources, such as Kafka or Flume, and pushing it into Spark for processing ***\n",
    "-  A receiver is a component that is responsible for writing data to external storage systems, such as HDFS or S3, and it works by pushing the data out of Spark\n",
    "- A receiver is a component that is responsible for scheduling Spark jobs and running them in a distributed manner, and it works by monitoring the available computing resources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are some features of receivers in Spark Streaming?\n",
    "\n",
    "- Receivers can be set up to pull data from external data sources\n",
    "- Receivers can automatically recover from failures and continue processing data\n",
    "- Receivers can be dynamically scaled up or down based on the incoming data rate\n",
    "- All answers are correct ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is `StreamingContext` in Spark Streaming and what is its role in the streaming process?\n",
    "\n",
    "- `StreamingContext` is a data structure that represents the input data in Spark Streaming, and it is responsible for processing the data streams in real-time\n",
    "- `StreamingContext` is the entry point for Spark Streaming applications, and it represents the main interface for creating DStreams and configuring the streaming process ***\n",
    "- `StreamingContext` is a component that is responsible for writing the output data to external storage systems, such as HDFS or S3\n",
    "- `StreamingContext` is a machine learning library in Spark that is used for training models on real-time data streams\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following code snippets correctly creates a StreamingContext in Spark Streaming?\n",
    "\n",
    "- \n",
    "``` python\n",
    "from pyspark.streaming import StreamingContext\n",
    "ssc = StreamingContext(sparkContext, 10)\n",
    "```\n",
    "\n",
    "- \n",
    "``` python\n",
    "from pyspark import StreamingContext\n",
    "ssc = StreamingContext(\"local[2]\", \"MyStreamingApp\", 10)\n",
    "```\n",
    "\n",
    "- \n",
    "``` python\n",
    "from pyspark.streaming import StreamingContext\n",
    "ssc = StreamingContext(sparkConf, 10)\n",
    "```\n",
    "\n",
    "- \n",
    "``` python\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "sc = SparkContext(\"local[2]\", \"MyStreamingApp\")\n",
    "ssc = StreamingContext(sc, 10) ***\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do `ssc.start()` and `ssc.stop()` methods do in Spark Streaming?\n",
    "\n",
    "- `ssc.start()` method starts the SparkContext and loads the configuration settings, while `ssc.stop()` method shuts down the SparkContext and releases the resources\n",
    "- `ssc.start()` method starts the streaming context and begins processing data streams in real-time, while `ssc.stop()` method stops the streaming context and releases the resources ***\n",
    "- `ssc.start()` method starts the PySpark shell and creates a new SparkSession object, while `ssc.stop()` method stops the shell and closes the session\n",
    "- `ssc.start()` method starts the Spark UI and displays the real-time streaming statistics, while `ssc.stop()` method stops the UI and closes the browser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does `ssc.awaitTermination()` method do in Spark Streaming?\n",
    "\n",
    "- `ssc.awaitTermination()` method waits for the completion of all batch processing tasks before stopping the streaming context\n",
    "- `ssc.awaitTermination()` method waits indefinitely until the streaming context is stopped or terminated by a user interrupt signal ***\n",
    "- `ssc.awaitTermination()` method waits for a specified amount of time for the streaming context to process the incoming data streams, and then stops the context\n",
    "- `ssc.awaitTermination()` method waits for the availability of new data streams and continuously processes the data until the streaming context is explicitly stopped\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are window operations in Spark Streaming?\n",
    "\n",
    "- A way to define the time period over which the incoming data is processed\n",
    "- A way to perform aggregation on a sliding window of data over a specific period of time ***\n",
    "- A way to filter out irrelevant data points from the incoming data stream\n",
    "- A way to perform machine learning algorithms on the incoming data streams\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the output of the following code that performs a window operation on a DStream?\n",
    "\n",
    "``` python\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sparkContext, 1)\n",
    "ssc.checkpoint(\"checkpoint_dir\")\n",
    "\n",
    "input_stream = ssc.socketTextStream(\"localhost\", 9999)\n",
    "numbers_stream = input_stream.map(lambda x: int(x))\n",
    "windowed_stream = numbers_stream.window(10, 5)\n",
    "count_stream = windowed_stream.count()\n",
    "\n",
    "count_stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "```\n",
    "\n",
    "- The code will not execute as there is no data source defined\n",
    "- The code will calculate the count of numbers over a sliding window of 10 seconds and print the results every 5 seconds ***\n",
    "- The code will calculate the count of numbers over a sliding window of 5 seconds and print the results every 10 seconds\n",
    "- The code will calculate the sum of numbers over a sliding window of 10 seconds and print the results every 5 seconds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following methods can be used to save a DStream in Spark Streaming?\n",
    "\n",
    "- `dstream.saveAsTextFiles(\"output_dir\")`\n",
    "- `dstream.saveAsParquetFile(\"output_dir\")`\n",
    "- `dstream.saveAsSequenceFile(\"output_dir\")`\n",
    "- All answers are correct ***\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements is true about `foreachRDD` in Spark Streaming?\n",
    "\n",
    "- `foreachRDD` is a transformation that applies a function to each RDD in a DStream ***\n",
    "- `foreachRDD` is an action that applies a function to each element in a DStream\n",
    "- `foreachRDD` is a method that writes each RDD in a DStream to a file system or external storage\n",
    "- `foreachRDD` is a method that allows custom processing of some RDDs in a DStream"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements is true about `persist()` in Spark Streaming?\n",
    "\n",
    "- `persist()` is a method that saves a DStream to a file system or external storage\n",
    "- `persist()` is a transformation that applies a function to each RDD in a DStream\n",
    "- `persist()` is a method that caches the RDDs of a DStream in memory or disk for faster processing ***\n",
    "- `persist()` is an action that returns the first element of a DStream"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which of the following statements is true about checkpointing in Spark Streaming?\n",
    "\n",
    "- Checkpointing is a method that saves a DStream to a file system or external storage\n",
    "- Checkpointing is a transformation that applies a function to each RDD in a DStream\n",
    "- Checkpointing is a mechanism that stores metadata about the state of a Spark Streaming application for fault tolerance ***\n",
    "- Checkpointing is an action that returns the first element of a DStream"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
