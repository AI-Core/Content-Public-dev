- when you start processing truly huge datasets, with billions of records, you'll start to realise that libraries like pandas simply won't cut it
- limitations, like the fact that you can't even fit your whole dataset on your machine, become a real problem
- and the inefficiencies of those libraries really start to add up

- Apache Spark is becoming the industry standard tool for big data processing, and it's not rare to see it bring about 100x increases in efficiency
- it can perform processesing distributed across many different machines
- meaning that the processing happens in parallel
- and in each of these compute nodes, the data is processed in RAM, rather than moving it to and from a hard disk, which usually takes a while
- spark has bindings for several popular programming languages, including python
- and it can be used to process data from (or send data to) a wide range of sources including S3, postgres, kafka and more
- because of it's in-memory processing, and compatibility with other tools, it's rapidly replacing Hadoop mapreduce - the legacy standard for big data processing
- it was originally part of the hadoop ecosystem, but has since evolved to be able to standalone - so you don't have to use it with hadoop

- spark currently has 4 main libraries:
- spark SQL, for running SQL-like queries
- Spark MLLin - for doing machine learning on Spark
- Spark GraphX - for graph data processing
- and Spark Streaming - for processing data as soon as it streams in

- so how does Spark run things under the hood?

- to run spark, you need to create what is called a `spark session`
- it communicates with a software called the cluster manager to send the code to run and data to process to different spark executors
- this happens using what are called `spark submit` scripts
- these spark executors run the spark job, and then return the result

- because spark runs on many machines at once, we need to manage this cluster of computers
- there are a bunch of ways to do this
- we can be lazy and just run it on our own machine
<!-- - we can run it in standalone mode, which  -->
- we can run it on a kubernetes cluster
- we can run it on Hadoop
- and there are other ways to run it too

- you can install spark , where it can be accessed as the library _pyspark_
