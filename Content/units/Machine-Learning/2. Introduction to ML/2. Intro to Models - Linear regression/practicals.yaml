- description: |+
    1. Create a conda environment called “sk_learn_tutorial”, with `pip` , `numpy` and  `sk-learn` installed. You can specify all of this in the `conda create` statement, see the [docs](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)
    2. Activate the environment and install the `sklearn` package using `pip`.
    3. Create a file called `linear_regression.py`. Inside the file, import the  `datasets`and `model selection` modules from sci-kit learn.
    4. Fetch the `California` dataset of California real estate prices, and assign the labels and features to variables called `X` and `y`. Print the shape of the features and labels.
    5. Divide the dataset into `train`, `validation` and `test` subsets, using the `model_selection.train_test_split` method. Hint - you will need to apply the method twice to generate the three subsets.
    6. Create a class called `LinearRegression`. The class should have two methods - the class constructor, which needs to randomly assign initial weights for each feature, and set a random seed for reproducibility. A method called `__call__` that runs when we call an instance of the class on some data, and returns a prediction based on the features in `X`.
    7. Create an instance of `LinearRegression` and use it to get the predictions based on the initial weights. Print the first 10 examples. Now print the first 10 actual values of y. What do you notice?
    8. We now need to tell the model how to improve. Add a new method to the `LinearRegression` class, called `update_parameters`. This method should update the model's weight and bias attributes to new values which are passed to the method as parameters.
    9. Define a function inside `linear_regression.py` that takes in the predictions and the labels, and calculates the mean squared error (MSE) of the difference between them.
    10. Call this function on the initial predictions and the labels, and assign the output to a variable called `cost`.
    11. Define a function inside `linear_regression.py` that takes in the features and labels of the training set and calculates the optimum values of the weights and biases for each feature. You can refer to the `Notebook` for help here!
    12. Call this function on `X_train` and `y_train` and assign the outputs to variables called `weights` and `biases`.
    13. Update the model with these optimised weights and biases using the `update_parameters` method.
    14. Call the model on the `X_train` data again, and use the `mean_squared_error` method to calculate the loss with the updated parameters.
    15. Finally, print out the first few values of `y_train` and `y_pred` again. Do they look any better than they did when you printed them in step 7?
    16. Congratulate yourself, you have built a script to solve a linear regression analytically!

  id: b62a69cb-d402-4b3d-bf74-c0c2e335e323
  name: Solve a Linear Regression Problem Analytically


- description: |+
    1. Use `scikit-learn`` to create a linear regression model for the house prices in the `California` dataset, this time using `scikit-learn`'s linear model.
    2. Evaluate the model on the test data using `scikit-learn`'s  `MSE` metric.
    3. Save the finished model to disk.

  id: c938c9de-5fb4-4dc2-a35b-8d07304f44af
  name: Evaluate the Performance of a Linear Regression Model

- description: |+
    1. Use the `matplotlib` library to plot the residuals of a linear regression model and interpret the results.

  id: 46036d2c-eac5-4dc6-888b-3dc44f94e07d
  name: Plot the Residuals of a Linear Regression Model


