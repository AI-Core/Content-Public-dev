# Linear Regression

- There are different ways we have to divide the algorithm of machine learning.
    - For example, if the data is labeled, we have a supervised learning algorithm
    - On the other hand, if the data is unlabeled, we have an unsupervised learning algorithm
    - One way to divide ML models is into supervised and unsupervised learning
    - In supervised learning, the label can be continuous values or discrete values
    - If the values are discrete, then the model is called a classification model
    - On the other hand, if the values are continuous, then the model is called a regression model
    - For example, the price of a house is a continuous value, so we can use a regression model to predict the price of a house
    - Don't confuse the label with the features to define a regression model
    - As mentioned, the price of a house is a continuous value, but the number of rooms is a discrete value.
    - This doesn't mean that we can't use a regression model using the number of rooms as a feature
    - One of the most common regression models is the linear regression

- Linear regression makes predictions using a linear model __`[SHOW THIS EQUATION y = Xw + b]`__
    - It makes a prediction `y` 
    - By computing a weighted `w` sum of the input features `X`
    - Plus a constant called the bias term `b`

- When training a model, it is important the number of features in the training set and the testing set is the same
    - Don't confuse the number of features with the number of samples
    - The number of features is the number of different variables in the data
        - For example, in the California housing dataset, there are 8 features, or 8 different variables that tell the characteristics of a house
        - So the dimension of the input features is the number of features times the number of samples
    - Similarly, if we have one label, the dimension of the label is the number of samples
    - That means that the dimension of the weights is the number of features
    - And the dimension of the bias term is 1
    - In other words, we will have a weight per feature
    - But, how do we know the value of these weights? 

- To explain how to compute the value of the weights and the bias term, we need to explain how to measure the performance of the model
    - Ideally, we would like to have weights and a bias term that make the model perform as well as possible
    - Let's see an example in a notebook to understand how the weights and the bias can impact the loss function
    - __`[OPEN THE NOTEBOOK]`__
    - In this notebook, we are going to use the California housing dataset
    - We start by loading the dataset and splitting it into training and testing sets
    - __`[RUN THE CELL]`__
    - Now, we are going to define a class called SimpleLinearRegression
        - This class is initialized with some random weights and a bias term
        - And when we make a prediction, we basically compute the weighted sum of the input features and add the bias term, which is included in the call magic method
        - Finally, it has a method called `update_params` that esentially replaces the weights and the bias term with the new weights and the new bias term
    - __`[RUN THE CELL]`__
    - Now, let's instantiate a SimpleLinearRegression object
    - We will use this to make a simple prediction in the test set, and assign the prediction to a variable called `y_pred`
    - __`[RUN THE CELL]`__
    - Let's take a look at what happened behind the scenes
        - The weights and the bias term are initialized with random values
        - We can check them by printing the corresponding attributes
        - __`[RUN THE CELL]`__
        - We can see that there are 8 weights and 1 bias term
        - These weights are multiplied by the input features and the bias term is added to the weighted sum
        - For example, let's see the output of the first sample in the test set
        - __`[RUN THE CELL]`__
        - Notice that we are using the "@" symbol to represent the matrix multiplication
        - And the output is 253.65, which is the same as the output we get in the predictions we made earlier
    - So, let's see how well the model performed
    - This cell plots the predictions and the actual values of the ten first samples in the test set
    - __`[RUN THE CELL]`__
    - The model performed very badly!
    - And this is normal, we haven't trained the model yet, and we are using random parameters
    - Ok, so we need to change the parameters so that the model performs better
    - But how do we measure the performance of the model?
    - There are different metrics that help us measure the performance of the model
    - In this video, we will look at the Mean Squared Error (MSE)
    - The MSE is a measure of how far the predictions are from the actual values
    - The MSE is the average of the squared differences between the predictions and the actual values
    - So, let's compute it using the values we predicted and the actual values
    - __`[RUN THE CELL]`__
    - The MSE is above 1 million, which doesn't sound good
    - So, if we start with random values for the weights and the bias, the predictions that you make are likely to return a really high loss
    - We can update the parameters to minimize the MSE
    - However, what would be the new weights and the new bias term?
    - Well, remember that what we wanted was to obtain the weights and the bias term that make the model to perform as well as possible, 
    - On a similar note, the greater the loss function, the worse the model performs
    - So, we need to find the weights and the bias term that minimize the loss function
    - We can use the derivative of the loss function and see when it becomes zero, 
    - __`[SHOW THE IMAGE IN THE MARKDOWN CELL] We might want to improve the quality of the image`__
    - So, we want to calculate the gradient of the MSE with respect to the weights
    - Since we are dealing with linear regression, the predictions can be written as `w*x`
    - Notice that this `w` includes both the weights and the bias term, and the input features `x` has an additional column of ones. 
    - That way, the derivative calculates the minimum of the loss function with respect to both the weights and the bias term
    - After some math, we can see that the gradient of the MSE with respect to the weights is
    $$
    2*X^TXw^* - 2*X^Ty
    $$
    - So, if we equal it to zero, we can find the weights and the bias term that minimize the loss function
    - which gives us 
    $$
    w = (X^TX)^{-1} X^Ty
    $$
    - Great, let's implement it in the code
    - __`[RUN THE CELL]`__
    - The code returned the optimal weights and the optimal bias term
    - We can update the weights and the bias term in the SimpleLinearRegression object
    - And make a new prediction to see how well the model performs
    - __`[RUN THE CELL]`__
    - After plotting the predictions and the actual values of the ten first samples in the test set, we can see that the model performs much better
    - Let's calculate the MSE of the model
    - __`[RUN THE CELL]`__
    - The MSE is now much lower, reaching 0.5, which is much much better!

- Ok, so we know that essentially, linear regression calculates the weights and the bias term that minimize the loss function
    - Other machine learning models, such as random forest, might have different loss functions
    - But basically, the idea behind many machine learning models is to minimize a loss function
    - Coding the algorithm and the loss functions from scratch can be a bit tedious

- Luckily, s klearn has also a class that implements linear regression
    - We can initialize an instance of this class using `sklearn.linear_model.LinearRegression`
    - When we train this model with our data, what it does behind the scenes is that it computes the parameters that minimize the loss

- Let's see a brief example on this
- __`[RUN THE DEMO IN THE NOTEBOOK]`__
    - We load the california housing dataset, and we split it into training and test sets
    - __`[RUN THE CELL]`__
    - We can train the model using the fit method
    - The fit method takes as input the training data
    - And it returns the instance of the model with the parameters that minimize the loss
    - __`[RUN THE CELL]`__
    - This instance stores the parameters that minimize the loss
    - We can take a look at the parameters using the coef_ attribute
    - __`[RUN THE CELL]`__
    - We obtain a list of numbers that represent the weights
    - But we can see what weight corresponds to which feature
    - __`[RUN THE CELL]`__
    - Notice how some weights are greater than others
    - It looks like population is not very important to predict the price of a house
    - However, this is not necessarily the case
    - Population has a larger scale than other features, so its weight will have a lower magnitude to have the same impact on the price
    - We will see more on how to deal with this in next videos
    - You can also see the bias term using the intercept_ attribute
    - __`[RUN THE CELL]`__
    - And we get the value of the bias term

- So now you know how to perform linear regression using a custom class or using the LinearRegression class from sklearn
