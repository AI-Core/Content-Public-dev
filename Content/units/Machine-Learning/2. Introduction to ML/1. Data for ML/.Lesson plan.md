- there is a classic definition of what machine learning is
	- "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."
	- basically, a computer gets better at a task, with respect to some metric, as it sees data
- [SHOW] diagram in overview nb of mapping between inputs and outputs
	- basically any task can be framed in this way
	- if we can define how to do a task this way, we can automate it
	- sometimes it's easy to define a function that performs this mapping
		- e.g. compute the BMI of a person given their height and weight
	- sometimes we dont know how to define the function which maps between input and output
		- this is a valid reason to apply machine learning
	- sometimes it may be faster  
	- machine learning is basically just having a computer find these functions instead of defining them ourselves
- [SHOW] whiteboard
	- there's a pretty standard recipe for building machine learning algorithms
		- it consists of four things
			- the data
				- supervised data consists of features and labels
				- draw x-y graph with ~6 data points that dont lie on a straight line
				- label x = age
				- label y = height
			- the model
				- draw a straight blue line that only roughly fits the data
			- the optimiser
				- draw a dashed blue line which slightly better fits the data
			- the criterion
				- draw vertical lines between the solid blue line and the datapoints
- the difference between optimisation and machine learning
	- in pure optimisation, we define an objective which we wish to minimise (or maximise)
		- it might be the revenue of our company 
		- and it might depend on how much of which products we produce
		- but we know how to compute it
	- in machine learning, we optimise the criterion
		- but the criterion may only be a surrogate for the thing which we actually want to optimise
		- in our graph, we dont actually want to minimise the length of the lines, we want to find a function which maps peoples' age to their height
		- we hope that the total line length is an indicator of how good our function is, but it doesnt measure it directly
		- the reason for this is that we don't have all of the data
		- in many real life scenarios, getting all the data is impossible
			- even if you collect a measure of the height-age for all people, you can never collect a measure of all possible people
			- how can you collect all possible images?
- what we just saw was an example where the input was 1-D and the output was 1-D
	- draw single datapoint
		- x^(i) = number
		- y^(i) = number
	- turn into dataset
		- X = [x^1, ..., x^n] (vector)
		- y (bold) = [y^1, ..., y^n] (vector)
	- turn into dataset with multiple features
		- x^(i) = [x^(i)_1, ...]
		- we could also have multiple labels too (e.g. object detection requires coordinates of box corners)
	- but now how do we visualise this on a graph?
		- we'll come back to this later

## Polychotomy of problems
- classification vs regression
	- how would the model be different?
	- how would the data be different?
- supervised vs unsupervised
	- how would the data be different?
	- how would the model be different?
		- perhaps cluster the data
		- perhaps map it to a new position in a different space

- scikit-learn (abbreviated sklearn) is a high-level machine learning library containing:
	- machine learning algorithms
	- example datasets
	- data pre-processing & pipelines
- sklearn is widely used in production for its simplicity and availability
- getting sklearn
	- pip install scikit-learn
	- import sklearn
	- notice that the import name and install name are different
- lets go back to our recipe
	- [ASK] around the class, what are the 4 ingredients?
- load in some data
	- lets look at the sklearn datasets documentation
	- sklearn.datasets.load_boston()
	- show the contents
	- get the data and targets as X and y
	- print their type
	- print their shape
	- so from X, we want to predict y
- now let's get a model
	- i know there is a model called linear regression
	- so let's look that up in the documentation
	- from sklearn.linear_model import LinearRegression
- now where are the criterion and the optimiser?
	- they are actually within the .fit() method of the model
	- we'll look at these two components much more later
- now we've fit the model to the data, we can make predictions using `.predict()`
- we can evaluate the performance of a model using the `.score()` method

## Saving a model
- once we've fit the model, we probably want to save it for later use
- we can save it using joblib, which comes with sklearn
	- joblib.dump(model, 'saved_model.joblib')
- then we can load it back in
	- loaded_model = joblib.load('saved_model.joblib')
	- loaded_model.predict(X)

## Bonus
- we cant visualise data with 3 features on a graph
- we should actually think of data in one space, and labels in another
- what we saw before is just a visualisation trick