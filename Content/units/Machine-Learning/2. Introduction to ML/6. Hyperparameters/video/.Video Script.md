# Hyperparameters

- We know that some parameters, like the weights and bias of a linear regression model, can be optimised directly by adjusting them using gradient descent
- There are some parameters which it doesn't make sense to optimise directly


_show X-Y graph of side-by-side polynomial models of orders up to p_

- For example, let's imagine I am training a polynomial model with p different powers of x. 
- This p is a parameter which controls the capacity of the model - the larger p is, the larger range of functions our model can represent
- We're trying to minimise the loss on the training set using gradient descent, and of course we know that increasing the capacity is going to make it easier to fit our data 
- So if we let it, the model will choose to maximise p
- But then of course the model will overfit
- And this is why we should not optimise the capacity parameter directly using gradient descent
- These kind of variables are set before training begins
- We call these kind of variables hyperparameters

_Show side by side some examples from each type_

- Other variables are hyperparameters because they actually can't be optimised directly during training
    - This is most commonly because they are required before training begins
    - Some examples might include the choice of optimisation algorithm, the batch size, the learning rate

- essentially, any variable required before training begins, or which should not be learned by the model is a hyperparameter
- we should set these before training

- Tuning hyperparameters is the main reason we need the validation set
    - evaluating them on the training set won't tell us how well the model generalises to unseen examples
    - and evaluating on the test set will likely lead us to overfit to perform well on those examples
- one common mistake is evaluating different hyperparameter combinations on different validation sets, by creating new splits every time you go to test a permutation
    - each permutation should be evaluated fairly, on the same validation set

- Reporting the score for a model is totally useless without hyperparameter tuning. You may be reporting a bad score simply because you have bad hyperparameters set.

## Finding the right hyperparameters

- So how do we find the right hyperparameters? 
- Well, we should try different hyperparameter permutations

- of course, we want to have some strategy to our search

_Show grid of random datapoints with label "Random search"_

- a basic strategy would be random search, where you sample random values within a range for each hyperparameter

_Show grid of random datapoints with label "Grid search"_

- a more exhaustive way to do this would be a grid search, where you try every possible combination of features within a range, using a grid search

- Grid search might be more comprehensive, but random search will explore a wider range of hyperparameters quickly

- of course, we can implement these strategies in code
- Let's look up how to do that with sklearn

_Search for sklearn random search_
_Search for sklearn grid search_

# Implementing grid search in code

_Switch to the example.ipynb to show the implementations for each_

- For example, let's use GridSearchCV to find the best hyperparameters for a random forest model
- This model has a bunch of hyperparameters that we want to tune
    - we will optimise two of them
        - `n_estimators`
        - `max_samples`
    - they are the hyperparameters that we want to tune
- In the GridSearchCV, we pass the model, and the hyperparameters we want to explore
- We can then fit that on the dataset
- and print the best score
- you can also see that the grid search took longer to run, because it's checking all permutations

- the n_estimators is a capacity controlling hyperparameter, but you can see that the model with the best performance doesn't use the largest value
- that's because under the hood, the search is measuring performance using cross validation on the validation set

- as the "CV", for cross validation, in the name suggests, the train-validation splitting is done inside these methods, so we should pass in the whole training set
- by default it uses k-fold cross validation with 5 folds
- so be careful applying this default keyword argument for larger models

_Switch to webcam view_

- doing k-fold cross validation on a single model is costly enough, let alone doing that for many different hyperparameter settings

- for such large models, the number of hyperparameter permutations you can evaluate will be bounded by the time you have available

- There are many more hyperparameter tuning strategies and also some more advanced work on automatic hyperparameter tuning, but I won't go into it here

- The key things that you need to remeber are that
    - Hyperparameters are model parameters that cannot or should not be learnt directly
    - They are different to parameters, which are learned directly
    - Reporting the score for a model is totally useless without hyperparameter tuning. You may be reporting a bad score simply because you have bad hyperparameters set.
    - Its pretty easy to implement some basic hyperparameter search algorithms using sklearn
    - But for large models, more hyperparameter tuning can be costly, because to evaluate each configuration, you need to train a new model