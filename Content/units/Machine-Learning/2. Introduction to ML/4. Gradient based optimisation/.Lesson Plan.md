## Gradient descent
- we've seen that the analytical solution is expensive to compute
- [ASK] how can we find the minimum in a different way?
- we move down the hill, bit by bit
- write equation for gradient
- explain diverging or moving too slow
	- learning rate
- this algorithm is at the heart of almost all state of the art machine learning algorithms
- we're going to look at more advanced variations of it later

## Ill conditioning & normalisation


## Comprehension Questions
- Explain to me, as clearly as you can, how the linear regression algorithm works
- What is normalisation and why do we need it?
- We should always normalise our data, but When will normalisation not change our result?
- What's the difference between standardisation and normalisation?
- How could I use linear regression to fit a cubic hypothesis for a problem given data with just a single feature?
- What is the "learning" in machine learning?

## Implementing gradient descent
- step by step walkthrough of linear_reg.py example
- first just outline
    - define the class
    - define the methods we're going to need
        - .predict
        - .fit
        - .__init__
        - ._get_mean_squared_error
    - dont fill any of the method body in
    - show how you intend it to be used
        - instantiate class
        - call .fit
        - call .predict
- then fill in the simple functions
- then fill in `compute_gradients` function
- plot graph of losses over each epoch

## Minibatch gradient descent
- update the example to perform mini-batch gradient descent