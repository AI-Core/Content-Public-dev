# Gradient Based Optimisation

- __`[Intro]`__
    - When training a model, we try to find the best parameters that make the model to be as similar as possible to the real data.
    - The difference between the prediction and the real data is called error, which is a measure of how far the model is from the real data.
    - From the error we can calculate differente metrics, such as the mean squared error, or the root mean squared error.
    - These metrics are used to measure the performance of the model, and they are the loss function of the model.
    - The goal of training a model is to find the best parameters that minimize the loss function.
    - If we think about it, the prediction depends on the parameters, and therefore, the error depends on the parameters.
    - So, the goal of training a model is to find the best parameters that minimize the error.
    - To do so, we use a technique called gradient descent.

- But first, what is a gradient?
    - A gradient is the derivative of a function with respect to a variable. 
    - As simple as that. 
    - Visually speaking, a gradient is the slope of the function at a point.
    - If the slope is positive, the function is increasing, and if the slope is negative, the function is decreasing.

- We can use this property to calculate the minimum of a function.
    - As mentioned, we want to find the best parameters, so we can calcualte the gradient of the loss function with respect to the parameters.
    - This is the principle of gradient descent.

- __`[Explain gradient descent using the classic example of walking through a mountain.]`__
    - To explain how gradient descent works, we will use the classic example of walking through a mountain.
    - Suppose you are lost in the mountains in a dense fog and you have to find the bottom of the valley.
    - You can only feel the slope of the ground below your feet. 
    - A good strategy is to go downhill in the direction of the steepest slope
    - Similarly, gradient descent measures the local gradient of the loss function with respect to a parameter
    - And it goes in the direction of descending gradient. 
    - Once the gradient is zero, you have reached a minimum!

- So, thanks to the gradient we know how to update the parameters to find the minimal loss function
    - The slope gives the direction of the descent.
    - But how do we set the step size?
    - We can set a parameter called learning rate, which determines how fast we update the parameters.
    - __`[Show an image like this:] When we are happy with the script I can work on better images`__
    - <p><img width=600 src=.images/Learning_rate_1.png></p>
    - The learning rate is a hyperparameter that we have to choose. 
    - It is important to choose a good learning rate, if it's too low, the algorithm will take too long to converge.
    - <p><img width=600 src=.images/Learning_rate_2.png></p>
    - On the other hand, if it is too high, the algorithm might not converge.
    - <p><img width=600 src=.images/Learning_rate_3.png></p>

- The graphs we showed earlier lookes like bowls with a single minimum point. 
    - However, not all functions look like this
    - They can also present irregularities, and other local minimums.
    - Fortunately, for Linear Regression, the loss function is a convex function, which means it has a single global minimum.

 - In practice this isn't such a big deal for a number of reasons:

    1. Firstly, the loss functions produced by combinations of certain loss functions with certain models, including MSE with linear regression as mentioned earlier, have only one minima. So the minima you find is always global
    2. Secondly, you can adapt the gradient descent algorithm in ways which mitigate getting trapped locally.
        - One way you can do that is to include momentum...
        - or add a learning rate scheduler which we will
        - We'll talk more about these later
    3. The final reason I'll mention for not worrying too much about local minima is that as you increase the number of parameters in a model, the chance of finding yourself in a local minima reduces exponentially

- So, how do we choose a good learning rate?
- __`[Run a demo of gradient descent.] In the notebook, you have an example in which you are going to change the value of the learning rate`__
    - Let's see an example of setting a good learning rate
    - First of all, this cell is training a model with the California housing dataset.
    - We have two custom classes, one that representes the linear regression model, and the other that represents the optimizer. 
    - In this case, the linear regression model contains the information about the parameters of the model
    - And the optimizer calculates the derivatives of the loss function with respect to the parameters and updates the parameters.
    - The learning rate is a parameter of this optimizer, and we can tweak it to see how it impacts the loss
    - Another parameter of the optimizer is the number of epochs, which is the number of iterations we do over the training data.
    - If we run this, it will show how the loss function changes throughout the epochs
    - As well as the evolution of the first parameter of the model, so we can see whether it converges or not
    - __`[Set the learning rate to 0.1 and run the cell.]`__
    - If we run the cell, we can see that the loss function converges to around `10.4` `[Change it with your value]` and the first parameter of the model is around `-1.38` `[Change it with your value]`.
    - Notice that the weights are updated at each epoch as the loss function decreases.
    - However, let's see what happens if we set a low learning rate 
    - __`[Set the learning rate to 0.001 and run the cell.]`__
    - We can see in this case that the loss function doesn't converge, and the first parameter is very slowly updating, but it doesn't converges.
    - Let's see what happens on the other end of the spectrum
    - __`[Set the learning rate to 0.15 and run the cell.]`__
    - In this case, the loss function overshoots the minimum, and the first parameter is fluctuating wildly `[Maybe you need to run it a few times until you get something more visually appealing]`.
    - <p><img width=600 src=.images/Learning_rate_4.png></p>
    - This is because the learning rate is too high, and each step is too big.

- __`[Differences between full batch, stochastic, and mini-batch gradient descent]`__
    - The algorithm we have seen so far uses the whole training set to update the parameters.
    - This is called full batch gradient descent.
    - The main problem with full batch gradient descent is the fact that it uses the whole training set to compute the gradients at every step
    - Which makes it very slow when the training set is large
    - We can go to the other end and use a single sample at a time to update the parameters.
    - This is called stochastic gradient descent.
    - Stochastic gradient descent randomly takes a single sample from the training set at each step to update the parameters.
    - Some advantages of stochastic gradient descent are:
        - It is much faster since it has very little data to manipulate at every iteration
        - It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration
        - Due to its random nature, it can help the algorithm jump out of local minima
    - However, due to the same randomness, stochastic gradient descent presents some issues:
        - It is much less regular than full batch gradient descent, so the loss function will bounce up and down, decreasing only on average. - It will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down
    - So, we can have something in between that doesn't use the whole dataset, but it doesn't use a single random sample at a time.
    - This is called mini-batch gradient descent.
    - Mini-batch gradient descent uses a subset of the training set to update the parameters.
    - It has the benefit of being faster and consuming less memory than full batch gradient descent.
    - And it is more regular than stochastic gradient descent, which will help the model to get closer to the minimum.
    - However, it doesn't escape from local minima as easy as stochastic gradient descent.
    - We also need to choose the batch size, which is the number of samples we use to run the algorithm.
    - The bigger the batch size, the more regular the algorithm will be, but the slower it will be.

- So, now that we know how gradient descent works, you might be wondering if all weights have the same impact on differentiating the loss function.
    - As mentioned earlier, the loss function doesn't always have the shape of a regular bowl
    - It can take the shape of an elongated bowl if the features have very different scales
    - Take a look at this picture of two different loss functions, one with features in a similar scale, and one with features in a different scale
    - <p><img width=600 src=.images/Normalization.png></p>
    - The images represent the loss function as a function of two parameters. 
    - The left image is the loss function with the features in a similar scale, and gradient descent goes straight toward the minimum, because that's where the slope is the steepest.
    - The right image is the loss function with the features in a different scale, and gradient descent goes in a direction almost orthogonal to the direction of the global minimum.
    - It will eventually reach the minimum, but it will take a long time.

- What we can do to solve it is to normalize or standardize the features.
    - These techniques bring features to the same value range
    - Normalization puts all values between 0 and 1
    - Whereas standardization puts all values centered around 0 with a standard deviation of 1
    - You should consider always normalizing the features before training a model
        - If you know that your loss function depends on the distance between features, you should normalize the features
        - But it is not always necessary, if for example, values are not continuous
    - Be careful when normalizing the features. 
        - Normalization and standardization get the mean value of all the dataset
        - So if you normalize the features and then you split the dataset into training and testing set, the testing set will have information about the training set
        - So we need to split the data before normalizing it
        - Another problem that you can come across is that the testing set will have a different mean value than the training set
        - So if you normalize the testing set with its mean value, the model will receive inputs that look different to what it saw in the training set
        - So you should normalise the testing set with the training set mean value
