- id: 782f956c-3546-4eed-98e0-c8c736a88c18
  name: Implement linear regression with gradient descent from scratch
  description: |+
    1. Work in groups, but repeat the exerecise individually later
    2. Make a new github repo called "Linear-Regression-From-Scratch"
    3. Don't add any fancy normalisation, mini-batching 
    4. Start by creating a skeleton of the code  
      - We want to create an sklearn-like class for a linear regression model
      - Define the class
      - Define the methods you expect it to need, but dont fill them in yet, just put `pass` inside
        - Which magic method will you need to define to start with?
        - Which methods do our sklearn models usually have?
        - What other pieces of the code might you be able to group together inside a method?
        - Which of these will be "private" methods?
          - How does that affect their names?
    4. Now below that class, code up how you expect the class to be used
      - How will an instance be created?
      - How will it be fit to the data?
      - How will predictions be made?
    5. Run your code. Yes, before you've defined the method bodies! Does it work? 
      - It's easier to debug as you go as your code is simpler
      - Lucky you checked now right!
      - Make a git commit
    6. Now comment the inside of the functions roughly what will happen in each line
    7. Now implement the methods, starting with the easiest methods first
      - Continue to run your code to test it works as intended
        - Don't be scared to put all kind of print statements in to debug
        - Remember to keep git committing
      - Anything strange happening? message me. It might make for a good demo.
    8. Save the gradient computation until last, and take special care here
    9. Refer to the pdf notes to find the gradients
    10. Graph the loss against each iteration
    11. Discuss: does it look good? 
      - What else could it have looked like?
      - How would you dealt with different graph appearances?
    12. Now you've done the simple implementation, add normalisation and compare your results
    13. Now you've done that, add mini-batching and compare your results
    14. Push this to github and tidy up the readme

- id: dafde938-d262-400b-80a7-19021dfbb66d
  name: Custom Yielding Dataloader
  description: |+
    1. Implement a `DataLoader` class which takes a few datasets and `yield`s batches of data
    2. Change your linear regression model to use it to perform mini-batch gradient descent

  
# - What is [gradient accumulation](https://towardsdatascience.com/gradient-accumulation-overcoming-memory-constraints-in-deep-learning-36d411252d01) and what problem does it solve?
# - What is [super convergence](https://medium.com/@abdelrhman.d/exploring-super-convergence-5fb3050b4667)?
# - Our `DataLoader` will not always return full dataset (check how it behaves for data with `100` samples and `batch_size=51`). Why is "leaving last batch out" fine for training? Why isn't it fine for validation? Make the dataloader return all data at each pass.
# - Create `LinearRegression` which has `predict` working on batches as well

# - Why didn't normalization help in case of analytical solution?
# - What other normalization schemes exist? Check out unit vector normalization
# - Try things presented in this notebook on different datasets. Maybe find one of your own and preprocess it?