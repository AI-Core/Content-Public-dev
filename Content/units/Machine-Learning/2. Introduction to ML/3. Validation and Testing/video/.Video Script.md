<<<<<<< HEAD
- Machine learning models are only useful if they can __generalise__ to make good predictions on unseen examples
- To estimate how well a model will perform on unseen data, we split our initial dataset into two different sets. One is for training on, and the other is for testing on.
-  The testing set is used for evaluating whether a model meets our requirements and estimating real world performance. That is all. It is not for making choices about our model.
- Sklearn provides a method `train_test_split()` in it's `model_selection` module to split our data.

```
from sklearn import datasets
from sklearn.model_selection import train_test_split

X, y = datasets.load_boston(return_X_y=True)

print(f"Number of samples in dataset: {len(X)}")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
```

## Validation

If we want to choose between two models, we can't base those decisions on the testing set. If we did, we would be making choices about the model that are biased towards doing well on answers that we are expecting.

Making choices based on the testing set is like seeing the answers on a test.

Instead, we create another set, called the __validation set__. This is used for comparing models or different options for the same model. We call this __cross validation__


```
X_test, X_validation, y_test, y_validation = train_test_split(
    X_test, y_test, test_size=0.3
)

print("Number of samples in:")
print(f"    Training: {len(y_train)}")
print(f"    Validation: {len(y_validation)}")
print(f"    Testing: {len(y_test)}")
```

> Pay attention: people commonly fail to understand the difference between the validation set and the testing set.

The difference between the validation set and the testing set is that we use the validation set to make choices about our models.

Such choices may include:
- Should I deploy the linear regression model or the neural network?
- Should I use the model which was trained on all the features of just the 3 I picked?
- Any choice of hyperparameter (which you will learn about shortly)

> We make the validation set by further splitting the training set.

```
import numpy as np
# ML algorithms you will later know, don't panic
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor

np.random.seed(2)

models = [
    DecisionTreeRegressor(splitter="random"),
    SVR(),
    LinearRegression()
]

for model in models:
    model.fit(X_train, y_train)

    y_train_pred = model.predict(X_train)
    y_validation_pred = model.predict(X_validation)
    y_test_pred = model.predict(X_test)

    train_loss = mean_squared_error(y_train, y_train_pred)
    validation_loss = mean_squared_error(y_validation, y_validation_pred)
    test_loss = mean_squared_error(y_test, y_test_pred)

    print(
        f"{model.__class__.__name__}: "
        f"Train Loss: {train_loss} | Validation Loss: {validation_loss} | "
        f"Test Loss: {test_loss}"
    )
```


## Summary of the different sets
- The training set is used to optimise the model parameters.
- The validation set is used to make decisions about which model is best to use or which settings are best for a model.
- The testing set is used to estimate how the model will perform on unseen data.

## How big should each set be?

Here is a good __rule of thumb__:
- Test = 15% 
- Val = 15%
- Train = 70%

The less data for validation, the less reliable will our estimates be, but more training samples will allow our algorithm to learn more. So the set size has to trade off those two things.

## Seeding
- When random numbers are generated in our code, it can make it hard to reproduce results
- Random numbers generated by python are not truly random, they just look random. 
- This means that there are ways to fix the sequence of random numbers generated
- to do that, we can just put `np.random.seed(2)` at the top of our file
- Here, the number 2 is our "random seed"
- It's good practice to average your training and testing results over a few seeds

## Data leakage
- Data leakage is where a model has access to information about the testing sets. 
- Never make any decisions about your model design using the test set

- Of course, data leakage can be caused by bad data splitting. These include
    - some samples are both in training and validation
    - model is simply evaluated based on performance on training data

- But there are less obvious ways to cause data leakage as well. 
One is to compute some statistics (like the mean) or new features (like difference from the mean) based on the data before splitting it, and then using those values in training, after splitting it. Those statistics contain data about the testing data which has since been split off.

## K-fold cross validation

- I also want to mention a technique for improving the accuracy of your validation scores called K-fold cross validation
    - in K-fold cross validation, you 
        1. firstly split the training set into K different folds, where k is a small integer like 5
        1. then turn by turn, you take each fold as the validation set
        1. and train a model on all of the data in the remaining folds and evaluate it's performance on that held out validation set
        1. then you average the validation performance across all k-folds
    - each fold gives you a new sample of how the model would generalise for a different split, and the average of them is likely to be more accurate then an individual fold
    - this does however require you to train K different models to validate a model
    - you usually do k-fold CV in scenarios where you don't have much data, and want to use as much of it as possible for training to avoid overfitting and increase the confidence in your results
    - if your dataset is large enough that the validation set is likely representative of the dataset as a whole, then the accuracy gains you get from k-fold cross validation probably won't outweigh the cost of training K different models for each parameterisation
    - this is especially true for large, modern, neural network models which are expensive to train
    - training one of them might be costly enough, let alone doing that for k of them


## In summary
- Our aim is to make models that generalise to make good predictions on unseen data
- we can use sklearn to split the data
- and we should split the data twice, to get a validation and testing set
- the testing set is used for evaluating the model to get an idea of how it will perform on totally unseen data, so that we can report this to stakeholders and ensure that our project requirements have been met
- but we should never make choices to change our model based on the test set because otherwise it wouldn't count as unseen data
- instead we use the validation set
- a decent split for your data might be something like 70% train, 15% val and 15% test
- the more data you train on, the more your model can learn, but the smaller your val or test sets are, the more unreliable their results
- you should use a random seed to make your results repeatable and average them over a few random seeds
- always make sure to avoid data leakage
- use K-fold cross validation in situations where your validation set may not be representative of the population
=======
# Validation and Testing

- __`[Explain why we need to divide our data into training, testing, and validation]`__
    - Think about why you want to train a model. You will eventually use the model to make predictions with data that doesn't have labels
    - But, how do you know if the model can perform well outside the data that you have?
    - In other words, how do you know if the model will generalize well?
    - The only way to know how well a model will generalize to new cases is to actually try it out on new cases
    - One way to do that is to put your model in production and monitor how well it performs. 
    - This works well, but if your model is horribly bad, your users will complain!

    - A better option is to split your data into two sets: the training set and the test set.
    - By evaluating your model on the test set, you get an estimation of how well your model will perform on instances it has never seen before.
    - That is why it is important to split your data into training and test sets.

    - On top of that, you can also split your data into a validation set and a test set.
    - To explain this, you need to know that models have a set of configuration parameters that can't be learned from the data. These are called hyperparameters. 
    - A hyperparameter can improve the performance of the model, but finding the best hyperparameter requires training the model on many different configurations.
    - Now, suppose you have a model, and now you want to set the hyperparameters to get the best metrics
    - One option is to train, for example, 50 different models using 50 different values for this hyperparameter.
    - You find the best hyperparameter that generalizes on your test set
    - The problem is that you are observing how well it performs multiple times on the same test set, and you found the hyperparameters to produce the best model for that set. 
    - This means that the model is unlikely to perform as well on new data.
    - A solution for this problem is to split your data into another set called the validation set.
    - You train multiple models on the training set
    - Then, you select the model and hyperparameters that perform best on the validation set
    - And finally, you run a single final test using the test set 


- Sklearn allows us to split the data into training and testing sets using the `model_selection.train_test_split` function
    - __`[Demo on how to split the breast cancer dataset into training and testing sets]`__
    - One problem that might arise is that the data is split randomly, so the training and testing sets are not necessarily well balanced.
    - This is a problem because the training set and the testing set have different distributions
    - So the model might not generalize well
    - To solve this problem, in the `model_selection.train_test_split` function, we can use the `stratify` parameter, which will distribute the data so that the training and testing sets have the same distribution as the original data
    - __`[In the notebook, you will see that the ratio of 0 over 1 is almost identical when stratifying the data]`__

- __`[Finally, discuss the concept of data leakage]`__
    - Be careful how and when you split your data.
    - If done incorrectly, you might end up passing information from the training set to the testing set. This is called data leakage.
    - The problem with data leakage is that the model implicitly learns from the training set
        - That would be like cheating, because to a certain degree, the model knows the answer
        - But when the model is tested on real data, it won't have that information
        - Meaning that the model is not generalizing well!
    - One way it can appear is when you compute transformations on the whole dataset, and then split the data into training and testing sets.
    - The transformation contains information of the whole dataset, so, when you split the data, the testing set implicitly contains information about the training set
    - Another reason your data can present data leakage is when you have repeated samples that appear in both training and testing sets.
>>>>>>> acbc071c6a0ffe08b3669dcff38bbbc87078e179
