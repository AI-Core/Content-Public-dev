- id: abd9d752-34f0-448d-8b03-4c53cd44d5d3
  name: Evaluating real performance on the house pricing
  description: |+
    1. Load in the California house price dataset
    2. Use sklearn to split the data into training and testing
    3. Read abot best practices for splitting data here
      - https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio
    4. Fit a linear regression model on the training set
    5. Score your model on the training set
    6. Score your model on the test set
    7. Run the script a few times and see, in general, which score is higher?
    8. Discuss why did i ask you to run it a few times, rather than just once?

- name: Reproducibility 101
  id: 255a747a-2920-4551-8580-f1349577ecd0
  description: |+
    1. Run your script for the last challenge again
    2. Discuss: why does the score change between runs?
    3. Look up how to set the random seed at the start of your program
    4. Run the script again
    5. You should see that fixing the random seed makes all random processes have the same outcome across runs
    6. Discuss what 2 random processes are happening in your program?

- name: Testing different models
  id: 91f94a98-c723-44b5-89cd-5344e04a5985
  description: |+
    1. Split the California dataset into training, test AND validation
    2. Evaluate 3 different models' scores on each set
    3. Save the scores for each set in a column and a row for each model
    4. Choose the best model and save it

- name: Train/test dataset splits
  id: 425bfc43-b7ca-43f0-8fe7-e9e66040d423
  description: |+
    1. This is an experiment to see how train/test split proportion changes the scores
    2. Split the boston dataset into train & test
    3. Fit a linear regression model on each
    4. Run the above process in a loop, changing the split proportion from 1% - 99%
    5. Create a graph which has split_proportion on the X-axis and score on the Y-axis
    6. Plot two curves
      - One for the train set score against split proportion
      - One for the test set score against split proportion
    7. Visualise them and discuss why it looks the way it does

- name: Train/test/validation dataset splits
  id: 0ed84fb2-e828-470d-8f58-dcbc0f7f6ee4
  description: |+
    1. Copy the code from the previous challenge as a starting point
    2. This is an experiment to see how train/test/validation split proportion changes the scores
    3. Split the boston dataset into train & test
    4. Now further split the training set into training and validation
    5. Vary the validation/training split from 1% - 99% of the original training set before it was split into 
    6. Fit a linear regression model on each
    7. Create a graph which has split_proportion on the X-axis and score on the Y-axis
    8. Plot three curves
      - One for the train set score against split proportion
      - One for the test set score against split proportion
      - One for score on the whole test set, against split proportion
    9. Visualise them and discuss why it looks the way it does

- name: Be careful!
  id: 88bf61ed-cc78-48b0-a364-14304eeb3c01
  description: |+
    1. Normalise the California dataset by subtracting the mean and dividing by the range
    2. This challenge is all about doing this in a way which prevents data leakage... so be careful!

- name: Your own data
  id: fca11d07-348b-4c8c-8bfd-14e05c4718ac
  description: |+
    1. Load a small part of your own data into the correct format (perhaps just one column as features and one as labels)
    2. Split it into test and training and validation
      - Choose a suitable model and fit it to your data
      - Compute the score on all sets
    3. Repeat this process with several other models and compare their score on the validation set to find and save the best one
    4. Save the model
    5. Load it back in and make a prediction on the test set