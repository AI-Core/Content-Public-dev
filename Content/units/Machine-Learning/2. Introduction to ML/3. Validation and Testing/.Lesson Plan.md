## Generalisation
- Previously, we fit a model and scored it on the same data.

- we don't want to build models that predict the labels that we already know. 
- We want to build models that predict things that we dont know!
	- like 
		- how likely is this person to upgrade their subscription?
		- how much should we quote this person for their insurance?

- draw a X-Y graph with some data on
- draw the "best" model, which passes through all of the points
- explain that even though we've minimised our criterion, that doesn't actually measure how good the model is
	- just how well it performs on the test data
- this means that the score on the training set isn't a good way to evaluate how the model will perform in the real world
- this comes back to the difference between optimisation and learning
	- because we dont have all the available data, the criterion doesnt reflect true performance
	- it's just an estimate of it
- so before we use these models in the real world, we should evaluate them on unseen data
- [ASK] how could we do this?
	- we could split our data into two parts
	- train on one
	- evaluate performance on the other
- it's absolutely necessary that the model does not see the test set during training
	- the whole point of the test set is to evaluate how we will do on unseen data
	- that data literally won't exist in production!
	- this is known as data leakage

## Code
- sklearn can help with this
	- look up "python sklearn split dataset" and check the documentation
	- show examples/train_test_split.py
		- point out kwarg test_size=0.2
			- what proportion of the data should we train on?
				- good rule of thumb is 20% testing data

## What is validaton
- often, we want to compare different settings across experiments
	- for example, we might want to choose between several different model options to find the best one
- [ASK] what would be the algorithm for comparing between models? 
- we should absolutely not evaluate the performance of each model by comparing their score on the test set
	- if we do that, then we are basing our model choice on data which is supposed to be unseen!
- instead, we should split the training set again, into a final training set, and a validation set
- note that we should keep the training-validation split consistent across different experiments
	- so that they are all being evaluated on the same data in the same way
	- otherwise you could have one model train on a particularly easy data split, and incorrectly conclude that it's the best model
- the only case in which we wouldnt do this, is if we want to test how the split proportion affects our performance

- [students have trouble grasping the difference between val and test]
	- could be worth talking about hyperparameters before validation?
	- page 118 of the deep learning book on "hyperparams and validation sets"
		- from: "earlier we discussed how a held-out test set...
		- to: "the generalisation may be estimated using the test set"

## Another data leakage problem
- if any information from any data outside of the training set is used when we fit the model, we've got data leakage
- the obvious cases are
	- choosing a model based on test set performance rather than validaton
	- or training on anything other than the training set
- a less obvious case can be where you compute some statistics like the mean from the entire dataset, and then use it to compute some features for the training set
	- for example, if we were to create a new feature, which is the difference from the average age, but we've calculated the average using the validation and test dataset too!
- when doing preprocessing, imagine you only have the training set

## Key takeaways
- the aim of ML is to find models whose performance generalises to unseen data
- to get an idea of how well our models are going this, we split our dataset into testing and training
- decisions about our models should never depend on the test set
- we want to be able to make choices about different models based on their ability to generalise
- but we can't use the test set to do this
- so we further split the training set into training and validation
- the validation set is used as a measure of generalisation which we CAN base model decision on
- after making choices about the model using the validation set, we can get an estimate of performance using the test set