- Machine learning models are only useful if they can generalise to make good predictions on examples which they have not seen during training
- There are two main ways in which a machine learning model can fail to generalise
- [SHOW] side by side graphs of linear function underfitting and wavy function overfitting
    - both should show dotted x^2 function with a few datapoints lying on it and a label y = wx^2
    - left should show a linear function with a label y_hat = b + w_1x
    - right should show a polynomial function with a label y_hat = b + w_1x + w_2x^2 + ... w_10x^10
- The first is that our model can underfit
- We call it underfitting, because how well it can fit the input-output relationship in our data is under the desired performance
- When underfitting occurs, the model performs badly on both the training set and the validation set
- we also refer to an underfit model as a biased model


- The other generalisation issue is that our model can overfit
- We call it overfitting, because how well it can fit the input-output relationship in our data is overly specific to the training set, such that it's predictions are poor for the validation set
- we also refer to an overfit model as model with high variance
- we call it variance because there is a wide variation of models which can give acceptable performance on the training set

- we call the ability of a model to represent a function it's capacity
- our overfit model has higher capacity than the underfit model

- [SHOW] Below that, loss curves for each case
- Here I'm showing graphs that plot the loss of each model against the number of gradient descent update steps
- Underfitting causes the model to perform badly on both the training and validation set
- You can see that the loss for neither the training or validation set reaches below an acceptable level
- Overfitting causes the model to perform well on the training set, but not on the validation set
- You can see that the training loss reduces below an acceptable level, but the validation set doesnt. This indicates that the model won't generalise to unseen data.

- ideally you want both curves to reach the threshold acceptable performance

- the gap between the training and validation loss curve at any given step is called the generalisation gap
- ideally we want there to be no generalisation gap

- sklearn won't give you these curves - it only gives you the final results, but if you implement the algorithm from scratch, or if you are using a library like pytorch then you can visualise these curves to quickly diagnose
- otherwise, just compare your loss metrics on train and validation with each other and with whatever your problem defines as acceptable performance

- So how can we address these issues?
- To reduce underfitting we can increase the capacity of our model so that it can represent more complex functions
- To reduce overfitting we can decrease the capacity of our model, so that it doesnt have the flexibility to pass through the datapoints in so many different ways
- We can also reduce overfitting by collecting more data
- By collecting more data we will increase the places along the true data generating function where our model has to pass through
- There are many other techniques for reducing the generalisation error, which are known as regularisation techniques

- Let's look at underfitting and overfitting from a mathematical lens to understand why we call them bias and variance

- Firstly, bias
- Mathematically, having a biased model means that if we were to try to optimise the model many times, the average parameter vector we end up with would still be different to the true parameter vector
- [SHOW] mathematical equation for bias
- [SHOW] function 3 - 4x + x^2 (passes through x=1 and x=3) and linear function y_hat = 1 + 2x trying to fit it
- Now let's imagine that we have collected data which in reality, comes from a quadratic function, 3 - 4x + x^2, and that we're trying to model it with a linear function y_hat = Xw + b, which currently has a bias equal to 1 and a weight equal to 2
- If we knew this function, we would know that it has 3 parameters, the bias, one coefficient for the x term and one coefficient for the x^2 term
- [DRAW] vector of [1, -4, 3]
- This function of course only has a parameter for the bias and the x term
- You could consider it as having an x^2 term which is always = 0
- Now we can tell that regardless of how we train that model, the parameter vector is always going to be different to the true parameters of the data generating function, because we are always going to be missing the x^2 term
- bias is when we are predicting the wrong thing

- [PAUSE]
- Note that in this example I'm talking about linear regression, and linear regression always has the same optimum parameters, but other models combined with other optimisers might result in different resulting parameterisations each time you train them
- I want to be very clear that we never know the true data generating function. If we did, we would have no need to build a machine learning model to predict an output from an input - we could simply plug the input into the function.

- Now let's turn to look at variance
- Mathematically, a model with high variance is one where there is significant variation in the values of the parameters each time it is trained
- [SHOW] mathematical equation for variance
- the equation for bias gives larger values when the estimated parameters are not consistent


-
# TO summarise