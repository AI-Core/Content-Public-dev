name: '2.2.2 Machine Learning: Regularisation'
questions:
  - options:
      - correct: true
        option: Regularization method
      - correct: true
        option: Can stop training based on validation loss not improving
      - correct: true
        option: Can stop training based on training loss not improving
      - correct: false
        option: We always wait for 1 epoch as the loss will never improve when it
          stagnates
      - correct: true
        option: We specify waiting period as the loss might improve after a few epochs
    question: What is early stopping?
  - options:
      - correct: false
        option: Usually we checkpoint every model every batch of data
      - correct: false
        option: Usually we checkpoint every model every epoch
      - correct: true
        option: Usually we checkpoint the best model (based on specified metric)
      - correct: true
        option: We can checkpoint models at different times and make an ensemble out
          of them
    question: Which of the following statements about model checkpointing is true?
  - options:
      - correct: false
        option: Different implementation for different loss functions
      - correct: true
        option: They are independent of the loss function
      - correct: true
        option: Can be coded directly and differentiated through
      - correct: true
        option: Can be coded indirectly on gradient weights
      - correct: true
        option: Regularizes weights of model
      - correct: false
        option: Regularizes cost function smoothing it's minima
      - correct: false
        option: Are used to minimize training loss faster
      - correct: true
        option: Are used to make the overfitting less severe
    question: Mark everything True about LP regularizations
  - options:
      - correct: true
        option: It is calculated on weights
      - correct: true
        option: sqrt term is not important, power of 2 is the key in the formula
      - correct: true
        option: Larger weights are penalized more than the smaller ones
      - correct: false
        option: All weights are penalized equally according to weight's magnitude
      - correct: false
        option: Good for variable selection
      - correct: false
        option: Introduces sparsity in models
    question: Mark everything True about L2 regularization
  - options:
      - correct: true
        option: It is calculated on weights
      - correct: false
        option: sqrt term is not important, power of 2 is the key in the formula
      - correct: false
        option: Larger weights are penalized more than the smaller ones
      - correct: true
        option: All weights are penalized equally according to weight's magnitude
      - correct: false
        option: Loss landscape is definitely smoother
      - correct: true
        option: Good for variable selection
      - correct: true
        option: Introduces sparsity in models
    question: Mark everything True about L1 regularization
  - options:
      - correct: true
        option: One can learn it from data
      - correct: true
        option: Find using domain knowledge
      - correct: true
        option: Find using Cross Validation or related techniques
      - correct: false
        option: The larger value the better model generalization
      - correct: true
        option: The smaller the value, the easier it is for the model to learn as
          it can move weights freely
      - correct: true
        option: Too large value may make the cost function dominated by regularization
          term
      - correct: false
        option: It is a parameter
    question: Make everything true about the regularization parameter (usually lambda)
  - options:
      - correct: true
        option: Severe form of overfitting
      - correct: false
        option: Unlikely to happen for neural networks
      - correct: true
        option: Models with higher capacity are more likely to exhibit this behaviour
      - correct: false
        option: It is good for generalization as it keeps data in memory for reference
      - correct: false
        option: More likely to happen with large amounts of data
      - correct: true
        option: More likely to happen with small amounts of data
    question: Mark everything True about memorization
