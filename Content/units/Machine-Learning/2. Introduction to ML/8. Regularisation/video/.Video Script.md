# Regularisation

## Setup

### What do i need open?

- Open VSCode
- Open the notebook called `.regularisation.ipynb`
- OBS
    

### What recording scenes do I need open?

- Screencapture of VSCode with your webcam capture in the corner
- Webcam capture in full screen

## Script

- __`[START SCENE] webcam in full screen`__
    - When you train your model, one problem that is likely to occur is overfitting.
    - Overfitting causes the model to learn too much from the training data.
    - As a consequence, the model will perform well on the training data, but when it is tested on new data, it will perform poorly.
    - In general terms, one of the reasons it happens is because the model has learned the noise in the training data.
    - And it makes the model to think that this noise is important, and it will try to apply the random pattern of the noise to the new data.
    - Technically speaking, one of the reasons our model is overfitting is because it is too complex.
    - We can reduce the complexity of our model by adding using different techniques. 
    - For example, we can use a simpler model
    - Or we can reduce the number of features in our model
    - Or we can add penalty terms to the loss function
    - These techniques are called regularisation
    - Let's take a look at an example to see how regularisation works.

<br>

- __`[CHANGE SCENE] Open VScode - .regularisation.ipynb`__
    - In this example we have an artificial dataset.
    - __`[RUN THE CELL]`__
    - As you can see, the dataset follows a quadratic pattern, to which I added some noise.

<br>

- __`[NEXT CELL]`__
    - One way to make a model in sklearn that fits a quadratic function is to use a PolynomialFeatures transformer.
    - This transformer adds new features to the dataset, by taking the powers of the existing features.
    - When we use the PolynomialFeatures transformer, we can set the degree of the polynomial
    - In this case, since we are talking about a quadratic function, we set the degree to 2.
    - __`[RUN THE CELL]`__
    - We obtain a new array with three features, where the first column is the power of 0, the second column is the power of 1, and the third column is the power of 2.

<br>

- __`[NEXT CELL]`__
    - We can use this new array to train our model.
    - And we are using a simple LinearRegression model.
    - __`[RUN THE CELL]`__
    - Ok, after training the model and make some predictions, we obtained the MSE of the model on the training data and on the test data.
    - And after plotting the results, we can see that the model is simply fitting the quadratic function to the data.
    - But it doesn't fit the noise

<br>

- __`[NEXT CELL]`__
    - Let's add some complexity to our model.
    - Instead of using a power of 2, we are using a power of 20
    - And we repeat the same process
    - __`[RUN THE CELL]`__
    - The MSE for the training set is much better, but the MSE for the test set is worse
    - As mentioned, this happened because the model is learning from the noise of the training set
    - So when it encounters a new dataset, it will try to apply the same pattern of the noise to the new data
    - So, you could see that by reducing the complexity of the model, we are able to increase the MSE of the test set
    
<br>

- __`[CHANGE SCENE] webcam in full screen`__
    - Another way to add regularisation to the model is to stop the training process when a metric is not improving or worsening.
    - In this case, the metric is the MSE calculated on the test set
    - So, if the MSE of the test set is not improving, it means that the model is not learning anything important from the training set
    - And if the MSE of the test set is worsening, it means that the model is learning from the noise of the training set
    - So, we could stop the training process when the MSE of the test set is not improving
    - This is called early stopping
    - As the name suggests, early stopping consists of stopping the training at the right time
    - You stop training the model when you see it starts underperforming on the test set
    - Let's see an example of early stopping

<br>

- __`[CHANGE SCENE] Open VScode - .regularisation.ipynb`__
    - In this example we will use a Stochastic Gradient Descent optimisation algorithm
    - We also split the dataset into a training set and a test set
    - And we set max_iter to be 1, so at each epoch, we will only update the weights once
    - In this `for` loop we are training the model, each iteration consists of updating the weights of the model once
    - __`[RUN THE CELL]`__
    - As we train the model, we also calculate the MSE on the test set

<br>

- __`[NEXT CELL]`__
    - Let's plot the rooted MSE on both the training set and the test set
    - We also calculate the best RMSE on the test set, to see if it reaches a minimum
    - __`[RUN THE CELL]`__
    - Notice that the RMSE on the training set is always improving
    - But, on the other hand, we can see that after epoch around 60, the MSE on the test set is worsening
    - Early stopping consists of stopping the training at that epoch and use the model that we obtained in that moment

<br>

- __`[CHANGE SCENE] webcam in full screen`__
    - So far we have seen that regularisation works by adding constraints to the model.
    - Another constraint that we can use is applied to the weights of the model.
    - There are three main ways to apply this constraint:
    - Lasso Regression, Ridge Regression, Lasso Regression, and Elastic Net
    - All of them add a penalty term to the loss function
    - Let's see an example of Lasso Regression

<br>

- __`[CHANGE SCENE] Open VSCode - Show the LASSO equation`__
    - LASSO stands for Least Absolute Shrinkage and Selection Operator
    - In Lasso Regression, the penalty term is the sum of the absolute values of the weights
    - So the equation for the loss function is the one shown here
    - Alpha is a hyperparameter that controls the amount of regularisation
    - By adding the absolute value of the weights to the loss functions, we are trying the shrink the weights
    - And if the weights are not relevant at all, it will eventually be zero
    - Hence, the name of the algorithm, we are shrinking and selecting the weights

<br>

- __`[NEXT CELL]`__
    - Let's use an example we saw earlier with the quadratic function
    - Once again, we create an artificial dataset that follows a quadratic pattern and add some noise
    - Then, we split the data into a training set and a test set
    - __`[RUN THE CELL]`__

<br>

- __`[NEXT CELL]`__
    - Now, we initialize two models, the LinearRegression model and the LassoRegression model
    - By default, in sklearn, the value of alpha in the Lasso regression is set to 1
    - __`[RUN THE CELL]`__
    - After plotting the predictions for both algorithms, we can see that the predictions of the Lasso regression are much smoother than the Linear regression
    - This is because the Lasso regression has zeroed out the high-degree polynomial features that added noise to the dataset
    - And it's keeping the low-degree features that are relevant

<br>

- __`[NEXT CELL] Show the equation`__ 
    - The second algorithm that adds a penalty term to the loss function is Ridge Regression
    - When you apply ridge regularisation, you add a penalty proportional to the sum of the squares of the weights
    - As you can guess, the model will be penalised for having large weights
    - This helps reducing overfitting by preventing the model from having large weights that can impact the outcome of noisy data

<br>

- __`[NEXT CELL]`__
    - Once again, let's train two models, one using the LinearRegression model and the other using the RidgeRegression model
    - __`[RUN THE CELL]`__
    - Observing the cells, we can see that the output of the Ridge regression is much smoother than the Linear regression
    - And in some cases, it almost look linear
    - This is because the model is again getting the giving more importance to the relevatn features
    - But, as opposed to Lasso, Ridge regression is not removing irrelevant features
    - So, it's a good idea to use Lasso Regression when you have a lot of features
    - And Ridge Regression when you have a few features that you know are important

<br>

- __`[NEXT CELL] Show the equation`__
    - Finally, we have elastic net regularisation
    - The regularization term is a simple mix of both ridge and lassoâ€™s regularization
    - You can control the mix ratio named r
    - If r = 0, then it is just ridge regression
    - If r = 1, then it is just lasso regression

<br>

- __`[NEXT CELL]`
    - We can use Elastic Net in sklearn using the ElasticNet class
    - And one argument that we can pass to the constructor is the r parameter, which by default is set to 0.5
    - __`[RUN THE CELL]`__
    - Once again, we are giving less importance to the irrelevant features
    - So the predictions are much smoother

<br>

- __`[CHANGE SCENE] webcam in full screen`__
    - You saw that regularization is a powerful technique to prevent overfitting
    - There are many ways to implement it
    - And mainly they consist of constraining the model
    - For example, by reducing the complexity of the model
    - Or by adding constraints to the training process, such as early stopping
    - Or by adding a penalty term to the loss function
    - Either way, it's important to bear in mind that, whenever you feel like the model is overfitting, you should think about regularization
    - The regularization technique that you use will depend on the problem you are solving
    - So make sure to check the problem you are solving and the regularization technique you are using!