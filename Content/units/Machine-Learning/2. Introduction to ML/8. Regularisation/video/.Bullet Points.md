# Regularisation

- Our key goal when building machine learning models is to make them generalise so that they produce accurate predictions on unseen examples
- That is, we want to reduce the generalisation error - the difference between performance on the training set and the validation set
- We call any technique designed to reduce the generalisation error of a model _Regularisation_

- Let me show you some of the fundamental regularisation techniques you should know about

## Reducing model capacity

- poor generalisation is caused by overfitting, and overfitting is symptomatic of overly complex models

_Show overly complex model_

- You can see here that one of these models is clearly overfit, because it has a very high capacity

- because of this, probably the simplest way to regularise your model is to reduce it's capacity, and use a simpler model
- in the case of a polynomial, that would mean using a lower order polynomial so that it can't represent such wild functions

## Weight decay

- weight decay encourages your model to be more simple without having to decrease it's capacity by making the model weights smaller

- we implement weight decay by adding a penalty to the loss function

_Show loss function with generic norm penalty_

- the form of this penalty is 
    - a parameter, lambda, which is known as the regularisation factor which controls how much the model is simplified
    - followed by a norm of the model weights

- what is a norm?
- a norm on the weights of our model is a mathematical term that gets larger as the weights gets larger

_Show side by side LP, L1 & L2 norm penalties_

- mathematically, what we would call the L-P norm is where you take a vector, raise the absolute value of each element to the power of a number, p, sum them up, and then take the pth root of that number
- in our case, that vector would be a vector of all of our model weights
- so the L1 norm would be equivalent to taking the sum of the model parameters
- and the L2 norm would be equivalent to finding the magnitude of that parameter vector
- you will see them written as the vector you want to take the norm of with 2 vertical bars on each side and a subscript indicating the value of p
- and the L2 norm is used so frequently that the subscript is often omitted

_Show graphs for each norm penalty_

- we are going to add these surfaces to our loss surface
- each of these norm penalties has a minima at zero, so they pull the minima of the loss surface towards zero

_Show adding the norm penalty and original loss functions_

- the regularisation factor scales the strength of that pull up and down
- graphically that would be scaling up the vertical height of the regularisation term's surface

_Switch to webcam_

- Why does this work?

_Show adding the norm penalty and original loss functions again_

- one way to think about it is that overfitting is when your model has high variance, which means there are many different parameterisations which perform equally well. 
- regularising the loss function puts a preference on those parameterisations whose parameter norms are closer to zero

_Switch to webcam_

- another way to visualise it is that each of these weights effectively determines how much influence a particular feature has

_Switch to ipad and show graphs with weights equal to zero, close to zero, and not close to zero_

- it's helpful to think about the parameter values in the limit when they are equal to zero
    - in this case, any change in feature is going to have zero inflence on the output, so the model will predict the same thing everywhere
    - that's a very simple model
    - as you allow the parameter values to move away from zero, each feature can exert more influence and change the shape of the function more
- so norm penalties help by reducing how much each feature changes the output, by reducing their weighted influence


- typically, we use the L2 norm penalty

_Show modified loss function_

- this formulation of the loss function is commonly called weight decay or ridge regression

_Switch to webcam_

- alternatively, you can use the L1 norm penalty

_Show modified L1 loss function_

## Which norm to use when?

- So, when would you want to use which one?
- Let's think about the way that they influence the parameter updates when using gradient descent
- the gradient of the regularized loss function is going to be equal to the gradient of the original loss function added to the gradient of the norm penalty
- the gradient indicates the strength with which the parameters are pulled in that direction
- the L2 norm penalty is steep far away from zero, and not steep close to zero
    - so it pushes down large parameter values strongly, but barely affects small parameter values
- the L1 norm penalty is the same steepness everywhere
    - that means that it will always pull the parameter towards zero, regardless of the parameter size
    - unless the gradient of the original loss function pulls strongly enough in the other direction, the parameter value will be pushed all the way to zero
    - and the only reason that the original loss function would pull in the other direction is if it is reduced by having a higher value
    - a result of that, is that if the parameter doesn't affect the original loss, then it will be pushed all the way to zero
    - so, lasso regularization can be used as a form of feature selection because it will tell you which features don't affect the original loss function, as their corresponding parameters will be pushed to zero
    - you can then remove these and save memory and compute when using your model

- You can also use what is called elastic net regularisation, which is a simple mix of both ridge and lasso regularization and includes a parameter which weights the importance of each


- one thing to be careful of is that you should not regularise the bias of a model

_Show diagram_

- that's because constraining the bias doesn't make the model simpler, which is the point of weight decay, it just makes it prefer passing through the axis closer to zero, which might just not be what the true data generating function looks like

_Switch to webcam_

- Another thing to note is that the regularized loss does not tell you how good the model is at performing well on the data
- it tells you how good the model is at performing well on the data and minimizing the norm penalty at the same time
- so once the model is trained, you want to evaluate the modelâ€™s performance on the data using the unregularized performance measure


## Early stopping

- One regularisation technique you can use is called _early stopping_

_Show early stopping graph_

- Early stopping is where you stop the training before the __training loss__ converges
- Instead, you stop the training when the __validation loss__ stops reducing and begins to increase
- That is, when the model begins to overfit
- In practice, you would save your model every so often and after training pick the one which achieved the best generalisation error

## Implementing early stopping in code
- So, here's how I would implement that in sklearn 

_Look up "sklearn linear regression early stopping"_

- we're looking for a model, so this SGD regressor looks good

_Open page_

_Switch to webcam view_

- The simple linear regression model uses the analytical solution to optimise the parameters.
- That analytical solution is unsolved until the moment that the calculation completes, at which point you have the parameters that give optimal performance on the training set
- As such, you can't see it's performance over time
- That means that you can't plot loss curves, and you can't evaluate the generalisation error periodically
- That tells us that we can only use early stopping for models which are trained iteratively

_Visualise being typed out "typewriter style": "we can only use early stopping for models which are trained iteratively"_

_Switch to screen capture and show helper functions in first cell_

- Before we use the SGD regressor, look at these helper functions I've got written
- One of them just creates some fake data, splits it, and normalises it
- And one of them uses that data to train a model, for 50 epochs
- we're going to implement the sgd regressor so that this fit method only does a single weight update each epoch
- after each update, we calculate and save the training and validation losses to visualise their curves later
- We also plot a dotted horizontal line at the lowest validation loss

_Switch to screen capture showing documentation in browser_

- So here, we use this different version of the linear model in sklearn, called the SGDRegressor, which is a linear regression model optimised iteratively using gradient descent

- We set max_iter to be 1, so at each epoch, we will only update the weights once
- the tolerance keyword argument is the difference between the train and val loss which we will stop training at
- and set warm_start=True so that the model continues from where it left off each iteration

- finally, when we get the loss curves for the model
- you can see that the model starts to overfit to the training set around here, so that's where we should stop our model

_Switch to webcam view_

- this is obviously a toy example for a few reasons
- firstly,we're not doing using minibatch gradient descent as we usually would
- secondly, we're evaluating the model on the validation set every step
    - for a large model and a large validation set, this could be expensive to compute and take a while
- in practice, we would do something like check the validation performance every 20 epochs, depending on the size of the dataset and model
- then, importantly, we would save a version of the model at that checkpoint
- then after the training process has run all the way, probably to the point of overfitting, we can look at the training curves and identify which of our saved models would result in the lowest generalisation error

## Outro
- There are many other types of regularisation too. But we'll save those for later
- For now, you should have a rough understanding of we can improve the generalisation ability using regularisation 
- we can do that using techniques like 
    - reducing model capacity
    - introducing penalties to the loss function
    - changing
