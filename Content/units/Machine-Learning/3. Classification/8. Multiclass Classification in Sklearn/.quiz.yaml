name: 2.3.2 Multiclass Classification
questions:
  - options:
      - correct: true
        option: Can be considered as multiple linear regressions ending with softmax
          function (multiple outputs)
      - correct: false
        option: Can be considered as multiple linear regressions ending with sigmoid
          function (multiple outputs)
      - correct: false
        option: Can be considered as single linear regression with sigmoid activation
          (single output)
      - correct: false
        option: Can be considered as a single linear regression with softmax activation
          (single output)
      - correct: false
        option: Output(s) are independent of each other
      - correct: true
        option: Output(s) are dependent of each other
    question: Mark everything true about multiclass classification
  - options:
      - correct: false
        option: Can be considered as multiple linear regressions ending with softmax
          function (multiple outputs)
      - correct: true
        option: Can be considered as multiple linear regressions ending with sigmoid
          function (multiple outputs)
      - correct: false
        option: Can be considered as single linear regression with sigmoid activation
          (single output)
      - correct: false
        option: Can be considered as a single linear regression with softmax activation
          (single output)
      - correct: true
        option: Output(s) are independent of each other
      - correct: false
        option: Output(s) are dependent of each other
    question: Mark everything true about multilabel classification
  - options:
      - correct: true
        option: Takes in logits and returns a probability distribution over different
          class labels
      - correct: false
        option: Takes in logits and returns class indices
      - correct: true
        option: Input is same shape as output
      - correct: true
        option: Output must not contain negative numbers
      - correct: true
        option: Output must sum to one
      - correct: true
        option: Used to normalize a probability distribution, like how the sigmoid
          is used in binary classification
    question: Mark everything true about the softmax function
  - options:
      - correct: false
        option: It is less computationally expensive to compute than a regular max
          function
      - correct: true
        option: It is smooth and differentiable
      - correct: false
        option: No reason
      - correct: false
        option: Typically run on software, rather than computed by hand
    question: Why does the softmax have "soft" in the name?
  - options:
      - correct: true
        option: Most of the time, output elements are all nearly zero, except for
          one, which is nearly one. Hence, it is like a max function which outputs
          zero everywhere and one at the position of the largest input element.
      - correct: false
        option: It relates to maximum likelihood estimation
      - correct: false
        option: It is used when we want to maximise the predicted likelihood of the
          correct label
      - correct: false
        option: It's a more extreme version of the sigmoid
    question: Why does the softmax have "max" in the name?
  - options:
      - correct: true
        option: We need it because we may obtain nan (not a number) for large values
      - correct: true
        option: Overflow causes problems in case of standard implementation
      - correct: false
        option: Underflow causes problems in case of standard implementation
      - correct: true
        option: We subtract largest value across the last dimension in order to stabilize
          it
      - correct: false
        option: We add largest value across the last dimension in order to stabilize
          it
    question: Mark everything true about the stable softmax
  - options:
      - correct: false
        option: It is more memory (RAM) efficient
      - correct: true
        option: It is less memory (RAM) efficient
      - correct: true
        option: It can be applied to both features and labels
      - correct: false
        option: It can be applied to both continuous and categorical features
      - correct: true
        option: For each different feature with c possible categorical values, one-hot
          encoding replaces that feature with c new binary features
      - correct: true
        option: It does not encode order in the data
      - correct: true
        option: We usually don't apply one-hot encoding to our labels
    question: Mark everything true about one hot encoding
  - options:
      - correct: true
        option: Generalization of BCE
      - correct: true
        option: Reducing the loss for just the true class's predicted probability
          by pushing it up towards one pushes the other class's predicted probabilities
          down
      - correct: true
        option: Used in multiclass classification
      - correct: false
        option: Used in multilabel classification
      - correct: false
        option: Takes in values in range (-inf, +inf)
      - correct: false
        option: Outputs values in range (-inf, +inf)
      - correct: true
        option: For a single example, the total loss is the average cross entropy
          loss applied to each output class probability
    question: Mark everything true about the cross entropy loss
