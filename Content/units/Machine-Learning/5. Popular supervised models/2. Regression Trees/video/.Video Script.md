# Regression Trees

- __`[THIS VIDEO WILL BE MUCH SHORTER, SINCE DECISION TREES ARE ALREADY EXPLAINED]`__

- Regression Trees is a decission tree whose objective is to predict a continuous value
    - They work the same way as classification trees, but now we are not dealing with classes, so we shouldn't compute Gini impurity
    - Instead we can compute a metric characteristic of regression models, usually Mean Squared Error (MSE)
    - Additionally, the buckets we get at each node are not classes, but continuous values
    - Let's see an example to understand how it works

- __`[RUN THE DEMO ON THE NOTEBOOK]`__
    - First, let's create a dataset with a single feature that resembles a quadratic function
    - We also add some noise to the target values
    - __`[RUN THE CELL]`__
    - We can see that the data looks like a quadratic function, but due to the noise we added, the function is not perfect
    - __`[NEXT CELL]`__
    - __`[RUN THE CELL]`__
    - Don't worry about this cell, this is just a helper function that we use to plot the predictions of the model
    - __`[NEXT CELL]`__
    - Now, we are creating and training two regression trees, one with a depth of 2 and one with a depth of 3
    - __`[RUN THE CELL]`__
    - And we obtain a couple of graphs that show the predictions of the models
    - Notice that the predictions of the first model have 4 possible different values, one around 0.83, another around 0.6, another around 0.15, and another around 0.7
    - The second model has many more possible values, 8 to be exact
    - Ok, so now that we have trained both models, let's take a look at the trees
    - __`[NEXT CELL]`__
    - __`[RUN THE CELL]`__
    - This cell has generated two .dot files, one for each tree
    - Let's open the first one and see what it looks like
- __`[OPEN "regression_tree_2.dot" and click on the three dots on the top right corner to open the preview]`__
    - We can see that the regression tree has a depth of 2, and each bucket has a corresponding value
    - This value is the average of all the values of the labels in that bucket
    - Notice also that each bucket has a squared error, which is the mean squared error of all the values in that bucket with respect to the average
    - Now, when the model makes a prediction, it looks at the value to be predicted and decides which bucket to go to based on the decision rules until it reaches a leaf node
    - For example, if x is 0, first, the model will put that into the left bucket because it's lower than 0.15, and then it will check whether x is lower than 0.078.
    - Since it's lower, it will go to the left bucket, so the model predicts that the label corresponding to 0 is 0.834
    - So, as you can see, there are only 4 leaf nodes, which is the number of possible values we saw in the graphs earlier
    - Notice that the model is underfitting because the model is too simple
- __`[OPEN "regression_tree_3.dot" and click on the three dots on the top right corner to open the preview]`__
    - Let's take a look at a slightly more complex tree
    - This tree is the one with a depth of 3 levels
    - Now, we have 8 leaf nodes, which is the number of possible values we saw in the graphs earlier
    - Great, the model is giving better results
- How does the model know where to make the splits?
    - Regression trees try to minimize the MSE of the buckets
    - It does it by minimizing the following equation:
    $$
    Loss = \frac{m_{left}}{m}MSE_{left} + \frac{m_{right}}{m}MSE_{right}
    $$
    - Where the $MSE_{left, right}$ is the Mean Squared Error at each child node, $m_{left, right}$ is the number of samples in each child node, and $m$ is the total number of samples in the node
    - So, we can see that regression trees and classification trees are very similar, they both try to minimize a metric in from a node to its children

- We can implement regression trees using the tree library from sklearn
    - We can use the DecistionTreeRegressor class from sklearn.tree
    - The regularization hyperparameters are the same as for classification trees
    - For example, you can set the max_depth as we saw to control the maximum depth of the tree
    - Another hyperparameter is the min_samples_leaf, which controls the minimum number of samples in a leaf node
    - Or you can set the min_samples_split to control the minimum number of samples in a node before splitting

- Similar to classification trees, regression trees have also limitations
    - They are prone to overfitting, so it's important to control the hyperparameters
    - Another problem is that the training step scales up as the number of samples and features increases
    - So, for large datasets, the training step can take a very long time