# Classification Tree

## Setup

- Install "Graphviz (dot) language support for Visual Studio Code" from the VSCode extensions to visualize the tree

## Script

- Decision trees are machine learning algorithms that can perform both classification and regression tasks
    - It's a non-parametric method that creates a model that predicts a target using simple decision rules
    - These decision rules are essentially True or False rules
    - For each rule, the decision will split the data into two buckets
    - One bucket for those samples that meet the rule
    - And another bucket for those who don't
    - These rules are based on the dataset we have
- Decision trees can be used for both classification and regression tasks, but for now, we'll focus on classification
- The algorithm is the same as explained, where the end goal is to predict the class of a sample
    - Let's see an example:
        - __`[RUN THE DEMO IN THE NOTEBOOK]`__
        - In this example we are using the famous iris dataset, where the target is the species of the flower, and the features are the measurements of the flower
        - Amongst those measurements, we have the sepal length, sepal width, petal length and petal width
        - We will train a simple decision tree to predict the species of the flower, and then we will visualize the rules that were used to make the predictions
        - __`[RUN THE FIRST CELL]`__
        - This first cell is loading the iris dataset and training a decission tree to classify the species of the flower
        - __`[RUN THE SECOND CELL]`__
        - With this second cell we are exporting the resulting tree to a .dot file, which can be visualized in a graph
    - __`[OPEN THE .dot FILE]`__
        - This file contains the information to plot the graph, as it is, it is not very readable. But we can install an extension in VSCode to plot it. 
    - __`[GO TO THE EXTENSIONS TAB AND LOOK FOR THE "Graphviz (dot) language support for Visual Studio Code" EXTENSION]`__
        - This extension will allow us to visualize the tree in a graph
    - Now that we have installed it, let's take a look at the tree
    - __`[OPEN THE TREE CLICKING ON THE THREE DOTS ON THE TOP RIGHT and SELECT OPEN PREVIEW TO THE SIDE]`__
    - <p align=center><img src=.images/classification_1.png width=800></p>
        - Alright, we have our decission tree plotted, but what does it mean?
        - As mentioned earlier, the tree is a visual representation of the rules that were used to make the predictions
        - Let's start from the top of the tree, also called the root node
        - We can see that we have 150 samples, and three classes, with a distribution of 50 samples in each class
        - In the root node we are asking: "Is the petal width lower or equal to 0.8?"
        - Based on that question, we can split the data into two buckets, samples with petal width lower or equal to 0.8 and samples with petal width higher than 0.8
        - So, for example, if a flower has a petal width of 0.4 cm, it will go to the first bucket and we will predict it is Iris-setosa
        - On the other hand, if a flower has a petal width of 1.2 cm, it will go to the second bucket and we still will be unsure about the species of the flower. We can ask another question, such us: "Is the petal width lower or equal to 1.75cm?"
        - Based on that question, we can split the data into two more buckets, samples with petal width lower or equal to 1.75 and samples with petal width higher than 1.75
        - So, once again, starting from the root node, if a flower has a petal width of 1.2cm, it will go to the second bucket and then it will go to the green bucket. 
        - The model would predict that the flower is Iris-versicolor, and we know that the prediction is fairly accurate because in that bucket we have 49 iris-versicolor samples and only 5 iris-virginica samples
    - So, how does the decision tree know what decision rules should be applied? 
    - Why did it use petal width as the decisive feature and why did it take those values for the petal width?
    - It's all about the splits. Remember that a machine learning model usually works by minimizing a loss function
    - In this case, it tries to minimize the Gini impurity at each node starting from the root node. 
    - So, what is the Gini impurity?
    - Let's take a look at the tree we created to understand it
    - __`[OPEN THE TREE CLICKING ON THE THREE DOTS ON THE TOP RIGHT and SELECT OPEN PREVIEW TO THE SIDE]`__
        - The Gini impurity is a measure of the distribution of the classes in a node
        - The higher the Gini impurity, the more uniform the distribution of the classes will be
        - On the other hand, the lower the Gini impurity, the more pure the split will be
        - It is calculated using this formula:
        $$
        G = 1 - \sum_{k=1}^{n} p_{i, k}^2
        $$
        - Where p_i is the ratio of class k instances among the training instances in the ith node.
        - For example, in the tree we saw earlier, the Gini impurity of the root node is 0.667:
        $$
        G = 1 - \frac{50}{150}^2 - \frac{50}{150}^2 - \frac{50}{150}^2 = 0.667
        $$
        - However, take a look at the orange bucket. It contains 50 samples of a single class. That means that the split is quite good, because we managed to isolate that class from the rest
        - The Gini impurity is 0:
        $$
        G = 1 - \frac{50}{50}^2 = 0
        $$
    - So, classification trees try to minimize the Gini impurity at each node using the following formula:
    $$
    Loss = \frac{m_{left}}{m}G_{left} + \frac{m_{right}}{m}G_{right}
    $$
    - Where the $G_{left, right}$ is the Gini impurity at each child node, $m_{left, right}$ is the number of samples in each child node, and $m$ is the total number of samples in the node

- As we have shown in the example, you can train a classification tree using sklearn, using the `tree` library
    - The class corresponding to this algorithm is `sklearn.tree.DecisionTreeClassifier`
    - __`[RUN THE DEMO IN THE NOTEBOOK]`__
    - Let's use the same dataset again, but now we will tweak some hyperparameters to see how the tree changes
    - The first hyperparameter we will explore is the maximum depth of the tree, which is represented by the `max_depth` argument
        - This argument controls how many levels of the tree we will allow. Previously we saw that we had 2 levels of the tree, but now we will allow 5 levels
        - __`[RUN THE CELL]`__
        - Ok, so we have created the tree, let's take a look at it
        - __`[OPEN iris_tree_5.dot and CLICK ON THE THREE DOTS ON THE TOP RIGHT and SELECT OPEN PREVIEW TO THE SIDE]`__
        - This looks much more complex, but the rules we explained earlier still apply
        - Notice that now we have 5 levels and many more buckets
        - This might look good since, as you can see, each bucket is pure
        - But there might be a problem with decision being so specific. Exactly, we might be overfitting.
        - So be careful when you choose the maximum depth of the tree
        - You can add regularization to the tree, by setting lower values to `max_depth`
    - Another hyperparameter we will explore is the minimum number of samples required to split an internal node
        - This argument controls how many samples we need to have at least in a node to split it
        - __`[RUN THE CELL]`__
        - We set the argument min_samples_split to 10. Once again, we created the tree, let's see what it looks like
        - __`[OPEN iris_tree_10.dot and CLICK ON THE THREE DOTS ON THE TOP RIGHT and SELECT OPEN PREVIEW TO THE SIDE]`__
        - What happened here? Where are the five levels we had previously?
        - The classification tree tried to split the data into smaller buckets, but since we told it that each bucket should have at least 10 samples to be split, it was not able to split some of the buckets that still contain different classes
        - So, min_samples_split is a way to add some regularization to the tree.
    - The last hyperparameter we will explore is the minimum number of samples required to be at a leaf node
        - This argument controls how many samples we need to have at a leaf node to be able to make a prediction
        - A leaf node is a node that has no children
        - Let's set min_samples_leaf to 12
        - __`[RUN THE CELL]`__
        - Ok, so we have created the tree, let's take a look at it
        - __`[OPEN iris_tree_12.dot and CLICK ON THE THREE DOTS ON THE TOP RIGHT and SELECT OPEN PREVIEW TO THE SIDE]`__
        - Once again, the tree looks different. Notice that each leaf node has at least 12 samples. 
        - Some buckets are still not pure, but the decision tree can't split them because the children nodes would have less than 12 samples
        - So, similar to min_samples_split, we can add regularization to the tree by setting min_samples_leaf
        
- You can see that decision trees are very easy to interpret. 
    - If you look at the tree and the decision rules, you can see what variables are more impactful for splitting the classes
    - However, be careful, decision trees are prone to overfit, a slight change in the hyperparameters can lead to a tree that makes very specific decisions
    - Additionally, when you train the model, the training algorithm compares all features on all samples at each node. 
    - For small datasets, it might not be a problem, but the training complexity scales up with the number of samples and features
    - This means that training the model can be very slow for large datasets
    - Conversely, in the prediction step, each node only requires checking the value of one feature
    - So predictions are very fast, even when dealing with large training sets.