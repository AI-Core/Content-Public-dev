# KNN

- K-Nearest Neighbours, or KNN for short, is a supervised learning algorithm that works on the principle that similar things are close in proximity.
    - The KNN algorithm is a simple algorithm that doesn't actually train a model
    - When you create the model, in reality, you are just creating a space of points with your training data
    - Then, when you want to predict a label, KNN selects the K nearest neighbours
        - Proximity is measured by some metric, for example, euclidean distance
    - Then you can calculate the average of their predictions (or most voted in case of classification) to obtain the prediction
    - When we talk about most voted, we are talking about the number of times a label appears within the K nearest neighbours
    - Let's see a graphical example of KNN
    - <p align=center><img src=.images/knn.png width=400></p>
    - In this example, when we "trained" the model, we simply put each training data point in the graph
    - Then, we want to predict the label of the yellow point
    - And we find the K nearest neighbours of the yellow point. But how do we find the value of K?
    - K is a hyperparameter that we have to set. In the example, if we set it to three neighbours, we see that KNN would predict the label of the yellow point as class B, because there are 2 points in class B and 1 point in class A
    - But if we set it to seven neighbours, we see that KNN would predict the label of the yellow point as class A, because there are 4 points in class A and 3 points in class B

- So, we don't have to train the model to make predictions
    - And as such, KNN has no parameters to learn, everything is manually set
    - Also, when building the model, we are placing our dataset in the metric space, so it relies on all the data to calculate the distances. 
    - This means, in terms of memory, the model is not very efficient
    - We have to be vareful with K, we can easily ovefit the model

- __`[RUN a demo on how to calculate distances in Python (Manhattan and Euclidian) using scipy]`__
    - As mentioned, KNN is computed using the distances between the point to be predicted and the training data
    - Luckily, scipy has a function that calculates the distance between each pair of the two set of inputs.
    - Let's say that we have two points, [3, 4] and [6, 8]
    - And we want to calculate the distance between [0, 0] and each of the two points
    - Scipy allows us to calculate different type of distances. Let's say we want to calculate the Manhattan distance and the Euclidian distance
    - __`[RUN THE CELL]`__
    - We can see that the Manhattan distance is the sum of the absolute values of the differences between the two points
    - And that the Euclidian distance is the square root of the sum of the squared differences between the two points
    - We can use these distances to calculate the K nearest neighbours

- __`[Implement the custom KNN class]`__
    - Let's take a look at the dataset we are using to train the model
    - __`[RUN THE CELL]`__
    - We can see that we have 6 different classes, and each class is clustered in a different area
    - Now, we code the KNN class, with only two methods
        - 'fit' basically creates the model putting the training data in the metric space
        - 'predict' is the method that we use to calculate the distances between the point to be predicted and the training data
            - Then, it computes the K nearest neighbours, and checks the most voted label
    - __`[RUN THE CELL]`__
    - Let's predict a couple of points: [1, 2] and [3, 4]
    - __`[RUN THE CELL]`__
    - We can see that KNN predicts that the first point is in class 1, and the second point is in class 0
    - Let's check that graphically
    - __`[RUN THE CELL]`__
    - In the graph, we can see that point [1, 2] is close to cluster 1, and 1 point is close to cluster 0

- What we have seen so far computes the distance to the K nearest neighbours and then checks the frequency of each label
    - However, it only counts the frequency of the label, without taking into account the distance
    - This is called __`unweighted`__ or __`hard voting`__
    - It would be a good idea to use weighted voting, where we take into account the distance between the point to be predicted and the training data
    - __`[RUN THE CELL]`__
    - We have a similar class, but in this case, we are also computing the weights as the inverse of the distance
    - So the closer the point is to the training data, the higher the weight
    - And we can see that the predictions work well

- KNN is a fairly simple algorithm, but it has some limitations
    - It needs to compute the distance between the samples to be predicted, so it has a high time complexity
    - High space complexity when we need to make predictions, since we need to store the whole dataset
    - It might be hard picking the right K
    - It highly depends on the distances, but scaling the dataset can affect determining who is the nearest neighbour, so we might need to try it with and without scaling