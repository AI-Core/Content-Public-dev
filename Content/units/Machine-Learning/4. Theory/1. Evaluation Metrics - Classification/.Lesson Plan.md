## What 


## Classification metrics

### Accuracy

### False positive rate
- What fraction of negative examples are classified as positive
- when the label is negative, how often does the model predict positive

### False negative rate
- What fractions of positive examples are classified as negative
- when the label is positive, how often does the model predict negative

### Precision
- When the model predicts a label as positive, what proportion of the time is the label positive?
- gets worse with false positives

### Recall
- What proportion of the true labels does the model correctly predict?
- gets worse with false negatives

### Comprehension
- false positives, false negatives, how do they each affect precision and recall?
    - precision reduced by false positives
    - recall reduced by false negatives
    - each is not affected by the other
- which metric would you want to prioritise in the case of spam email filtering?
    - you dont want false positives
        - email from family or friend marked as spam
    - cost of false negatives is low
    - so you want to maximise precision
- what about for 
- what about in cancer diagnosis?
    - you dont want false negatives
        - deadly disease goes unnoticed
    - so maximise recall
    - what about precision?
        - but you also don't want to have many healthy people undergoing lengthy, uncomfortable diagnoses
        - so precision also should be high?

### F-1 Score
- harmonic mean of precision and recall
    - harmonic mean is the mean of some rates when the denominator is not constant
    - i.e. when you do some things at certain rates, but you don't do them for the same
    - e.g. 
- hence accounts for both
- gives equal weighting to each, hence the 1
- the F-beta score can be used to weight each

### ROC Curves
- how does a classification model's performance change as we change the classification threshold
- sweep over the thresholds and plot them on a graph of false negative rate vs false positive rate
- can also be plotted on a precision-recall graph
- can be plotted for different models to compare how they perform in different operating points

### AUC
- Turn ROC curve into a single metric
- Rough way to compare different models based on their ROC curves
- Doesn't allow you to understand performance based on context of precision and recall

### Operating points
- Problem context tells you what acceptable thresholds for false negatives or false positives are

### Comprehension