# Evaluation Metrics

- __`[Explain what evaluation metrics are]`__
    - They measure the performance of the model
    - They give us a baseline to know if the model could be improved
    - We need them to ensure that the model is not underperforming

- While we can use loss, this metric is not very human interpretably
    - Accuracy, on the other hand, gives a good perspective of how the model is performing

- __`[Explain accuracy]`__
    - One way to measure the performance of a model is to calculate the accuracy
    - It is the percentage of correct predictions over the total number of predictions
    - As mentioned, is very easy to interpret
    - The problem is that it doesn't take into account the empirical distribution
        - So we can have a high accuracy just by assuming everything belongs to a label

## Classification metrics

- A much better way to evaluate the performance of a classifier is to look at the confusion matrix
    - The confusion matrix is a table that shows the number of correct and incorrect predictions
    - <p align=center><img src=.images/confusion_matrix.png width=600></p>
    - If the predicted label is positive and the actual label is positive, the outcome is a true positive
    - A true negative is an outcome where the model correctly predicts the negative class.
    - A false positive is an outcome where the model incorrectly predicts the positive class
    - And a false negative is an outcome where the model incorrectly predicts the negative class.
    - Depending on the case, we might need want to prioritize false positives or false negatives
    - So, we can tweak the model to move that balance towards a type or error
    - Some metrics that can help us with that task are the Precision and the Recall

- Confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. 
- An interesting one is called precision
    - It is the ratio between the True Positive over the total number of predicted positives
    - So, from all the predicted TRUEs, how many are correct?

- Precision is typically used along with another metric named recall
    - or also known as sensitivity
    - It is the ratio between the true positive over the total number of actual positives
    - In other words, from all the true examples, how many did our model predicted?

- It is often convenient to combine precision and recall into a single metric called the F1 score
    - Useful if we want to keep balance between Precision and Recall
    - It is given by the harmonic mean of our precision and recall

- __`[RUN A DEMO ON OBTAINING THE CONFUSION MATRIX] Run the demo on the notebook`__
    - Obtaining the confusion matrix using sklearn is quite easy
    - We can use the confusion_matrix function from sklearn.metrics
    - Then, we need to pass the predicted labels and the actual labels
    - __`[RUN THE CELL]`__
    - And we obtain an array. 
    - In this example, we are predicting whether a patient has cancer or not
    - So in the confusion matrix we have:
        - 52 true negatives
        - 1 false positive
        - 6 false negatives
        - And 84 true positives
    - So the accuracy is 84 + 52 / (84 + 52 + 6 + 1) = 0.951
    - The precision is 84 / (84 + 1) = 0.99
    - The recall is 84 / (84 + 6) = 0.93
    - And the F1 score is 2 * 0.99 * 0.93 / (0.99 + 0.93) = 0.947
    - As mentioned earlier, we might want to prioritize false positives or false negatives, so we would like to prioritize the recall or the precision
    - For example, if you trained a classifier to detect videos that are safe for kids, you would probably prefer a classifier that rejects many good videos (low recall) but keeps only safe ones (high precision)
    - Instead of a classifier that has a much higher recall but lets a few really bad videos show up in your product 
    - To give more weight to a certain metric, we can set a threshold for the model.
    - This way, we can tell the model the level of confidence we want to give to a certain prediction

- For each threshold, we obtain a different precision and recall
    - When training a classification model, we are finding the optimal threshold for a certain metric, such as accuracy
    - But optimality can vary depending on the problem
    - So we can use the ROC curve that is plotted using recall and (1 - specifity) where specifity is the ratio of the predicted negative labels over how many actual negative labels we had in the data
    - Each point in the ROC curve is the value of the FPR and the recall for a given threshold
        - So we have a visual way to check the FPR and the TPR and their corresponding threshold
        - That way we can change it according to the problem we are tackling
    - We can use this curve to measure the overall performance of the model by calculating the area under the curve or AUC
    - The closer to one, the better
    - <p><img src=.images/roc.png></p>

## Regression metrics

- In regression we can't simply assume that a prediction is incorrect
    - We need to measure how similar to the actual value our prediction is

- Some of the most popular metrics are Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE)
    - MSE ensures that the sign is always positive by adding the square of each error
    - However, it returns the square of the original units, so we might want to obtain the root square of the previous operation. (RMSE)
    - We can also measure the error with the MAE
        - It is more intuitive and it can be interpreted easily
        - But MAE doesn't penalize large errors as MSE does

- Another metric that can be used in regression is the R2 score
    - It is a measure of how much variance of the data is actually explained by our model
    - Write and explain the equation based on the definition of R2:
    $$
    R^{2} = 1 - \frac{MSE}{Var(y)} = 1 - \frac{\sum_{i=1}^{N}(y - \hat{y})^{2}}{\sum_{i=1}^{N}(y - \bar{y})^{2}}
    $$

- In sklearn, we can compute these metrics using different libraries
- __`[RUN A DEMO ON HOW TO COMPUTE MSE, RMSE, MAE AND R2]`__
    - Let's use the California housing dataset to train a model and compute these metrics
    - We will use the LinearRegression model from sklearn
    - We can use the mean_squared_error, mean_absolute_error, and r2_score functions from sklearn.metrics
    - The mean_squared_error function has a parameter called squared, which if set to False, returns the RMSE
    - So, we can see that the outcome of the model doesn't look very good
    - Let's change the model to a more powerful model, RandomForestRegressor
    - We compute the same metrics, and now the outcome is much better!