questions:
  - question: What is the difference between AdaBoosting and Gradient Boosting?
    answer: Instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor.
  - question: Gradient Boosting usually implements what type of weak learner?
  - question: How does Gradient Boosting differ from Gradient Descent?
  - question: What is the pseudo-residual value?
    answer: Is the error between the actual value and the predicted value computed by an intermediate model while adding estimators to the gradient boosting
  - question: How does Gradient Boosting choose the value of the first estimator?
    answer: We can use maximum likelihood estimation to calculate the constant value that makes the distribution of the data to be as similar to said constant value. That value is the mean value of the dataset
  - question: Name three different hyperparameters in the Gradient Boosting method
    answer: Learning rate, max_depth (which corresponds to the max depth of the decision tree), and the number of estimators
  - question: Can you use Gradient Boosting for both classification and regression?
  - question: What parameters can you tweak in sklearn's Gradient Boosting?
    answer: If your AdaBoost ensemble underfits the training data, you can try increasing the number of estimators or reducing the regularization hyperparameters of the base estimator. You may also try slightly increasing the learning rate.
  - question: If your Gradient Boosting ensemble overfits, what hyperparameters should you tweak and how?
    answer: If your Gradient Boosting ensemble overfits the training set, you should try decreasing the learning rate. You could also use early stopping to find the right number of predictors (you probably have too many).
  - question: The usual estimator in Gradient Boosting is Decision Tree. Is it the same then as using Random Forest?
    answer: No, Random Forest uses Decision Trees as well, but RF uses Bagging 
  