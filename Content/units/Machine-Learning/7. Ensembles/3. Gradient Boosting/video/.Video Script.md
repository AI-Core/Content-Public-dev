# Gradient Boosting

- Another very popular boosting algorithm is gradient boosting.
    - Gradient boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor
    - It usually forms an ensemble of decision trees
    - However, whereas AdaBoost tweaks the weight of each sample
    - Gradient boosting tries to fit the new predictor to the residual errors made by the previous predictor.
    - The models' predictions are all combined with an equal weight, which is called the learning rate

- So the idea is to add a new predictor to the ensemble, and adjust the weights of the previous predictors to make the new predictor fit the residual errors.
    - So, the first predictor is trained and the errors we obtained are used to train a new model 
    - The first model is updated using a model that was trained using the errors 
    - This creates a new model that fits the residual errors.
    - The new model generates more errors, and a new model is trained using the new errors.
    - This process is repeated until a certain criteria is met.

- Okay, let's see a practical example
    - __`[RUN THE MODEL IN THE NOTEBOOK]`__
    - Let's generate some data samples. In this case we will simulate a quadratic function with some noise
    - Additionally, in this cell we also define a helper function that will help us plot the data
    - __`[RUN THE FIRST CELL]`__
    - Let's start with a simple decision tree
    - We use this model to make predictions. 
    - __`[RUN THE SECOND CELL]`__
    - As we can see the model has a high bias, and we can calculate the error as the difference between the predicted value and the actual value
    - __`[RUN THE THIRD CELL]`__
    - Now, we train a second decision tree using the error made by the first tree
    - We can use these predictions to update the ensemble model
    - __`[RUN THE FOURTH CELL]`__
    - Now, notice that the predictions of the second tree are the addition of the predictions of the first tree plus the predictions of the model trained with the previous errors
    - Once again, the new model presents errors that we can use to train a new decision tree
    - __`[RUN THE FIFTH CELL]`__
    - We obtain a new model for these errors
    - And we can use the predictions to add them to the previous ensemble
    - __`[RUN THE SIXTH CELL]`__
    - Now, we can see that the ensemble model is starting to perform better than the original model
    - So, we can see how gradient boosting works, each iteration learns a new model based on errors, so the model can learn from previous mistakes

- We can perform the same operation using the GradientBoostingRegressor from sklearn
    - __`[RUN THE SECOND PART OF THE NOTEBOOK] Using Gradient Boosting with sklearn`__
    - In sklearn, you can use the GradientBoostingRegressor class to train a model that uses gradient boosting
    - One of the hyperparameters that we haven't used in the previous example was the learning rate
    - This parameter controls how much the model updates the weights of the previous predictors
    - But we haven't multiplied any of the errors by a factor, so the learning rate we used was 1
    - So we create a new GradientBoostingRegressor with a learning rate of 1, using 3 estimators, and a max depth of 2 to simulate the same configuration as the previous example
    - __`[RUN THE NEXT CELL] `__
    - Compare this graph with the previous one
    - The prediction is the same! Meaning that GradientBoosting in sklearn is doing the same procedure we did in the previous example
    - You can set different hyperparameters to change the configuration of the model
    - For example, let's try a learning rate of 0.1
    - __`[RUN THE NEXT CELL]`__
    - The prediction is much worse now. This is because the learning rate is too low
    - Perhaps we can try a learning rate of 2
    - __`[RUN THE NEXT CELL]`__
    - Wow, the new model takes too much into account the errors made by the previous model, so it easily explodes
    - Ok, so it looks like a learning rate of 1 was fine
    - We can also change different hyperparameters to regularize the model
    - For example, let's set the number of estimators to 100
    - __`[RUN THE NEXT CELL]`__
    - You can see that the prediction doesn't seem to generalize very well
    - So we can reduce the number of estimators to reduce overfitting
    - Another hyperparameter that can be tweaked is the max depth of the tree
    - Let's try a max depth of 15
    - __`[RUN THE NEXT CELL]`__
    - Once again, if the max depth is too high, the model will seem to learn the noise
    - So, to summarize, you can regularize the model decreasing the number of estimators or the max depth