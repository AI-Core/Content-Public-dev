# Random Forest and Bagging

- Machine learning models give a prediction for a new data point based on the training data.
    - But what if we train multiple models on the same data and then combine the predictions?
    - Suppose you ask a complex question to thousands of random people, then aggregate their answers.
    - In many cases you will find that this aggregated answer is better than an expert’s answer. 
    - This is called the wisdom of the crowd. 
    - Similarly, if you aggregate the predictions of a group of predictors you will often get better predictions than with the best individual predictor. 
    - A group of predictors is called an ensemble
    - So this technique is called Ensemble Learning

- Ensemble learning is a techinque that combines multiple models to create a better model.
    - The idea is that each model has a vote on the final decision.
    - The final predictor aggregates the predictions of each classifier and predict the class that gets the most votes
    - For example, in the next figure, the model will predict that the new sample belongs to class 1
    - <p align=center><img src=.images/bagging_2.png></p>
    - This is called hard voting.
    - As opposed to hard voting, soft voting is when the models vote based on the average of each classifier’s prediction.
    - Let's see an example of ensemble learning
- __`[RUN THE DEMO IN THE NOTEBOOK]`__
    - __`[RUN THE CELL]`__
    - This model uses the moon dataset and splits the data into training and test sets.
    - It also trains three different models
        - Logistic Regression
        - Decision Tree Classifier
        - K-Nearest Neighbors
    - __`[RUN THE CELL]`__
    - sklearn has a class called voting classifier that can be used to combine multiple models
    - This class accepts a list of models and a voting method, in this case, let's pass the voting method as "hard"
    - __`[RUN THE CELL]`__
    - Let's train each model and the ensemble model to see the accuracy of each model
    - __`[RUN THE CELL]`__
    - Look at that! The ensemble model has a higher accuracy than the individual models.

- However, this is only true if all classifiers are perfectly independent, making uncorrelated errors
    - But this is not the case since they are trained on the same data. 
    - They are likely to make the same types of errors
    - So there will be many majority votes for the wrong class
    - Reducing the ensemble’s accuracy.

- One solution is to use very different training algorithms, just as we did in the example
    - But that approach can be time consuming and tedious

- Another approach is to use the same training algorithm but to train them on different random subsets of the training set. 
    - When sampling is performed with replacement, this method is called bagging (short for bootstrap aggregating)
    - When sampling is performed without replacement, it is called pasting

- The following figure shows the concept of Bagging:
    - <p align=center><img src=.images/bagging.png></p>
    - The training set is bootstrapped into different subsets, and each subset is used to train a different model.
    - Then, the results are aggregated into a single model
    - So we can use it on the test set to see how well it performs.

- Another scikit class you can use is BaggingClassifier
    - This class ensembles different estimators of the same type and provides a way to control the number of estimators that are used.
    - Let's see an example using BaggingClassifier with Decision Trees
    - __`[RUN THE DEMO]`__
    - In this cell we are going to train a simple decision tree classifier with the same dataset as before
    - __`[RUN THE CELL]`__
    - We can see that we have an accuracy of around 85.6%
    - Let's see in the next cell the accuracy of the bagging classifier
    - __`[RUN THE CELL]`__
    - We can see that the bagging classifier has an accuracy of around 92%
    - That's a great improvement!
    - Let's plot the decision boundaries of both models
    - Don't worry about the details in this code, this function is a helper to represent the decision boundaries
    - __`[RUN THE CELL]`__
    - Let's plot the graphs
    - __`[RUN THE CELL]`__
    - As you can see, the ensemble model generalizes much better than the decision tree model
    - So the ensemble has a comparable bias but a smaller variance

- We have just saw an ensemble that uses decision trees
    - This type of ensemble is so popular that they constitute the algorithm called Random Forest
    - So, a random forest is an ensemble of simple decision trees
    - By reducing the complexity of each decision tree, we reduce its variance
    - And by ensembling their decisions, we reduce the bias 
    - They are trained over the bagging method

- __`[RUN THE TWO FINAL CELLS ON THE NOTEBOOK]`__
    - As you can see here, we are using the RandomForestClassifier from sklearn.ensemble
    - We are using the same hyperparameters as we did earlier with the bagging classifier
    - __`[RUN THE CELL]`__
    - And as you can see, we obtain the same accuracy as before
    - So, to reiterate, random forest is a bagging classifier that uses decision trees
    - An additional advantage of random forest is that they make it easy to measure the relative importance of each feature
    - sklearn measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average
    - You can access the result using the `feature_importances_` variable
    - Let's check a different example using the breast cancer dataset
    - We use this dataset to train a new model
    - __`[RUN THE CELL]`__
    - From the model, we can extract the feature importances
    - __`[RUN THE CELL]`__
    - But we just get a bunch of numbers...
    - Each number corresponds to the relative importance of a feature
    - So let's associate each feature with its importance
    - __`[RUN THE CELL]`__
    - We sort the list by importance
    - And we can see that the most important feature is the "worst concave points", and the least important in the "concave point errors"
    - YOu might want to use this tool to remove features that are not important
    - That way you can reduce the noise in the data and reduce overfitting 