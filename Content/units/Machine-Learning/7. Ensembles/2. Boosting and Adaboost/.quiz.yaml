name: Boosting and Adaboost
questions:
  - question: Which of the following is true about boosting methods?
    options:
      - option: In boosting methods, individual base algorithms are independent of each other
        correct: false
      - option: In boosting methods, the performance is improved after aggregating the result of weak learners
        correct: true
      - option: Increasing the number of estimators may overfit the model
        correct: true
      - option: Increasing the number of estimators may underfit the model
        correct: false
      - option: Each base algorithm learns from the errors of the previous base algorithms
        correct: true
      - option: Each base algorithm learns independently
        correct: false
  - question: Select everything true about Adaboost
    options:
      - option: Adaboost is a boosting method
        correct: true
      - option: Adaboost is a bagging method
        correct: false
      - option: Adaboost can be considered both a boosting and a bagging method
        correct: false
      - option: Adaboost has an hyperparameter that controls the weight of the base algorithms
        correct: true
      - option: Adaboost has an hyperparameter that controls the maximum error available for each base algorithm
        correct: false
      - option: AdaBoost is best used to boost the performance of decision trees on binary classification problems
        correct: true
  - question: Each base algorithm is weighted by a factor. How is the weight of each base algorithm determined?
    options:
      - option: From the error of the previous base algorithms
        correct: true
      - option: From the error of the previous base algorithms and the number of estimators
        correct: false
      - option: From the number of estimators
        correct: false
      - option: From the error of the previous base algorithms and the number of training instances
        correct: true
  - question: Which of the following are boosting methods?
    options:
      - option: AdaBoost
        correct: true
      - option: Gradient Boosting
        correct: false
      - option: Random Forest
        correct: false
      - option: Bagging
        correct: false
      - option: Neural Network
        correct: false
      - option: XGBoost
        correct: true
  - question: Which of the following are parameters of the AdaBoostClassifier in sklearn?
    options:
      - option: n_estimators
        correct: true
      - option: learning_rate
        correct: true
      - option: algorithm
        correct: true
      - option: n_jobs
        correct: false
      - option: base_estimator
        correct: true
      - option: random_state
        correct: true