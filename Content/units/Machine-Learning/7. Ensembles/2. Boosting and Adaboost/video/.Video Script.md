# Boosting and Adaboost

- Ensemble methods are used to combine weak learners to produce a strong learner.
    - There are different type of ensemble methods, for example bagging, and boosting
    - As opposed to bagging, which trains weak learners in parallel    
    - The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. 
- The next image shows the concept of boosting.
    - <p align=center><img src=.images/boosting.png></p>
    - So, as opposed to bagging, boosting doesn't train the models in parallel and then aggregates the results (hence the name bootstrap aggregating), but it rather trains the model taking into account the predecessor mistakes

- One of the most popular boosting methods is AdaBoost, short for Adaptive Boosting
    - It pays more attention to the training instances that the predecessor underfitted. 
    - This results in new predictors focusing more and more on the hard cases.
    - This sequential learning technique has some similarities with Gradient Descent
    - One important drawback to this sequential learning technique is that it cannot be parallelized 
    - As a result, it does not scale as well as bagging 

- Explain how it works:
    - A first predictor is trained and its weighted error rate r is computed on the training set
    - <p align=center><img src=.images/error.png></p>
    - Each sample's weight is initially 1/m, where m is the number of samples
    - Then, we can calculate the weight of that predictor
    - <p align=center><img src=.images/weight.png></p>
    - where eta is the learning rate
    - The more accurate the predictor is, the higher its weight will be
    - And we update the weights, boosting those samples that were misclassified
    - <p align=center><img src=.images/update_rule.png></p>
    - After this, the weights are normalized
    - The process is repeated with the updated weights until a certain criteria is met
    - To make predictions, AdaBoost computes the predictions of all the learners and weights them using alpha
    - The predicted class is the one that receives the majority of weighted votes
    - This means that when the model correctly classifies a sample, the weight of that sample is decreased, so the next learner doesn't focus too much on that sample
    - We also need to weight the learners, because otherwise, the correctly classified samples will decrease their weights to a point where they are almost zero. 
    - Increasing the weight of the learners that correctly predicted those samples can counter the decrease of the sample weight. 

- Let's see a brief demo to see what it looks like
    - __`[RUN THE FIRST PART OF THE NOTEBOOK]`__
    - Don't worry about this first cell, it is a helper function to plot the decision boundaries of the model
    - In the next cell we can see that we are loading the moon dataset
    - We are going to use support vector machines to train the model
    - And at each iteration, we are updating the weights of the samples
    - The weights that are updated are the ones that were misclassified, and we are updating them by multiplying them by the learning rate
    - In this example, we do it for 5 iterations
    - And after running the cell, we can see the decision boundaries of the model at each iteration
    - __`[RUN THE SECOND CELL]`__
    - We can see that at each iteration, the model is learning from the mistakes made by the previous model
    - For example, the first model looks like it is misclassifying the many examples because it looks almost linear
    - But the second model knows the mistakes made by the first model and it is able to learn from them
    - However, it is still making mistakes
    - And the third model is able to learn from the mistakes made by the second model, and so on and so forth

- The learning rate we used was 0.8, let's see what happens if we change it
    - If we set the learning rate to 0.1, we can see that the model is barely changing at all
    - __`[RUN THE THIRD CELL]`__
    - On the other hand if we set the learning rate to 1.5, we can see that the model is exploding after a few iterations because it is "exaggerating" the mistakes made by the previous model
    - __`[RUN THE FOURTH CELL]`__
    - So, the learning rate is a hyperparameter that needs to be tuned carefully

- This method we have seen just now can be implemented by a sklearn class called AdaBoostClassifier
    - The default learner is a decision tree
    - You can set different hyperparameters, such as the number of estimators, learning rate, and the base estimator
    - __`[RUN THE FIFTH CELL]`__
    - In this case, it might look like the model might be overfitting, because the boundaries seem to be very specific
    - If your model overfits, you can decrease the number of estimators, or use a simpler base estimator instead of decision trees
    - __`[RUN THE SIXTH CELL]`__
    - In this case, we reduced the number of estimators to 20, and the boundaries are much more general