## Intro to torch for auto differentiation
- the torch tensor data type
- `.backward()`
- `requires_grad`
- `grad_fn`
    - defines what function it was created from and the inputs that were passed to it
- `.grad`

## 10 min break

## Challenge 1

## Demo Hello World Model
- show how linear regression model would be implemented in pytorch
- create a class
- inherit from torch.nn.Module
    - what is torch.nn.module?
        - allows you to do .parameters() to get a generator of parameters of the model
        - does some other useful things
        - 
    - init the parent class using super
    - initially naively implement the forward pass as a y_hat = xw + b
    - then replace that with self.linear_layer = torch.nn.Linear
    - show the torch.nn docs
        - show some example layers
        - all layers inherit from torch.nn.Module
        - so your model is basically another layer in itself
        - that is, it just represents a transformation of some data, like the layers do
        - later we'll build our own layers
    - define the __call__ method
    - then define the forward method
        - overwrites the __call__ method and does some other useful things
            - 
        - important:
            - as is convention, the first dimension of any data passed forward through a pytorch model represents the batch dimension
            - that is, it will have the size of the number of examples in that batch
    
## Challenge 2: Implement your own linear regression model

## Group activity: check off the checklist of things your model should implement
- inherits from torch.nn.Module
- initialises the parent class using super
- implements the forward method
- has linear layer from torch.nn.Module

## Demo a training loop and introduce optimisation
- lets firstly implement a naive full batch gradient descent in pytorch
- we're missing an optimiser!