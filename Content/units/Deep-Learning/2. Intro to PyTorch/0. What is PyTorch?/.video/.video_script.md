# Pytorch

- open .video script
- open .code.ipynb
- open .slides.pptx
- install torch
  
## Intro

- Open source machine learning framework that accelerates the path from reaseach to prototyping to production deployment
- Used for neural network, uses hardware acceleration, based on `numpy`.

## Data Types

- Uses multiple datatypes for instance floats, doubles and halfs.

## Casting

__`[SHOW IN VS CODE]`__ Casting various torch objects to different datatypes

## Device

- Pytorch can use multiple device types
  - CPU for data loading 
  - specialized devices, GPU, TPU for running data through the neural network.
  
__`[SHOW IN VS CODE]`__ How to check GPU available and line to initialize GPU if available

## Automatic differentiation

__`[SHOW ON SLIDES]`__ Differentiation graph also known as a tape

- Can calculate gradients of loss w.r.t parameters, we need to use `requires_grad=True` for when creating tensors, most of pytorchs tensors have this parameter.

__`[SHOW IN VS CODE]`__ creating tensors with `requires_grad` and that we can use them to calculate a sum.

## Back propagation
- We run backpropagation on a tensor and explicitly call it.
  
__`[SHOW IN VS CODE]`__ Explicitly running `loss.backward()`

## grad_fn, how pytorch keeps track of operations

- Pytorch keeps functions which created the tensor in `grad_fn`
- If `grad_fn` is `None` it's a tensor created by the user with `requires_grad` set to `True` or `False`

__`[SHOW IN VS CODE]`__ Printing out `grad_fn` for the tensors 

## Modules

- Torch basically acts like a Numpy high level namespace
- For a full list of PyTorch functions refer to the documentation 

__`[SHOW IN BROWSER]`__  Pytorch documentation https://pytorch.org/docs/stable/nn.html

__`[SHOW IN VS CODE]`__ Examples of various Torch functions being used.

## import torch.nn as nn

- Contains neural network related modules
- All layers are presented as classes which you instantiate
- Layers can be mixed together and depend on `torch.nn.Module` they're all instances of it.

__`[SHOW IN VS CODE]`__ Examples of creating layers

## torch.nn.functional as F

- Pytorchs neural network components provided as functions instead of objects
- `torch.nn` produces more readable code

__`[SHOW IN VS CODE]`__ Example of `torch.nn` vs `torch.nn.functional` 

### Pros of torch.nn

- You can create stateful objects
- implicitly creates the weights with correct initialization
- It's easier to compose and separate the steps

### Pros of torch.nn.functional

- Good for none-stateful objects(softmax has no attributes or weights)
- Directly applicable
- Useful when you only need to use a function once.

__`[SHOW IN VS CODE]`__ Example of using `torch.nn` vs `torch.nn.functional` for softmax

## torch.nn.Module

- base class for every deep learning model in Pytorch (usually neural networks)
- We inherit from it when creating a more complicated module

__`[SHOW IN VS CODE]`__ Example of coding up a linear regression model inheriting from `torch.nn.Module`

## torch.nn.Parameter

- can be used if we want a tensor to be a part of `nn.Module` we wrap it in `nn.Parameter`

__`[SHOW IN VS CODE]`__ Example of printing out all parameters 

  - Here `self.other_tensor` is not named as a parameter so it won't be able to be easily optimized and is just an attribute.

## forward method

- Users should implement the logic of how data passes through the neural network inside this method. 
- When running data through the model we use the `__call__` method to ensure any hooks are registered for the module to run correctly.


__`[SHOW IN VS CODE]`__ calling the model 

  - Pytorch requires `(batch_size, n_features1, ..., n_features2) tensors as input.
  - In the example above show batch size = 64, and 15 input features to the model

### Outro






