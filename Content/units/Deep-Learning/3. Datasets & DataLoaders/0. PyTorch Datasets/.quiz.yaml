name: "Deep Learning: PyTorch Datasets and Dataloaders"
questions:
  - options:
      - correct: false
        option: pytorch.data
      - correct: false
        option: pytorch.utils.data
      - correct: true
        option: torch.utils.data
      - correct: false
        option: torch.data
    question: Where can we find PyTorch's data functionality?
  - options:
      - correct: false
        option: \_\_init\_\_, \_\_len\_\_
      - correct: true
        option: \_\_len\_\_, \_\_getitem\_\_
      - correct: false
        option: \_\_init\_\_, \_\_len\_\_, \_\_getitem\_\_
      - correct: false
        option: \_\_getitem\_\_
      - correct: false
        option: \_\_init\_\_
    question: What methods do we HAVE TO define for map style PyTorch Datasets so it works correctly with DataLoader?
  - options:
      - correct: true
        option: They allow us to use batches created from provided Dataset
      - correct: true
        option: Recommended way to iterate over data in PyTorch
      - correct: false
        option: Can save RAM memory as they only implement \_\_getitem\_\_
      - correct: false
        option: We can access data using indexing (\_\_getitem\_\_) through it
      - correct: true
        option: Makes it easy for us to use traditionally used features in deep learning (shuffling, batches)
      - correct: false
        option: Automatically casts data to device we want (e.g. to GPU)
      - correct: true
        option: Can speed up memory transfers to GPU using `pin\_memory`
      - correct: false
        option: Works only with `torch.utils.data.Dataset` instances
      - correct: true
        option: Works with `torch.utils.data.IterableDataset` instances
      - correct: false
        option: Can use arbitrarily many threads to load data
    question: Mark everything True about DataLoaders
  # - options:
  #     - correct: true
  #       option: High level wrapper over PyTorch
  #     - correct: true
  #       option: Provides many training related utilities out of the box
  #     - correct: false
  #       option: Does not provide metrics
  #     - correct: true
  #       option: Provides callbacks
  #     - correct: true
  #       option: Structures code into self-contained classes
  #     - correct: true
  #       option: Allows us to standardize process of creating neural networks
  #     - correct: false
  #       option: We should us it always and forget about PyTorch low level API
  #   question: Mark everything True about PyTorch Lightning
  # - options:
  #     - correct: true
  #       option: Inherits from torch.nn.Module and can be mixed with it freely
  #     - correct: false
  #       option: Is a standalone class which ALWAYS wraps the whole training
  #     - correct: true
  #       option: Allows us to define the whole training procedure
  #     - correct: true
  #       option: Allows us to define neural networks (even multiple ones)
  #     - correct: true
  #       option: Can be fed to Trainer class to remove boilerplate code
  #     - correct: true
  #       option: Handles details like moving data/modules to device for us
  #   question: Mark everything True about pl.LightningModule
  # - options:
  #     - correct: true
  #       option: Specified default behavior
  #     - correct: false
  #       option: Does not change it's default behavior based on method where it is
  #         called from
  #     - correct: true
  #       option: Allows us to log data from multiple points in training (epoch, step)
  #     - correct: false
  #       option: Can only log loss value
  #     - correct: false
  #       option: Can log any value
  #     - correct: true
  #       option: Can log to multiple loggers
  #     - correct: false
  #       option: By default logs everything to stdout
  #   question: Mark everything True about PyTorch Lightning log method (from LightningModule)
  - options:
      - correct: true
        option: Higher level abstraction over torch.utils.data.Dataset
      - correct: true
        option: Takes a dataset as an input and packs the data into batches.
      - correct: false
        option: Optimizes weights of neural network
      - correct: false
        option: Usually runs on a scalar variable (implicitly fed with `1` value as dL/dL = 1 where L is loss)
      - correct: false
        option: Can be run on non-scalar variables without any intervention
      - correct: false
        option: Can be run on non-scalar variables with appropriate tensor fed to `torch.autograd.backward` function
    question: Mark everything True about DataModule
  # - options:
  #     - correct: true
  #       option: Runs LightningModule (neural network and specified training) on data
  #     - correct: true
  #       option: Allows us to specify training properties (number of GPUs, mixed precision
  #         training etc.)
  #     - correct: true
  #       option: Can be used with Lightning's DataModule instances
  #     - correct: false
  #       option: Has to be used with Lightning's DataModule instances
  #     - correct: false
  #       option: Is provided via external library called "bolts"
  #     - correct: true
  #       option: We should use `.fit` method in order to run it
  #     - correct: false
  #       option: We should use `.train` method in order to train on it
  #     - correct: true
  #       option: We should use `.test` method in order to run the code on test data
  #   question: Mark everything True about PyTorch Lightning's Trainer
  # - options:
  #     - correct: true
  #       option: Contributed by the community, tested by PyTorch Lightning team
  #     - correct: true
  #       option: Provide off the shelf DataModules & Neural network models
  #     - correct: true
  #       option: Provides utilities like new loss functions or callbacks
  #     - correct: false
  #       option: Cannot be used independently from PyTorch Lightning
  #   question: Mark everything True about PyTorch Lightning Bolts
