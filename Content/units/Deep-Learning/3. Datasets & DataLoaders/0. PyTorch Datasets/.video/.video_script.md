# Datasets and DataLoaders

- Pytorch provides a way to create your own data via `torch.utils.data` module. 
  - allows custom datasets
  - quickly load data in batches using multiprocessing
  - pin data to GPU memeory for faster transfers

## Map-style datasets

- Those are the most common datasets and have a really simple structure
  - Create a class by inheriting from `torch.utils.data.Dataset`
  - Override `__init__` optionally if needed
  - Define get item method returns a single sample
  - len method returns how many samples total

__`[SHOW IN VS CODE]`__ Example of a loading a dataset class structure

## DataLoaders 

- In deep learning we use batches of data, as much as we can fit into GPU
- Pytorch provides `torch.utils.data.DataLoader`
- Dataloader batches the data for us to be consumed by neural networks.

### Automated batching

__`[SHOW IN VS CODE]`__  Example of auto batching

- `torch.utils.data.Dataloader` has a lot of arguments all of them optional apart from `Dataset` 

__`[SHOW IN BROWSER]`__  https://pytorch.org/docs/stable/data.html explain some arguments

### num_workers

- `num_workers` specifies how many processes in parallel will be used to load data
- Not enough workers 
  - Pipeline will wait for data while neural network is idle
  - underutilization
  - longer training times

### Too many workers

- Likely to crash in extreme cases
- May crash after a while(models should be saved)
- Might make the whole system laggy

### How to find correct amount of workers?

- Start with number of available cores divided by two (`multiprocessing.cpu_count()`), usually is a good default.
- Test a little larger, in case of crash continue division by two and repeat.

## Sampler

- Sampler generates indices which are used to get data from `Dataset` contained in `DataLoader`
- this means, we can control __how__ our dataset is actually sampled, which allows us to:
    - perform over/undersampling
    - sample some examples more often, because:
      - they are more important
      - they are harder for neural networks and should be seen more often

## Torchvision

- `torchvision` is part of PyTorch project and described as:
- The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.

__`[SHOW IN VS CODE]`__ Example using MNIST dataset

## Splitting data

- Sometimes we might need to split the data (usually to get our validation dataset)
- Pytorch provides `torch.utils.data.random_split` for those cases:
    
__`[SHOW IN VS CODE]`__ Example of splitting data

## Dataloaders

- Dataloaders are created as a dictionary; you may sometimes see this approach:
  - Pros:
    - easier to read (instead of `training_dataloader`)
    - easier to use
  - Cons:
    - still a lot of repetition

__`[SHOW IN VS CODE]`__ Creating a DataLoader from a dictionary