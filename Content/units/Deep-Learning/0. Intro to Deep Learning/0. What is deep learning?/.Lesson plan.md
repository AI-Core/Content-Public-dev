## Walk through slides

## Comprehensions
- What's deep about deep learning?
- What functions can a neural network model?
    - any, well almost, as long as they are continuous
- What do I mean when I talk about a "hidden" node?
- What's an activation function? What are it's properties?
    - any nonlinear, differentiable function
- Why do I need an activation function?
- If my input is z
    - is z^2 an activation function?
    - is kz an activation function?
        - no, it's linear
    - step function?
        - no, not differentiable
    - sin z?
    - negative relu?
- What does relu stand for?
- What's the function for relu?
    - max(0, z)
- What are some common activation functions?
- You, test us on some other neural network jargon. What words did you learn? I'll pick someone else to tell us what that means
- What do I need to change about my optimiser, stochastic gradient descent, to make sure it works for neural networks?
- What's the curse of dimensionality?
- How can a neural network model any continuous function?!
- If a neural network with just one layer can model any continuous function, why do I use deeper networks?
