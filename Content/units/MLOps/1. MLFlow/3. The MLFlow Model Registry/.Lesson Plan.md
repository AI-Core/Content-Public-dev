- MLflow is an open source platform for managing the end-to-end machine learning lifecycle.
- it can do things like:
	- track experiments

- the main things 
- `pip install mlflow`
- [DEMO] structuring an MLFlow project
	- open examples/hello world
	- show MLProject
		- gets data, trains model, simple
	- make the conda env by running `conda env export > conda.yaml`
	- from cli run `mlflow run .`
- [DEMO] experiment tracking
	- open examples
	- from cli run `mlflow run .`
	- remember to stop running the ui from the previous example, and re-run it in the new example dir
	- show MLProject
	- make the conda env by running `conda env export > conda.yaml`
	- from cli run `mlflow ui`
	- show experiment runs
		- the green tick indicates it succeeded
		- you can see terminal params passed in and metrics logged to the right
		- click into an experiment to see the logged artifacts
<!-- - [DEMO] saving models in MLFlow format -->
<!-- - [DEMO] deploying a model and testing locally
	- deploy as a flask api running locally
	- deploy as a docker container
		- builds a docker image -->


## Model registry
- [EXPLAIN] model registry
	- we can register a model as such
	- there are other ways to do this too
	- once we've registered models, we can get them back as shown in examples/model_registry/infer.py
- we can log experiment metadata to a database, which we call a backend store
	- we need to set up a database backend store to use model registry functionality, and we'll get an error if we dont
		- we can run storage locally using the following command
		- ```
		mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts --host 127.0.0.1
		  ```
		- this sets up 2 things:
			- a local database running sqlite, where your experiment and model metadata is stored
			- a directory where your experiment artifacts, like models, will be stored
			- both of these will be running on the host specified (in this case, localhost)
		- common errors:
			- you may get an error and need to set an environment variable MLFLOW_TRACKING_URI to the URL wherever your backend storage is (http://localhost:5000 if you run the command above)
			- writing localhost rather than explicit loopback address 127.0.0.1
- we can save model artifacts to S3
	- if we do this, we'll be able to test locally, and read our models in from remote machines like EC2 instances or in lambda functions which can access that S3 bucket
- key takeaways:
	- mlflow is a library for managing the machine learning model lifecycle from end-to-end
	- it can track experiments
	- it is an industry standard tool for model registry
