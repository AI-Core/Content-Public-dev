{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Airflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider the following DAG code to run a DAG . If you keep the default configuration, in what order will the tasks run?\n",
    "\n",
    "``` pyhton\n",
    "    task_0 >> [task_1a, task_1b, task_1c] >> task_2\n",
    "\n",
    "```\n",
    "\n",
    "- task_0 -> task_1a -> task_1b -> task_1c -> task_2\n",
    "- task_0 + task_2 -> task_1a + task_1b + task_1c \n",
    "- task_0 -> task_2 -> task_1a + task_1b + task_1c\n",
    "- task_0 -> task_1a + task_1b + task_1c -> task_2 ***\n",
    "- task_2 -> task_0 -> task_1a + task_1b + task_1c\n",
    "- task_2 -> task_1a -> task_1b -> task_1c -> task_0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are alternatives to Airflow?\n",
    "\n",
    "- Luigi ***\n",
    "- Docker\n",
    "- Bash and crontab ***\n",
    "- AWS Step Functions ***\n",
    "- Caffe\n",
    "- CloudReactor ***\n",
    "- MLFlow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select everything True about Airflow\n",
    "\n",
    "- Airflow works on the main programming languages, such as Python, Java, and C\n",
    "- In the same DAG, the same task can only be ran once ***\n",
    "- The scheduler will run even after you restart your computer\n",
    "- The dummy operator is a wildcard to run any type of operation\n",
    "- Unless specified, if the start date has passed, Airflow will run a backfill ***\n",
    "- You can change variables, Xcoms, and connectors from either the CLI, or the UI *** \n",
    "- You can run Airflow both locally and in Docker ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are some of the motivations for using Airflow?\n",
    "\n",
    "- Orchestrating a series of tasks ***\n",
    "- Streaming large amounts of data\n",
    "- Programmatically deploying a ML model based on user defined of metrics ***\n",
    "- Creating a workflow for performing ETL ***\n",
    "- Hosting a web application framework\n",
    "- Running a test suite before deploying your program ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are some of the features of Airflow?\n",
    "\n",
    "- Has an integrated UI which can display and run DAGs in a browser ***\n",
    "- Uses Directed Acyclic Graphs (DAGs) to schedule and run complex tasks ***\n",
    "- It is used in industry often by data engineers to schedule and perform complex ETL/ELT tasks ***\n",
    "- Is closed source and was built only to be used in the cloud\n",
    "- All tasks used by Airflow need to be scheduled and it can't be used for simple tasks\n",
    "- Work with all programming languages C++, Java, Kotlin etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are some of the inbuilt **core** operators which come with Airflow?\n",
    "\n",
    "- PythonOperator: Used to run Python callable ***\n",
    "- BashOperators: Used to execute bash commands ***\n",
    "- EmailOperator: Used to send emails ***\n",
    "- CPlusPlusOperator: Used to run a C++ callable\n",
    "- CMDOperator: Windows operator to run command line commands\n",
    "- CassandraOperator: Cassandra operator used to start and stop Cassandra clusters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is true about Airflow variables?\n",
    "\n",
    "- Airflow variables can be used to improve efficiency by define commonly used variables ***\n",
    "- Airflow variables can be configured in the Airflow UI ***\n",
    "- The `Variable` class must be imported to use Airflow variables ***\n",
    "- Airflow variables can only be used with certain operators\n",
    "- Airflow variables must always be used\n",
    "- Airflow variables cannot be configured from the command line"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is true about Airflow XComs?\n",
    "\n",
    "- Xcoms can be useful when you want to pass the results of a task to another task ***\n",
    "- The results of a task be stored in a special XCom database with `xcom_push` and retrieved with the `xcom_pull` methods ***\n",
    "- You always need to use XComs when defining Airflow tasks\n",
    "- Xcoms can only be seen in your code and are not available to view in the Airflow UI\n",
    "- The primary use of XComs are to save detailed logs of the result of tasks run\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is true about Airflow DAGs?\n",
    "\n",
    "- Code to create DAGs must be stored in the `dags` folder ***\n",
    "- You can only have one custom DAG created with Airflow\n",
    "- DAGs can only be run from the Airflow UI\n",
    "- The path that Airflow looks for DAGs cannot be changed\n",
    "- Airflow will periodically check the folder where DAGs are stored and add them to the Airflow UI ***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In what order will the following DAG run? \n",
    "\n",
    "```python\n",
    "task_0 >> [task_1, task_2] >> task_3\n",
    "```\n",
    "\n",
    "- task_3 -> task_0 -> task_1 + task_2\n",
    "- task_3 -> task_0 -> task_1 -> task_2\n",
    "- task_0 -> task_1 -> task_2 -> task_3\n",
    "- task_0 -> task_1 + task_2 -> task_3 ***\n",
    "- task_3 -> task_1 + task_2 -> task_0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are some of the default arguments which can be specified when initialising a new instance of the DAG class?\n",
    "\n",
    "- `start_date` ***\n",
    "- `retry_delay` ***\n",
    "- `retries` ***\n",
    "- `end_date` ***\n",
    "- `dag_number`\n",
    "- `dag_language` \n",
    "- `manager_id`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is true about parameters which can be provided to the DAG class?\n",
    "\n",
    "- `dag_id` used to define the name of the DAG, viewable in the Airflow UI ***\n",
    "- `dag_id` used to define the name of the DAG, only viewable by running it with the command line\n",
    "- `schedule_interval` used to define how often and when the DAG will run ***\n",
    "- `schedule_interval` used only to determine the date that a DAG will stop running \n",
    "- `default_args` provide the default arguments to the DAG on initialisation\n",
    "- `default_args` provide a boolean that specifics whether to use default arguments or not"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2592652612463181e69ac003232387e3e9a99279aa6b168e76f5df16d5110f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
