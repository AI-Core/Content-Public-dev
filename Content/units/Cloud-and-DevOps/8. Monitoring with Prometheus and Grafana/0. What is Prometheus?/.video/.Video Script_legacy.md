# Intro to Prometheus

## Setup

open .code.ipynb
open .slides.ipynb
use `prometheus.yml` file in .video scripts folder to configure the server


### Intro

- Prometheus is an open source monitoring and alerting toolkit gathering and processing data locally
- It was originally developed at SoundCloud and now is a fully open source community driven project. Prometheus is designed to collect and store metrics as time series data.
- It stores metric information with a timestamp which it was recorded alongside key-value pairs called labels.

- A few facts about it:
  - It's written in golang a programming language developed at google.
  - It provides API's for different languages (including Python), but...
  - We use our browser to query data using Prometheus specific language called PromQL
  - The Prometheus dashboard resides on `localhost:9090` by default.

### Main componenets

__`[SHOW ON SLIDES]`__ Overview of Prometheus 
__Prometheus server__ The server scrapes and stores the time series data.
__Client libraries__ Instrument the application code.
__Push gateway__ supports scraping short lived jobs
__Alertmanager__ Handles alerts
__Exporters__ for supported third party services like HAProxy, StatsD, Graphite and many more.

## So what's going on above?

- Prometheus scrapes data (like metrics):
  - from short lived jobs via the push gateway
  - and from long running jobs directly
  - All samples (values with timestamps) are stored locally together with their metadata.
  - It runs predefined rules on collected data:
    - to gather and aggregate new records
    - To process them and send alerts
- Consumers then use the Prometheus API to visualise the data

## When to use Prometheus

-  When you want to record numerical time series data:
   -  This could be various training metrics gathered across epochs/batches
   -  hardware related data
   -  network traffic or other statistics

- Data is stored locally and does not rely on network storage
- Due to this if something fails then you always have access to the data as each Prometheus server is self-contained and independent.
- When shouldn't you use it?
  - You shouldn't use it if you need very detailed data coming in incredibly fast, for example billing information. As Prometheus is designed to scrape data every few seconds and data is kept locally so might fill fast.

### Data
- All prometheus data is stored as a timestamped timeseries differentiated by metric and (optionally) label.
  - Metric names and labels should be alphanumerical
  - Samples are float64 numerical types
  - timestamps are recorded in milliseconds

__`[SHOW ON SLIDES]`__ Example of how metrics are displayed.
- The general layout of a query is 
- The metric name first 
- Then the `instance` label letting us know the instance which we are checking the cpu for
- then the `job` label letting use know the job we're checking the cpu time for.

### Metrics

- Prometheus provides four metrics out of the box
__`[SHOW ON SLIDES]`__ The four metrics
- counter
  - Counter is monotonically increasing counter (can be restarted)
  - It's useful for:
    - number of requests
    - tasks completed
    - anything that can grow or start a new

- Gauge
  - Is a single value which can increase or decrease
  - Useful for:
    - memory usage monitoring
    - temperature
    - __anything which can change value arbitrarily__

- Histogram

  > __Samples observations and groups them in buckets (which you can configure)__

  Let's say our historgram metric is named `our_super_histogram`. In this case the following operations on histogram are available (__notice we add suffixes to metric name!__):
  - `our_super_histogram_bucket{le="<upper inclusive bound>"}` - cumulative counters for the observation buckets
  - `our_super_histogram_sum` - sum of all values
  - `out_super_histogram_count` - count of observations

- Summary

  > __Samples observations and provides them as a sliding time window__

  - Provides `sum` and `count` like histogram
  - `out_super_summary_quantiles{quantile="value"}` - quantile of observations (one can do something similar for histogram but using functions)

- Functions
  - There are also functions one can run to query for things like `day_of_month()`, we will talk about them in more detail later

### Jobs and instances

- __An instance is an endpoint you can scrape data from__, for instance an EC2 instance or a Docker container and will be a single process.
- __A job is a collections of the same instances__, like multiple EC2 instances and Docker containers, those are usually replicated for reliability/flexibility.

### Installation

- > Prometheus was written in `golang` a programming language developed by Google in 2009, hence __it is contained in a single compiled executable__

- There are two ways to deploy Prometheus we can either go to the download page __`[VISIT PAGE]`__https://prometheus.io/download/ and download the specific installation of prometheus to run it locally.
- Or the second option which we will be using is to run the server in a Docker container. 
- First we need to pull the Prometheus image from Docker Hub. Using __`[SHOW IN TERMINAL]`__ `docker pull prom/prometheus`. 
- Next we need to create a Prometheus config file so we can update it to scrape more metrics.

__`[SHOW IN VS CODE]`__ Sample configuration `prometheus.yml`
-  Create a `prometheus.yml` file on your system (watch out for filepaths with spaces in the name) and add the sample configuration to it. 
- You can find the sample configuration in the notebook under installation.
- After this we will need to edit the Docker command to tell it where to get our `prometheus.yml` file. 

__`[SHOW IN VS CODE]`__ Docker command to run 
  - Here edit the `/path/to/prometheus.yml` to point to where your config file it stored.
  - We also run the flags `--web.enable-lifecycle` and `--config.file` to enable live reloading of the config and tell prometheus where to to host the config file on the server.
  - We also bind the containers 9090 port to the 9090 port on the localhost
  - remove the container with `--rm` when the process is killed
  - `--name` names the containers 
  - and `-v` allows us to mount the config in the container.


- If all goes well Prometheus will now be running on `localhost:909` in your browser.

__`[GO TO localhost:9090 IN BROWSER]`__ 
- You should now see the Prometheus dashboard.
- You should select local time so Prometheus logs metrics in your time zone and query history.
- You can now begin typing expressions in the expression field and Prometheus will suggest queries on metrics you can run.
- Prometheus can track itself so we can run expressions starting with `prometheus` to start tracking prometheus. 
- For instance `prometheus_build_info` and then execute the query.
- Notice Prometheus gives you hints about what the metric does when you hover over it.
- Run the query and you will get the output showing the build info for Prometheus. 
  - `metric name` here is `prometheus_build_info` 
  - labels are `goversion, instance, job etc`

### Configuring Prometheus

__`[SHOW USING PROMETHEUS]`__
- Previously we ran a default Prometheus configuration you can see it by going to `localhost:9090/config` or selecting __Status > Configuration__ from the dashboard.
- Prometheus comes with a lot of configuration options and you can check them out from the Prometheus configuration page.

- A few things to keep in mind about the config file:
- Whats amazing about Prometheus is you can reload its configuration live without effecting the collection of your metrics. 
- Just be sure that your file is in a valid YAML format though or it won't be updated.
- All metrics specified under the `global` heading are available to all the other scraper config defined in the configuration file. i.e if you create a `scrape_config` to scrape an EC2 instance then it will use the global parameters unless explicitly changed.

- Let's edit our config file just changing the `scrape_interval` and `scrape_timeout` to 1s to see how we would update it live. 

__`[SHOW EDITING prometheus.yml]`__ 
 - Now we have change the parameters we just need to update the config now.
 - We can do this using a `curl` command from the terminal since we set the `--web.enable-lifecycle` flag when creating our container.

__`[RUN IN BASH TERMINAL]`__
 - curl -X POST http://localhost:9090/-/reload
  
__`[VIEW ON PROMETHEUS]`__ Changed config file
  - Going back to your Prometheus config we can see that it has been edited now with the updated scrape intervals.

### scrape_configs

- Scrape configs specify a set of targets which we can scrape and parameters describing how to scrape each target.
- For instance your can specify a `scrape_config` to scrape metrics from an EC2 instance or Kubernetes.
- Targets are defined either statically or dynamically
  - statically - configured in our config YAML file by a `scrape_config`
  - dynamically - Using service discovery options. By defining service discovery options we can allow Prometheus to track new instances of a particular service. For example defining a service discovery config to find newly created Docker containers.
  - You can read more about service discovery options from the Prometheus website but we will be concentration on static configurations here.

### Exporters

- Exporters are libraries which make it easier for you to export existing metrics from third-party systems and make them available to Prometheus to track i.e an exporter so you can track GitHub metrics.

here are hundreds of exporters currently all in different states of development, the full list of the most common ones [here](https://prometheus.io/docs/instrumenting/exporters/) and a complete list of all exporters on GitHub [here](https://github.com/prometheus/prometheus/wiki/Default-port-allocations):
- **official** - best practices and verifired by Prometheus; __always pick them if possible__
- **unofficial** - working, not verified for best practices or may have overlapping functionalities
- **in development** - to be released as either of the two above

Important things to keep in mind:
- __Most of the exporters occupy ports `9100`-`9999`__ and any new exporter should use it if any is available (see the github list above to see which ports exporters run on)
- There are a few exporters outside the standard range (once again, see the github list)

## Setting up the Node exporter (Linux/MAC)

- Here we will be setting up the `node_exporter` using for MAC, for Linux and Windows the installation can be found in the notebook.

> `Node` exporter is a single static binary one can download and run straight from the workstation
- run `brew install node_exporter`

- Now with the `node_exporter` installed we just need to run an instance of it. 
- 
__`[RUN IN TERMNIAL]`__ 
- node_exporter --web.listen-address 127.0.0.1:9100
- Now the our node exporter should be running in the background collecting metrics for use with prometheus.
- All we need to do is edit and update our Prometheus config to allow Prometheus to track the exporters metrics.

__`[SHOW EDITING prometheus.yml]`__ Edit the file to track the node exporter, config is in .code.ipynb
- Here we add a name job `node` which will setup scraping of our nodes metrics.
- We just need to define the target (the internal IP address and port our exporter runs on).
- We will also give the node exporter a group name of `production` which will create a group label in Prometheus for easy labelling. 

- Now we just need to update the Prometheus config file so Prometheus can retrieve the metrics.
- There are two ways to do this
  - Mount the directory in your `localhost` to the Docker container during runtime.
    - Which allows us to update the config live
  - Create a new Docker image and copy the configuration into it.
    - Great when you don't expect your config to change often.
- We have setup the auto reload feature so we will use this to update the config.
- Once you have edited the config we will need to run the `curl` command again to `POST` the changes of our config to Prometheus.

__`[RUN IN TERMINAL]`__ curl -X POST http://localhost:9090/-/reload

__`[SHOW ON PROMETHEUS]`__ Updated config file at http://localhost:9090/config
- Now you can see the changes we have made to our config locally have been updated on the Prometheus server live. 

__`[SHOW IN PROMETHEUS]`__ go to http://localhost:9090/targets
- Going to the targets page we can see all the targets that Prometheus is scraping from currently.
- Now that we have added the node exporter you can see it as a target on the target page.
- It has the UP state letting you know that it is currently collecting metrics.
  - Targets can be in other states as well.
    -  **Down** Prometheus cannot connect to the target.
    - **Unknown** Prometheus doesn't know where to find the target, usually due to a configuration issue.

__`[SHOW ON PROMETHEUS]`__ Show where all the metrics are included in the metrics list on prometheus. http://localhost:9100 for metrics node exporter.
- Going to the metrics page for the node exporter will give you a list of all available metrics currently being exported from the node exporter.
- Now we should have the node exporter scraping metrics for Prometheus

__`[SHOW ON PROMETHEUS DASHBOARD]`__ Using expressions on dashboard to query different `node_exporter` metrics
  - Now using expressions starting with `node` we should be able to run queries on these different metrics.
  - Typing `node` you will get a list of all available metrics available to your from the node exporter.
  - Try running some queries on different expressions for the node exporter and checking the graph to see the results.

### Outro

- I hope you enjoyed this introduction to setting up and using Prometheus, it is a very powerful tool enterprise level tool to scrape many metrics from multiple targets, quickly and with a high degree with accuracy. This is just some of the targets you can setup Prometheus to scrape data from you can find many others on the Prometheus website, thanks everyone.

