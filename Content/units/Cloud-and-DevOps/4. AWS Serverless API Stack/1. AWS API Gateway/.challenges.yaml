- id: bc9477af-848f-42c1-96fd-9459085daf35
  name: Create your first API
  description: |
    - firstly, create an API in API gateway
    - create a new resource
    - create a POST method on that resource
    - create a new lambda function which simply returns "Hello World"
    - set it up as the backend integration for your new API resouce's POST method
    - deploy the API to a new stage called "prod" for production
    - make a request to the API through python and get the data back

# - id: 0b1dd8b7-b82c-4e9c-93d0-e9cc9431eb73
#   name: Serverless Ride Hailing App
#   description: |
#     Build a serverless application using AWS API Gateway, Lambda and a new service AWS DynamoDB to emulate a ride hailing app
#     - Build an API with a resource called "riders" 
#     - Create a subresource called "ride" and create it a POST method with a lambda function backend integration
#     - Define the lambda function for that integration 
#       - it should take in a dictionary containing the user_id, current location, and destination
#       - it should then push these into a dynamo database to create a new entry for the ride. So we'll need to set that up!
#     - Go to the DynamoDB console and then create a new table
#       - Give it a partition key (primary key) called ride_id
#         - notice how you don't need to define anything else in the schema - this is a schemaless database!
#     - Create a dynamodb client in your lambda function using boto3 and use it to put the data into the database
#     - Now create a resource called "drivers"
#     - Create a subresource with a GET method which gets the current rides in the database
#     - Create another resource which PUTs the driver's id into a column in the database
#     - Now filter the drivers' GET method do only get rides with no driver_id

- id: 6348c554-3f0e-4a79-8a29-6a150e829555
  name: Create a new API for classifying examples from the Iris dataset
  description: |
    - locally, create an sklearn model and save it to a `.joblib` file
    - create a directory to contain the contents of a deployment package and move inside
    - edit your script that creates the model so that it saves the latest model inside the package
    - install sklearn locally so that you can zip it into a deployment package in the next step
    - create a deployment package for AWS lambda by creating a zip file which contains the `.joblib` file and the sklearn source code
    - use the AWS CLI to upload the deployment package to AWS Lambda
    - create a new API resource with a post method, and this lambda function as the backend integration
    - test locally to make sure that the API works

- id: a50f082c-cf92-4601-9ecb-9cd3092883ca
  name: Create an API that interacts with your MLFlow model registry
  description: |
    - create a MLFlow model registry running inside a docker container
    - deploy this container to EC2 and expose the port which the registry is listening on
    - create a MLFLow lambda layer
    - create a lambda function which uses the MLFlow client to interact with the registry
      - you'll need to add the MLFlow lambda layer
      - make sure to set the MLFLOW_TRACKING_URI environment variable to the registry's endpoint
      - test that it works from within the lambda function
    - create an API with a /predict endpoint, which has this lambda function as the backend integration
    - test making preditions to the API locally to make sure that it works